{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4807e94-963f-41df-b5d1-086e3df6c76b",
   "metadata": {},
   "source": [
    "*created 21 Jan 2026, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 5303 \"AI and Machine Learning in Ocean Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES5303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844859c7-e5a4-4df2-b2dc-b4386bd70af8",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Here we switch gears to deal with ***sequences*** of data. Examples of these are:\n",
    "\n",
    "* Text data (e.g. genomic data, novels)\n",
    "* Time-series data\n",
    "* Acoustic data (e.g. music)\n",
    "* Speech data\n",
    "\n",
    "The main thing is you think the future states depends on the past, and what you want to do is make predictions based on previous data, i.e. there is some ***memory*** effect. One class of neural networks that builds in some memory effect is the ***Recurrent Neural Network*** (RNN), and we are going to do this through `keras`.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Introduce recurrent neural networks and demonstrate its use in time-series predictions.\n",
    "> 2. Experiment with the GRU and LSTM architectures.\n",
    "> 3. A quick look at ConvLSTMs.\n",
    "\n",
    "Load some basic things as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fffbe6-a608-4c0d-a1d3-471fea44a8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PyTorch (not using it very much if at all here though; just load the base one)\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# keras related\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"  # use PyTorch as backend\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "keras.backend.clear_session() # force a clean keras session (clears models etc.)\n",
    "\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# check for GPUs availability\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19ace93-238e-4163-bc34-c42387c69f01",
   "metadata": {},
   "source": [
    "---\n",
    "## a) Basics of RNNs\n",
    "\n",
    "One example of the architecture is below; this is not the only one, but illustrates the some key components of RNNs.\n",
    "\n",
    "<img src=\"https://i.imgur.com/fozD28d.png\" width=\"600\" alt='rnn'>\n",
    "\n",
    "> NOTE: You might see it depicted differently in the other online materials, usually described as a feedforward network that gets used repeatedly. The above one makes more sense to me personally because the hidden state makes it a bit more subtle than that \"feedforward\" description.\n",
    "\n",
    "The main thing that is different to neural nets introduced so far is the presence of a ***hidden state*** alongside the input data; I am going to refer to the tuple `(input_data, hidden_state)` as the ***input***. Here is that the predictions depends on the hidden state itself, and how `input_data` gets updated depends on the `hidden_state`. For example,  if `f(input_data) = (input_data + hidden_state, input_data)`, then `f(1) = 1` the first time but `f(1) = 2` the second time if `hidden_state = 0` to begin with, since the `hidden_state` has updated when `f` is called the second time.\n",
    "\n",
    "The ordering of operation above diagram is as follows:\n",
    "\n",
    "1. Choose input `(input_data, hidden_state)`; `input_data` is from your training data, `hidden_state` is normally initialised to zeroes.\n",
    "2. `input_data` gets transformed in some way by a neural network block `A` (say).\n",
    "3. `hidden_state` gets transformed in some way by a different neural network block `B` (say).\n",
    "4. The returned array of numbers from `A` and `B` are summed and then activated (usually a `tanh` function, although `ReLU` is also used) and returns a `new_hidden_state` (which is an array of numbers).\n",
    "5. This `new_hidden_state` is transformed by another neural network block `C` and activated accordingly to give an `new_input`.\n",
    "6. Take the output (`new_input`, `new_hidden_state`) and pass through the RNN again.\n",
    "7. That generates a sequence of data, from which you can evaluate the loss again the provided training data.\n",
    "8. Do back-propagation relative to that loss, update the parameters in `A`, `B` and `C`, and iterate.\n",
    "\n",
    "In this case there is some freedom in how you specify the size of the `hidden_state`, the neural network blocks (`A`, `B`, `C` can in principle all be different as long as their transformation rules are defined consistently), more things can be put in also (e.g. memory gates relating to ***LSTM*** and ***GRU*** architectures), and the neural networks can include convolutions also (e.g. ***ConvLSTM***). For this particular course we are only going to deal with the simple RNNs.\n",
    "\n",
    "### 1) Time-series data from Lotka-Volterra (or Predator-Prey) model\n",
    "\n",
    "For the demonstration here we are going to generate some time-series data first. For this I am going to numerically solve the [Lotka-Volterra](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations) or ***predator-prey*** model, given by\n",
    "\\begin{equation*}\n",
    "    \\frac{\\mathrm{d}x}{\\mathrm{d}t} = a x - b x y, \\qquad \\frac{\\mathrm{d}x}{\\mathrm{d}t} = -c y - d x y,\n",
    "\\end{equation*}\n",
    "where ($x$, $y$) = (prey, predators) populations (in some units), and $(a,b,c,d)$ are parameters governing birth, death from predation, death and birth from predation (in some consistent units with how $x$, $y$ and $t$ are defined). I am going to leverage the `scipy` inbuilt functionalities to generate a sequence of data resulting from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfd2f7f-2b0c-482e-b1e0-c5d6b77c1da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data from Lotka-Volterra\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "def LotkaVolterra(t, z, a, b, c, d):\n",
    "    x, y = z\n",
    "    return [a*x - b*x*y, -c*y + d*x*y]\n",
    "\n",
    "t = np.linspace(0, 15, 300)\n",
    "a, b, c, d = 1.5, 1, 3, 1\n",
    "sol = solve_ivp(LotkaVolterra, [0, 15], [10, 5], args=(a, b, c, d), t_eval=t,\n",
    "               rtol=1e-6, atol=1e-9)\n",
    "x, y = sol.y\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(t, x, label=r\"prey\")\n",
    "ax.plot(t, y, label=\"predator\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$x, y$\")\n",
    "ax.set_title(r\"time series\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(x, y, 'k-', alpha=0.7)\n",
    "ax.plot(a/b, c/d, 'k*', markersize=12)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.set_title(r\"phase portrait\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30297ba3-a641-49eb-98fb-3c935a759546",
   "metadata": {},
   "source": [
    "Lotka-Volterra is known to have cyclic/periodic behaviour, as seen by the time-series but more clearly from the ***phase portrait*** where I plot $x(t)$ against $y(t)$ for all values of $t$. I've also marked on the non-trivial ***equilibrium point*** of this system as a star.\n",
    "\n",
    "First I want to process the data (which is nested in `sol.y`) to be fed into a RNN. I am going to be lazy and not have any testing data (but see the tests I will do later). In the simplest case I would do a one-step prediction, i.e. the training input data is `[x[i], y[i]]` of `i=0, 1, ... N-1`, and the training output data is `[x[i+1], y[i+1]]` for the same `i` values. In this case the RNN is only ever triggered once; you could argue the it's not really an RNN, but note that there is the hidden state block floating around affecting the outcomes, so it is certainly not the usual neural network either.\n",
    "\n",
    "The below subroutine is defined to chop up the sequences accordingly, and it's done so that it could in principle produce arbitrary input sequence lengths, which I will use later.\n",
    "\n",
    "> NOTE: I could introduce randomness by shuffling which samples of sequences I select (e.g. with a `DataLoader`). I could introduce test and validation data by not passing all the data to the network during the training; this I don't do but is easy to put in with `sklearn` and `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8135330-3d78-4a7e-b8dc-33f7c43db02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop it up into sequences (assumes data sequence is of dim larger than 1)\n",
    "def create_sequences(data, seq_length=1):\n",
    "    x_dum, y_dum = [], []\n",
    "    n_sample = data.shape[0] - seq_length\n",
    "    for i in range(n_sample):\n",
    "        x_dum.append(data[i:i+seq_length, :])  # sequence used to predict end point\n",
    "        y_dum.append(data[i+seq_length, :])    # end point\n",
    "    return np.asarray(x_dum), np.asarray(y_dum)\n",
    "\n",
    "X, Y = create_sequences(sol.y.T, seq_length=1)  # need transpose of \"sol.y\" to get right shape\n",
    "\n",
    "print(f\"input shape is (batch_size, seq, dim) = {X.shape}\")\n",
    "print(f\"input shape is (batch_size, dim) = {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2a0f1-8006-4749-a766-eda4424ffc31",
   "metadata": {},
   "source": [
    "Notice in this case the shape of the inputs has an extra dimension that encodes the input sequence length (in this case it's one).\n",
    "\n",
    "> NOTE: The resulting singleton dimensions may need to be ***squeezed*** out later.\n",
    "\n",
    "For the RNN architecture, you could try and build it manually, but you could also just use the inbuilt `PyTorch` or `keras` functionality; I am going to use the latter. \n",
    "\n",
    "Below is my attempt at wrapping up a RNN. The variable namings I defined here mirrors the namings that are expected in `nn.RNN` in `PyTorch`:\n",
    "\n",
    "* `input_size`: Dimensionality of input. For the Lotka-Volterra this would be `2`, because ($x,y$).\n",
    "* `output_size`: Dimensionality of output. For the Lotka-Volterra this would also be `2`, because it is predicting the next state in time.\n",
    "* `seq_length`: This is not in `PyTorch`, and is the dimension of sequence length, i.e. number of time-steps you throw in.\n",
    "* `hidden_size`: How big you want the memory block to be. Arguably bigger is better, but this increases your neural network blocks `B` and `C` in the schematic above, which may promote over-fitting and slow down your training steps etc.\n",
    "* `num_layers`: How many of these Elman RNNs you want to stack together. If you have `num_layers=2`, then you would have neural network blocks `(A1, B1, C1)` and `(A2, B2, C2)` each with their own associated weights and/or biases that are can be trained.\n",
    "\n",
    "> NOTE: In the below `rnn_cells` definition below you need to use the `SimpleRNNCell` variant. There is a `SimpleRNN` that you can call, but that will lead to crashes (because of dimensionality mismatch). The `Cell` part is important, and the `RNN` part considers the stacked cells (even if it is a stack of 1) as one \"layer\".\n",
    ">\n",
    "> I could have used `Sequential` below but I didn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe4bd5-77a3-496f-be13-e2441f468f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras wrap of a simple RNN (possibly stacked) with a linear layer output\n",
    "\n",
    "def simple_rnn(input_size, output_size, seq_length=1, hidden_size=1, num_layers=1):\n",
    "\n",
    "    # need to use the \"Cell\" variant to loop up\n",
    "    rnn_cells = [layers.SimpleRNNCell(hidden_size) for _ in range(num_layers)]\n",
    "\n",
    "    inputs = keras.Input(shape=(seq_length, input_size))\n",
    "    x = layers.RNN(rnn_cells)(inputs)  # this is one block\n",
    "    outputs = layers.Dense(output_size)(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"simple RNN\")\n",
    "\n",
    "    return model\n",
    "\n",
    "dummy = simple_rnn(2, 2, seq_length=1, hidden_size=60, num_layers=1)\n",
    "dummy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f740079-0127-4985-86a8-4f9baac63345",
   "metadata": {},
   "source": [
    "I am going to be keeping it simple and make things reasonably small. I want to do some shuffling, so I am going to define a simple `Dataset` object and use a `DataLoader` also.\n",
    "\n",
    "> NOTE: There are various things you can switch on with the RNN cells that can be done above. You could output the `hidden_state`, but then I didn't because I don't really need it.\n",
    "\n",
    "> NOTE: Not going to standarise the data here. You may want to at least min/max the data to `[-1, 1]` or similar. In this case I would argue you probably want to normalise it by non-dimensionalising the data with respect to the birth or decay rate `a` or `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ffdbf6-d6d1-4306-94fc-97ed8ca91187",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, in_tensor, out_tensor):\n",
    "        self.inp = in_tensor\n",
    "        self.out = out_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inp)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx], self.out[idx]\n",
    "\n",
    "# specify the seeds\n",
    "torch.manual_seed(1234)\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# use all the data for training (don't have to do this)\n",
    "X_train, Y_train = X, Y\n",
    "train_dataset = MyDataset(X, Y)\n",
    "print(f\"In  data shape = {train_dataset.inp.shape}\")\n",
    "print(f\"Out data shape = {train_dataset.out.shape}\")\n",
    "n_sample = train_dataset.__len__()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=n_sample // 1, \n",
    "                              shuffle=True)\n",
    "\n",
    "# compile\n",
    "RNN = simple_rnn(2, 2, seq_length=1, hidden_size=60, num_layers=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "RNN.compile(loss=keras.losses.MeanSquaredError(),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            )\n",
    "\n",
    "train_log = RNN.fit(train_dataloader, \n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    callbacks=[TqdmCallback(verbose=1)],\n",
    "                   )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad231103-60e0-404c-97ab-f0cda1dfa474",
   "metadata": {},
   "source": [
    "### \"Easy\" testing\n",
    "\n",
    "I am first going to do the easy case first of throwing in the input and seeing what the output is. The given data is going to be markers, and the predictions are lines. According to the loss curve above it should be pretty good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ad168-b818-4714-968d-22e90c758a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare results\n",
    "predictions_train = RNN.predict(X_train)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(Y_train[:, 0], 'C0x', alpha=0.5)\n",
    "ax.plot(Y_train[:, 1], 'C1x', alpha=0.5)\n",
    "ax.plot(predictions_train[:, 0], 'C0-')\n",
    "ax.plot(predictions_train[:, 1], 'C1-')\n",
    "ax.set_xlabel(r\"ind\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(Y_train[:, 0], Y_train[:, 1], 'kx', alpha=0.5)\n",
    "ax.plot(predictions_train[:, 0], predictions_train[:, 1], \"k-\")\n",
    "ax.plot(a/b, c/d, 'k*', markersize=12)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4682651e-fd44-4610-a171-4176bc658c51",
   "metadata": {},
   "source": [
    "It looks pretty good, although you can see some issues with the phase portraits, which is indicating there are issues with the extreme values (the peaks).\n",
    "\n",
    "> NOTE: Could quanitfy this properly with computing the RMS errors over time for example.\n",
    "\n",
    "### \"Harder\" testing\n",
    "\n",
    "Here would be I provide the initial condition, and then keep hitting the predicted output with the RNN. This is a more relevant but harder test because ultimately we might want to use the RNN to make predictions, but the errors with each prediction may accumulate with every pass of the RNN, which may or may not cancel out.\n",
    "\n",
    "> NOTE: For this nonlinear oscillator we might suspect it would be ok given the limit cycle behaviour, but who knows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cad851-e426-419c-96c1-b57f8040bc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a whole chunk of zeros, dump in the prediction, and use as the next input\n",
    "predictions_from_init = np.zeros(Y_train.shape)\n",
    "X_in = X_train[0, :, :]\n",
    "for i in range(Y_train.shape[0]):\n",
    "    Y_pred = RNN.predict(X_in.reshape(-1, 1, 2), verbose=0)  # suppress outputs\n",
    "    predictions_from_init[i, :] = Y_pred.squeeze() # squeeze that dummy singleton dim\n",
    "    X_in = Y_pred # use previous prediction as new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef3a62e-321b-40e9-9375-c21390728d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series and phase portrait\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(Y_train[:, 0], 'C0x', alpha=0.5)\n",
    "ax.plot(Y_train[:, 1], 'C1x', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], 'C0-')\n",
    "ax.plot(predictions_from_init[:, 1], 'C1-')\n",
    "ax.set_xlabel(r\"ind\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(Y_train[:, 0], Y_train[:, 1], 'kx', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], predictions_from_init[:, 1], 'k-')\n",
    "ax.plot(a/b, c/d, 'k*', markersize=12)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "# ax.set_ylabel(r\"$y$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71530b5-cac4-4f7e-9db8-5423cf898aa9",
   "metadata": {},
   "source": [
    "So this one does not do very well after a while, which is perhaps not entirely surprising given we only ask it to do one-step predictions, and the errors start adding up...\n",
    "\n",
    "### \"Hard\" test again but increasing the input sequence length\n",
    "\n",
    "I am going to re-train the model with a longer sequence and then do the \"hard\" test on it, to see if providing more previous time snapshots will help with the recovering the cyclic behaviour at least.\n",
    "\n",
    "> NOTE: With increased sequence length I will have a smaller amount of samples. For this case I could always run the model for longer to generate more data, but I am not going to do that for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa290a2f-13ce-4402-a2bf-0d5035566401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use longer sequences of data\n",
    "seq_length = 10\n",
    "X, Y = create_sequences(sol.y.T, seq_length=seq_length)\n",
    "\n",
    "print(f\"input shape is (batch_size, seq, dim) = {X.shape}\")\n",
    "print(f\"input shape is (batch_size, dim) = {Y.shape}\")\n",
    "\n",
    "# specify the seeds\n",
    "torch.manual_seed(1234)\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# use all the data for training (don't have to do this)\n",
    "X_train, Y_train = X, Y\n",
    "train_dataset = MyDataset(X, Y)\n",
    "print(f\"In  data shape = {train_dataset.inp.shape}\")\n",
    "print(f\"Out data shape = {train_dataset.out.shape}\")\n",
    "n_sample = train_dataset.__len__()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=n_sample // 1, \n",
    "                              shuffle=True)\n",
    "\n",
    "# compile\n",
    "RNN = simple_rnn(2, 2, seq_length=seq_length, hidden_size=60, num_layers=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "RNN.compile(loss=keras.losses.MeanSquaredError(),\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "            )\n",
    "\n",
    "train_log = RNN.fit(train_dataloader, \n",
    "                    epochs=200,\n",
    "                    verbose=0,\n",
    "                    callbacks=[TqdmCallback(verbose=1)],\n",
    "                   )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "# ax.plot(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf8943-c446-4dd3-9772-2f209da6be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a whole chunk of zeros, dump in the prediction, and use as the next input\n",
    "predictions_from_init = np.zeros(Y_train.shape)\n",
    "X_in = X_train[0, :, :]\n",
    "for i in range(Y_train.shape[0]):\n",
    "    Y_pred = RNN.predict(X_in.reshape(-1, seq_length, 2), verbose=0)  # suppress outputs\n",
    "    predictions_from_init[i, :] = Y_pred.squeeze() # squeeze that dummy singleton dim\n",
    "\n",
    "    # update the input sequence\n",
    "    X_dum = np.zeros(X_in.shape)\n",
    "    X_dum[0:-1, :] = X_in[1:, :]\n",
    "    X_dum[-1, :] = Y_pred\n",
    "    X_in = X_dum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d60f3a1-a84e-40e4-8845-f119700b6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series and phase portrait\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(Y_train[:, 0], 'C0x', alpha=0.5)\n",
    "ax.plot(Y_train[:, 1], 'C1x', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], 'C0-')\n",
    "ax.plot(predictions_from_init[:, 1], 'C1-')\n",
    "ax.set_xlabel(r\"ind\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(Y_train[:, 0], Y_train[:, 1], 'kx', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], predictions_from_init[:, 1], 'k-')\n",
    "ax.plot(a/b, c/d, 'k*', markersize=12)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a39da-4812-49cc-a64a-152d0c41d2cd",
   "metadata": {},
   "source": [
    "Here we see that in this case a longer sequence is doing one of the populations better, although its repeating too frequently. Saying that, the populations are actually going extinct then \"resurrecting\", so there are obvious funny things going on...\n",
    "\n",
    "Anyway, the point is that there is no guarantee ML models satisfy the underlying model behaviour just from data. How you force the resulting model to satisfy constraints is a different type of beast; one method is using something like PINNs, which we will encounter in a later session.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Play around with the system parameters ($a,b,c,d$) and/or initial conditions to see how the time-series changes (e.g. whether there are regime shifts), and see how that may or may not affect the RNN performance.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Play around with the sequence length you provide and see what happens.\n",
    "> \n",
    "> <span style=\"color:red\">Q.</span> Introduce train-test-validate accordingly.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Suppose you train an RNN on a time-series trained on a specific choice of ($a,b,c,d$). How does it perform if you deploy the same RNN but on a time-series associated with a different choice ($a,b,c,d$)? Does it depend on the exact regime (cf. above question)?\n",
    "> \n",
    "> <span style=\"color:red\">Q.</span> Consider adding noise to the data and then do the training.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Consider training a neural network to denoise the data and then do the RNN training. You could do the denoising of the predictions on the fly, or before you pass the data into the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41723cb-ea39-4b65-b96d-42545fd00c04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1afc33bc-9a8e-4855-a062-25b4f2a6cd4d",
   "metadata": {},
   "source": [
    "---\n",
    "## b) LSTMs and GRUs\n",
    "\n",
    "These stand for ***[Long Short-Term Memory](https://en.wikipedia.org/wiki/Long_short-term_memory)*** and ***[Gated Recurrent Unit](https://en.wikipedia.org/wiki/Gated_recurrent_unit)***, and are popular variations of the RNN architecture. Both of these introduce extra ***gates*** in the neural network architecture to control information flow; the GRUs could be seen as a slimmed down version of LSTMs, so mostly going to deal with LSTMs here.\n",
    "\n",
    "One issue that happens with training of neural nets with many layers and also RNNs with long sequences is the ***exploding*** or the ***vanishing gradient***. To show what is happening, suppose we consider represent the neural net as\n",
    "\\begin{equation*}\n",
    "    x_t = N(x_{t-1}, \\theta),\n",
    "\\end{equation*}\n",
    "where $N$ is the mapping, $x_t$ is the input at time $t$, and $\\theta$ the model parameters. When we take one gradient, we get by chain rule\n",
    "\\begin{equation*}\n",
    "    \\mathrm{d}x_t = \\nabla_\\theta N(x_{t-1}, \\theta)\\; \\mathrm{d}\\theta + \\nabla_x N(x_{t-1}, \\theta)\\; \\mathrm{d}x_{t-1}.\n",
    "\\end{equation*}\n",
    "Then notice that\n",
    "\\begin{equation*}\n",
    "    \\mathrm{d}x_{t-1} = \\nabla_\\theta N(x_{t-2}, \\theta)\\; \\mathrm{d}\\theta + \\nabla_x N(x_{t-2}, \\theta)\\; \\mathrm{d}x_{t-2}.\n",
    "\\end{equation*}\n",
    "Collecting in $\\mathrm{d}\\theta$ we have\n",
    "\\begin{equation*}\n",
    "    [\\nabla_\\theta N(x_{t-1}, \\theta) + \\nabla_x N(x_{t-1}, \\theta) \\nabla_\\theta N(x_{t-2}, \\theta)]\\; \\mathrm{d}\\theta,\n",
    "\\end{equation*}\n",
    "and convince yourself as you take have longer sequences (or more neural network layers) then you are going to have more and more terms being multiplied by each other. These magnitude of these terms relate to the properties of $N$: \n",
    "\n",
    "* If they tend to take values larger than 1, then the product will increase exponentially, leading to an exploding gradient.\n",
    "* If they tend to take values smaller than 1, then the product will decrease exponentially, leading to a vanishingly small gradient.\n",
    "\n",
    "The former should be quite easy to spot because the model training would just fail. The latter implies the updates become increasingly small/limited and the model starts \"stalling\". For RNNs, this means the information further away from present time is only minimally contributing to updates, and is not being \"learnt\" in some sense. This is not an entirely desirable property and we would like to do something about it.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Try this for yourself with the above data by putting in long sequences and/or making the RNN quite deep. You may need to make the time window of the above example longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0d403e-8ee8-469b-b0c3-165fd621924f",
   "metadata": {},
   "source": [
    "LSTMs (original paper [here](https://www.bioinf.jku.at/publications/older/2604.pdf)) introduces **input**, **output**, **forget gates**, each with their own weights, RNN parts and biases, with appropriate activation functions (normally sigmoids). The main thing to notice these \"gates\" outputs things from 0 to 1: 0 shuts things off, while 1 turns things on completely. These form the components of the LSTM memory block.\n",
    "\n",
    "* Input gates decide what info is stored over time\n",
    "* Output gates decide what info is outputted\n",
    "* Forget gates (I think I would describe them as activation/deactivation gates) control what information is used; think of these are filters for things with low information content.\n",
    "\n",
    "For an example of why this might work, consider the following sentence:\n",
    "\n",
    "> \"Julian, noted for his relentless questioning style, is not a welcome guest at the post-grad seminars.\"\n",
    "\n",
    "Observe that\n",
    "* `Julian` implies `his` but not really the things after, so the `Julian` piece of information could be discarded after a certain point.\n",
    "* `relentless questioning` wants to be kept until `not a welcome quest` probably.\n",
    "* Probably don't need `Julian` once we get to `not a welcome guest`.\n",
    "\n",
    "Discarding information allows you to have shorter chains (which is good), and is the ***short-term*** part. The whole memory block being choosy about which parts to remember/forget (so to me it's not really \"forget\", but \"deactivate\") enables the short-term memory to be maintained ***long*** term, hence LSTM.\n",
    "\n",
    "At a very coarse level, GRUs basically don't have the output gates. \n",
    "\n",
    "Going to demonstrate the above example but only with LSTM. It's basically as simple as a swap from `RNNCell` to `LSTMCell`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c4090-f302-4cbf-bcac-8ad6c3616cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same game but for only for LSTMCell (GRUCell is done basically the same)\n",
    "\n",
    "def simple_lstm(input_size, output_size, seq_length=1, hidden_size=1, num_layers=1):\n",
    "\n",
    "    # need to use the \"Cell\" variant to loop up\n",
    "    lstm_cells = [layers.LSTMCell(hidden_size) for _ in range(num_layers)]\n",
    "\n",
    "    inputs = keras.Input(shape=(seq_length, input_size))\n",
    "    x = layers.RNN(lstm_cells)(inputs)  # this is one block\n",
    "    outputs = layers.Dense(output_size)(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"simple LSTM\")\n",
    "\n",
    "    return model\n",
    "\n",
    "dummy = simple_lstm(2, 2, seq_length=1, hidden_size=60, num_layers=1)\n",
    "dummy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74d65c-ade9-4530-8f73-84c22c6c8975",
   "metadata": {},
   "source": [
    "Notice the basic LSTM has more trainable parameters than the single layer simple RNN (by about a factor of four). We train it as usual but only using the data from the previous step; this is slightly defeating the point of an LSTM block (because the sequence is short), but lets see what we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bd1e6-7348-4e71-ab79-769c5b219083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use longer sequences of data\n",
    "seq_length = 1\n",
    "X, Y = create_sequences(sol.y.T, seq_length=seq_length)\n",
    "\n",
    "# specify the seeds\n",
    "torch.manual_seed(1234)\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# use all the data for training (don't have to do this)\n",
    "X_train, Y_train = X, Y\n",
    "train_dataset = MyDataset(X, Y)\n",
    "n_sample = train_dataset.__len__()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=n_sample // 1, \n",
    "                              shuffle=True)\n",
    "\n",
    "# compile\n",
    "LSTM = simple_lstm(2, 2, seq_length=seq_length, hidden_size=60, num_layers=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "LSTM.compile(loss=keras.losses.MeanSquaredError(),\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             )\n",
    "\n",
    "train_log = LSTM.fit(train_dataloader, \n",
    "                     epochs=200,\n",
    "                     verbose=0,\n",
    "                     callbacks=[TqdmCallback(verbose=1)],\n",
    "                    )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "# ax.plot(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93339141-5a43-4cda-84f6-cf263b24102d",
   "metadata": {},
   "source": [
    "We are going to subject the model to the hard test of using it's own predictions to predict the later steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d069cb-7d3c-4f50-b10f-b833dcc35fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a whole chunk of zeros, dump in the prediction, and use as the next input\n",
    "predictions_from_init = np.zeros(Y_train.shape)\n",
    "X_in = X_train[0, :, :]\n",
    "for i in range(Y_train.shape[0]):\n",
    "    Y_pred = LSTM.predict(X_in.reshape(-1, 1, 2), verbose=0)  # suppress outputs\n",
    "    predictions_from_init[i, :] = Y_pred.squeeze() # squeeze that dummy singleton dim\n",
    "    X_in = Y_pred # use previous prediction as new input\n",
    "\n",
    "# time series and phase portrait\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(Y_train[:, 0], 'C0x', alpha=0.5)\n",
    "ax.plot(Y_train[:, 1], 'C1x', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], 'C0-')\n",
    "ax.plot(predictions_from_init[:, 1], 'C1-')\n",
    "ax.set_xlabel(r\"ind\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(Y_train[:, 0], Y_train[:, 1], 'kx', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], predictions_from_init[:, 1], 'k-')\n",
    "ax.plot(a/b, c/d, 'k*', markersize=12)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "# ax.set_ylabel(r\"$y$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13351f-1a61-430f-87f3-411067ffd49e",
   "metadata": {},
   "source": [
    "It's pretty good at the beginning but then it ends up \"stalling\", but at least it's not predicting resurrections...\n",
    "\n",
    "Below training is going to use a longer sequence to do the same thing.\n",
    "\n",
    "> NOTE: I find the training to be noticeably slower than the simple RNN. This is not surprising given there are more trainable parameters.\n",
    ">\n",
    "> Below uses `seq_length=10` as a default mostly for comparison with above. I find that `seq_length=20` does pretty well, if you are willing to ignore the fact one of the population goes extinct and resurrects. See later comments at the ***ConvLSTM*** section for some other things you should try modifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef79a6b0-852e-4461-a2a8-d6759169e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use longer sequences of data\n",
    "seq_length = 10\n",
    "X, Y = create_sequences(sol.y.T, seq_length=seq_length)\n",
    "\n",
    "# specify the seeds\n",
    "torch.manual_seed(1234)\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# use all the data for training (don't have to do this)\n",
    "X_train, Y_train = X, Y\n",
    "train_dataset = MyDataset(X, Y)\n",
    "n_sample = train_dataset.__len__()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, \n",
    "                              batch_size=n_sample // 1, \n",
    "                              shuffle=True)\n",
    "\n",
    "# compile\n",
    "LSTM = simple_lstm(2, 2, seq_length=seq_length, hidden_size=60, num_layers=1)\n",
    "\n",
    "learning_rate = 0.01\n",
    "LSTM.compile(loss=keras.losses.MeanSquaredError(),\n",
    "             optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             )\n",
    "\n",
    "train_log = LSTM.fit(train_dataloader, \n",
    "                     epochs=200,\n",
    "                     verbose=0,\n",
    "                     callbacks=[TqdmCallback(verbose=1)],\n",
    "                    )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f24bc2f-e875-442a-9c9c-4fa384b958cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a whole chunk of zeros, dump in the prediction, and use as the next input\n",
    "predictions_from_init = np.zeros(Y_train.shape)\n",
    "X_in = X_train[0, :, :]\n",
    "for i in range(Y_train.shape[0]):\n",
    "    Y_pred = LSTM.predict(X_in.reshape(-1, seq_length, 2), verbose=0)  # suppress outputs\n",
    "    predictions_from_init[i, :] = Y_pred.squeeze() # squeeze that dummy singleton dim\n",
    "\n",
    "    # update the input sequence\n",
    "    X_dum = np.zeros(X_in.shape)\n",
    "    X_dum[0:-1, :] = X_in[1:, :]\n",
    "    X_dum[-1, :] = Y_pred\n",
    "    X_in = X_dum\n",
    "\n",
    "# time series and phase portrait\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(Y_train[:, 0], 'C0x', alpha=0.5)\n",
    "ax.plot(Y_train[:, 1], 'C1x', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], 'C0-')\n",
    "ax.plot(predictions_from_init[:, 1], 'C1-')\n",
    "ax.set_xlabel(r\"ind\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(Y_train[:, 0], Y_train[:, 1], 'kx', alpha=0.5)\n",
    "ax.plot(predictions_from_init[:, 0], predictions_from_init[:, 1], 'k-')\n",
    "ax.plot(a/b, c/d, 'k*', markersize=12)\n",
    "ax.set_xlabel(r\"$x$\")\n",
    "ax.set_ylabel(r\"$y$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a65bbe0-06ab-43b9-8ec9-7f66440a7225",
   "metadata": {},
   "source": [
    "The model is producing oscillations that are predator-prey model like, although it is not at the right amplitude or period. The phase difference is ok (the prey comes up and down before the predator). Structure seems promising, and further tuning might get it better.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Play around with the sequence length you provide to it and see what happens.\n",
    ">\n",
    "> See also the next section on other things to vary.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try playing around with the architecture and other model hyper-parameters to see how you improve/degrade the model performance.\n",
    "> \n",
    "> <span style=\"color:red\">Q.</span> Do the same thing but for GRU, and see where the differences in model (e.g. number of degrees of freedom) and performance (e.g. skill, training speed etc.) occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4652dc54-ce67-4b57-b073-910afdeead48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70272f95-d635-4bd4-b1a3-fe3d38780a92",
   "metadata": {},
   "source": [
    "---\n",
    "## c) A quick look at ConvLSTMs\n",
    "\n",
    "In the RNN blocks the default is to use the fully connected neural networks, but there is no reason you can't use convolution layers instead (e.g. sequence of images such as in a video). A common one is ***ConvLSTM*** (i.e. CNN + LSTM).\n",
    "\n",
    "In the below case I am going to generate data from the [Korteweg-De Vries](https://en.wikipedia.org/wiki/Korteweg%E2%80%93De_Vries_equation) or the KdV equation, which is sometimes invoked as a model for tsunami waves in the (because it has special nonlinear stable solutions called ***solitons***). The resulting data is of the form $u(x,t)$, i.e. for every time-frame there is a 1d graph. I set it up so that anything that goes out form one side of the domain comes back from the other side (i.e. the domain is periodic in $x$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d5e1e-4f78-425d-98f7-e0d600e1394b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force solve the KdV equation with two solitons initial condition (sort of)\n",
    "# pseudo-spectral for periodic domain\n",
    "# (taken from https://scipy-cookbook.readthedocs.io/items/KdV.html)\n",
    "\n",
    "from scipy.integrate import odeint\n",
    "from scipy.fftpack import diff as psdiff\n",
    "\n",
    "# define the initial conditions for a single soliton, generate two of these and add them\n",
    "# together\n",
    "# (the two solition solution is not what is described but it's good enough...)\n",
    "def kdv_exact(x, c):\n",
    "    u = 0.5*c*np.cosh(0.5*np.sqrt(c)*x)**(-2)\n",
    "    return u\n",
    "\n",
    "# define the subroutine for generating \\partial u / \\partial t\n",
    "def kdv(u, t, L):\n",
    "    ux = psdiff(u, period=L)\n",
    "    uxxx = psdiff(u, period=L, order=3)\n",
    "    return -6*u*ux - uxxx\n",
    "\n",
    "# set the space and time grid\n",
    "L = 50.0\n",
    "N = 64\n",
    "dx = L / (N - 1.0)\n",
    "x = np.linspace(0, (1-1.0/N)*L, N)\n",
    "\n",
    "# Set the time sample grid.\n",
    "T = 200\n",
    "t = np.linspace(0, T, 501)\n",
    "\n",
    "# initial conditions with two solitons (caveat as above\n",
    "u0 = kdv_exact(x-0.33*L, 0.75) + kdv_exact(x-0.65*L, 0.4)\n",
    "sol = odeint(kdv, u0, t, args=(L,), mxstep=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86146f2-8d5d-4523-bb3f-8e3724d38b07",
   "metadata": {},
   "source": [
    "For this example I set two waves going. I am going to first plot the numerical solution as a function of space $x$ at fixed $t$, and then in the second plot I am going to do a [HovmÃ¶ller](https://en.wikipedia.org/wiki/Hovm%C3%B6ller_diagram) that shows the solution as a function of $x$ and $t$ together (with $t$ going up). \n",
    "\n",
    "Either way, convince yourself what you have is that both waves are going to the right, the taller wave travels faster and catches up with the shorter wave, and eventually overtakes it. The interesting thing to observe is that when they overlap they interact somewhat (the shorter wave gets \"pulled back\"), but both come out effectively intact. The intactness would be obvious in the linear wave equation case (where each wave is a solution and the waves don't interact with each), but is not a trivial fact for nonlinear equations like the KdV equations where the linearity does not allow solutions to add to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6c2b9-3a9e-4e4c-97a3-2df1510e3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshots in time\n",
    "\n",
    "Nt = len(t)\n",
    "inds = [0, Nt // 10, Nt // 5]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.axes()\n",
    "for i in range(len(inds)):\n",
    "    ax.plot(x, sol[inds[i], :], label=f\"$t = {{{t[inds[i]]:.2f}}}$\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$u$')\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d9974d-5776-4306-a197-43ecbde66a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hovmoller plot (time going up)\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.axes()\n",
    "cs = ax.contourf(x, t, sol, levels=101)\n",
    "cax = plt.colorbar(cs)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da14e5-a253-4704-b6dc-b3a1373e7be8",
   "metadata": {},
   "source": [
    "The network architecture is largely inspired from the Keras example for video frame prediction [here](https://keras.io/examples/vision/conv_lstm/). For this we are going to have inputs that is a sequence of $n$ frames, and the target is also $n$ frames shifted by one time-step (e.g. $X = [u(1), u(2), u(3)], Y = [u(2), u(3), u(4)]$, as opposed to before when we simply took $Y = [u(4)]$). This means we need to split the data up slightly differently, which is done in the subroutine below.\n",
    "\n",
    "In the below I am going to take a sequence length of 10. \n",
    "\n",
    "For the `keras` syntax the `ConvLSTM` cells what a channel dimension also (in the channel last convention), so I am also going to add in a dummy dimension that has nothing in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a2748-bcb9-48fb-91cf-3b902eaecbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chop both train and test into sequences\n",
    "def create_sequences_2(data, seq_length=1):\n",
    "    x_dum, y_dum = [], []\n",
    "    n_sample = data.shape[0] - seq_length\n",
    "    for i in range(n_sample):\n",
    "        x_dum.append(data[i:i+seq_length, :])      # input sequence\n",
    "        y_dum.append(data[i+1:i+seq_length+1, :])  # target sequence shifted by 1 time index\n",
    "    return np.asarray(x_dum), np.asarray(y_dum)\n",
    "\n",
    "# chop it up into sequences (previous subroutine works ok here)\n",
    "seq_length = 10\n",
    "X, Y = create_sequences_2(sol, seq_length=seq_length)  # data already in the right shape so no .T\n",
    "\n",
    "# force a \"channel\" dimension (don't actually use it, here for syntax reasons with Conv)\n",
    "X, Y = X.reshape(X.shape + (1,)), Y.reshape(Y.shape + (1,))\n",
    "\n",
    "print(f\"input shape is (batch_size, seq, dim, channel) = {X.shape}\")\n",
    "print(f\"input shape is (batch_size, dim, channel) = {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08ea5a-6730-488c-9852-d15681723b25",
   "metadata": {},
   "source": [
    "The model network below has the following parts:\n",
    "\n",
    "* The `Input` shape should be `(batch_size, seq_length, dim, channel)`, but note that\n",
    "  - I don't actually need to specify `batch_size` (so don't do anything)\n",
    "  - I am not going to specify the `seq_length`, and this is denoted `None`\n",
    "  - the `*input_shape` is for `(dim, channel)` which I am going to infer from the training data later.\n",
    "* 1st `ConvLSTM1D` block with the option for what is called `batch_normalisation`\n",
    "  - it's `1D` because my data is 1d in space\n",
    "  - this first block has 16 filters and a larger `kernel_size`\n",
    "  - `padding=\"same\"` to not change the dimension of `dim`\n",
    "  - `return_sequences=True` to for it to output a sequence of `seq_length`, and not change the dimension of `seq_length`\n",
    "  - not going to elaborate on what `batch_normalisation` is, but it's a thing that can help with model performance (although I switch it off by default)\n",
    "* 2nd `ConvLSTM1D` block is the same but with a smaller `kernel_size`\n",
    "* 3rd block is a `Conv2D` because I want to force a convolution in `(seq_length, dim)`\n",
    "  - `padding=\"same\"` keeps the dimension size of `(seq_length, dim)`\n",
    " \n",
    "The output is going to be something of shape `(seq, dim, channel)`.\n",
    "\n",
    "> NOTE: I didn't have to do that 3rd block, but it is a choice I made. There are other things you can do probably; see exercise below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890eb08-f81c-410f-a10c-f336d55f482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_ConvLSTM(input_shape, batch_normalisation=False):\n",
    "\n",
    "    inputs = layers.Input(shape=(None, *input_shape))  # (seq [as None], space, channel)\n",
    "\n",
    "    # block 1\n",
    "    x = layers.ConvLSTM1D(\n",
    "        filters=16,\n",
    "        kernel_size=5,\n",
    "        padding=\"same\",\n",
    "        return_sequences=True,  # keep the sequences (otherwise the next step bugs out)\n",
    "        activation=\"relu\",\n",
    "    )(inputs)\n",
    "    if batch_normalisation:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # block 2\n",
    "    x = layers.ConvLSTM1D(\n",
    "        filters=16,\n",
    "        kernel_size=3,\n",
    "        padding=\"same\",\n",
    "        return_sequences=True,\n",
    "        activation=\"relu\",\n",
    "    )(x)\n",
    "    if batch_normalisation:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # block 3\n",
    "    #   squash this down to final sequence with Conv2D to force data to \"talk\"\n",
    "    #   Conv2D because data is (t, x); use Conv3D for (t, x, y) and etc.\n",
    "    #   could use Conv1D to force data to \"talk\" only along one dim I suppose\n",
    "    \n",
    "    outputs = layers.Conv2D(filters=1, \n",
    "                            kernel_size=(3, 3), \n",
    "                            activation=\"sigmoid\", \n",
    "                            padding=\"same\"\n",
    "                           )(x)\n",
    "    model = keras.Model(inputs, outputs, name=\"simple ConvLSTM\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# throw in dimensions after seq_length (so thorw in (..., dim, channel))\n",
    "dummy = simple_ConvLSTM((X.shape[2:]), batch_normalisation=False)\n",
    "dummy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645ece36-26ab-40e3-b258-8e602043784c",
   "metadata": {},
   "source": [
    "The training is as usual, throwing in the relevant information to initialise the model.\n",
    "\n",
    "> NOTE: I have specified a small `batch_size`, which seems to help with model performance skill, although it does make it slow-ish (below code takes about 5 mins to run on my laptop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ba90a9-b8a1-476d-af85-d36b1ef24705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usual training\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "simple_ConvLSTM = simple_ConvLSTM((X.shape[2:]), batch_normalisation=False)\n",
    "\n",
    "learning_rate = 0.001\n",
    "simple_ConvLSTM.compile(loss=keras.losses.MeanSquaredError(),\n",
    "                        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       )\n",
    "\n",
    "# predicts whole sequence, but will only use the end one\n",
    "train_log = simple_ConvLSTM.fit(X, Y, \n",
    "                                epochs=50,\n",
    "                                batch_size=10,  # small batch size seems to help with skill\n",
    "                                validation_split=0.1,\n",
    "                                verbose=0,\n",
    "                                callbacks=[TqdmCallback(verbose=1)],\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4f701c-9ee1-43f5-8063-2760016cf73a",
   "metadata": {},
   "source": [
    "Below we plot the loss but I am going to put it on a log scale to show the small values. \n",
    "\n",
    "> NOTE: One thing to be careful here is how you interpret the value of the loss. In the previous cases where I standardise the data, a loss larger than 1 is \"bad\". In the present case because I didn't scale the data and the loss has a dimension, so it is not clear where the threshold for \"bad\" is.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> The threshold for \"bad\" for this set up is going to much smaller than 1, and anything larger than $10^{-2}$ or $10^{-3}$ is probably bad. Convince yourself why that is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b3c50-6c4c-47ad-8af8-5db026e3ba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.semilogy(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.semilogy(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19005489-2238-4a24-bb32-3a936957b3fb",
   "metadata": {},
   "source": [
    "I am going to do a chained prediction. The way this is going to work is that I choose some time and initialise with the first 10 sequences (because I took `seq_length=10` above). The model is going to predict 10 frames, but I am going to keep the last one: for example, for $t=0, \\ldots 9$ the model would predict $t=1, \\ldots 10$, and I am going to discard all except the last one. I am then going to piggyback on that new prediction to predict subsequent frames.\n",
    "\n",
    "Further more I am going to squeeze out the dummy `channel` dimensions that I don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771c9d3-3e1e-4e7a-8734-507d204dde5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chained prediction\n",
    "\n",
    "t_ind_init = 201\n",
    "Y_true = sol[t_ind_init::, :]  # from some index to the end\n",
    "\n",
    "# initialise array of zeros to dump outputs in\n",
    "Y_pred = np.zeros(Y_true.shape)\n",
    "Y_pred[:seq_length, :] = Y_true[:seq_length, :]\n",
    "\n",
    "# make predictions: add in dummy dims for model, then squeeze out dummy dims\n",
    "# (probably not memory efficient here)\n",
    "for i in range(Y_pred.shape[0] - seq_length):\n",
    "    if i % 30 == 0:\n",
    "        print(f\"working at {i} / {Y_pred.shape[0] - seq_length}...\")\n",
    "    new_seq = simple_ConvLSTM.predict(\n",
    "        Y_pred[np.newaxis, i:seq_length+i, :, np.newaxis], \n",
    "        verbose=0)\n",
    "    Y_pred[seq_length+i, :] = new_seq[0, -1, :, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d78cd24-e982-472a-a00e-16b3d329aad0",
   "metadata": {},
   "source": [
    "Only going to plot the resulting HovmÃ¶ller diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf328ae-463c-4e4d-83da-7e8b14e06f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hovmoller plot (time going up)\n",
    "\n",
    "cmin, cmax = Y_true.min(), Y_true.max()\n",
    "\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cs = ax.contourf(x, t[t_ind_init::], Y_true, levels=np.linspace(cmin, cmax, 101))\n",
    "ax.plot([x[0], x[-1]], [t[t_ind_init+seq_length], t[t_ind_init+seq_length]], 'w--')\n",
    "cax = plt.colorbar(cs)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$')\n",
    "ax.set_title(r'Model truth')\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cs = ax.contourf(x, t[t_ind_init::], Y_pred, levels=np.linspace(cmin, cmax, 101))\n",
    "ax.plot([x[0], x[-1]], [t[t_ind_init+seq_length], t[t_ind_init+seq_length]], 'w--')\n",
    "cax = plt.colorbar(cs)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$')\n",
    "ax.set_title(r'Model truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04332e6b-056a-4e96-8a61-a27d9bb10ee2",
   "metadata": {},
   "source": [
    "In the above what we see is that the predictions are ok for a bit, but then it starts losing integrity. Some aspects such as the interaction of the waves is sort of kept (at around $t=150$). There is arguably some limited skill that could probably be improved quite a bit with model tuning etc.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Try plotting the individual outputs of the time frames to see how the predictions change in time compared to the target data (i.e. do line plots rather than do Hovmoller diagrams).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Can try and be quantitative and compute the mismatch over time of the predictions (e.g. the $L^2$ error as a function of time say). \n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Because of how I set up the above ConvLSTM architecture, in the above I predict whole sequences even though I only use the end one. Try modifying it so that it doesn't need to do that. One way would be to add a linear layer instead (or in addition) of the `Conv3D` layer above to squash the `seq_length` dimension down to `1`. I assume there are other ways to do it also.\n",
    ">\n",
    "> (For things larger than one space dimension, e.g. an image, you would need to flatten, do linear layer and the reshape the output instead).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Experiment with model architecture and hyper-parameters etc.\n",
    ">\n",
    "> The probably interesting one here is `seq_length` and `batch_size`: I find increasing `batch_size` tends to degrade the model particularly when `seq_length` is larger, which is unfortunate because larger `seq_length` and smaller `batch_size` increases the training time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc726c-b4da-4624-bc47-6827a67d3b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c19b1dd-79a7-4bdf-bb46-961902768f32",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bc669-bb96-47a3-a8f2-90a1b9431c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da8619d9-09b8-4303-b942-c39545482853",
   "metadata": {},
   "source": [
    "## 1) The Lorenz (1963) model\n",
    "\n",
    "The code for generating the data for the famous butterfly diagram from the [Lorenz (1963) model](https://en.wikipedia.org/wiki/Lorenz_system) is given below. Have a look at deploying RNNs for that; you will need to modify the sizes here because the Lorenz63 model state is described by three variables. \n",
    "\n",
    "This is an example where we have [chaos](https://en.wikipedia.org/wiki/Chaos_theory) in the sense of sensitive dependence on initial conditions, and the predictive step is particularly hard. \n",
    "\n",
    "The question might not be whether we have skill in predicting the state $(x,y,z)$, but whether the resulting model at least respects the statistics, i.e. whether the predictions swirl round the two wings. Compare this to predicting the weather and the climate respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b949017b-036e-4a52-a7d4-8edea6a5bb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def lorenz63(t, state, sigma, rho, beta):\n",
    "    x, y, z = state\n",
    "    dx = sigma * (y - x)\n",
    "    dy = x * (rho - z) - y\n",
    "    dz = x * y - beta * z\n",
    "    return [dx, dy, dz]\n",
    "\n",
    "state0 = [1.0, 1.0, 1.0]  # Initial state\n",
    "params = (10.0, 28.0, 8.0/3.0)  # sigma, rho, beta\n",
    "\n",
    "t = np.linspace(0, 50, 5000)\n",
    "sol = solve_ivp(lorenz63, [t[0], t[-1]], [0.1, 0.1, 0.1], t_eval=t, args=params)\n",
    "\n",
    "x, y, z = sol.y\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "ax.plot(x, y, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1522126e-7c47-4ad0-b725-625883222ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccdd04f2-75b0-47d7-82b1-32738f54d0c5",
   "metadata": {},
   "source": [
    "## 2) Time-series forecasting\n",
    "\n",
    "Try and adapt this [atmospheric example](https://keras.io/examples/timeseries/timeseries_weather_forecasting/) that makes use of observational data yourself. The example page provides the data and so forth.\n",
    "\n",
    "If you can do that you could try doing something similar for the provided bacteria data if you like; ssee the assignments folder for the data file. The data does have a time element to it (every two weeks?), although in the assignment I suggest for you to group it in terms of seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81bde4-2889-454a-87d3-bc3a7e2d74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf7c7f0-48c1-4c8a-be57-215271fd6e56",
   "metadata": {},
   "source": [
    "## 3) ConvLSTM for Kuramoto-Sivashinsky\n",
    "\n",
    "As above for ConvLSTM but do this for the [Kuramoto-Sivashinsky](https://en.wikipedia.org/wiki/Kuramoto%E2%80%93Sivashinsky_equation) equaton, which has some nominal application in ocaenography. This is a chaotic system for large enough domain sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6c539-a775-4c96-a0e0-28596a50ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Kuramoto-Sivashinsky equation\n",
    "#    model variable here is u(x,t), you can think of it as amplitude of something\n",
    "#    would suggest trying to vary L, in particular decreasing it here\n",
    "\n",
    "# define the subroutine for generating \\partial u / \\partial t\n",
    "def ks(u, t, L):\n",
    "    ux = psdiff(u, period=L)\n",
    "    uxx = psdiff(u, period=L, order=2)\n",
    "    uxxxx = psdiff(u, period=L, order=4)\n",
    "    return -uxx - uxxxx - u*ux\n",
    "\n",
    "# set the space and time grid\n",
    "L = 32*np.pi\n",
    "N = 128\n",
    "dx = L / (N - 1.0)\n",
    "x = np.linspace(0, (1-1.0/N)*L, N)\n",
    "\n",
    "# Set the time sample grid.\n",
    "T = 200\n",
    "t = np.linspace(0, T, 501)\n",
    "\n",
    "# initial conditions (selection of cosines and sines will do)\n",
    "u0 = np.cos(x/16)*(1+np.sin(x/16))\n",
    "sol = odeint(ks, u0, t, args=(L,), mxstep=5000)\n",
    "\n",
    "# Hovmoller plot (time going up; have a think about what this is showing)\n",
    "fig = plt.figure(figsize=(6, 5))\n",
    "ax = plt.axes()\n",
    "cs = ax.contourf(x, t, sol, levels=101)\n",
    "cax = plt.colorbar(cs)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$t$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef399752-55c0-4385-b637-099b8c82d4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d413545-f3f0-4cbf-9e4e-35b5018b7fff",
   "metadata": {},
   "source": [
    "## 4) Text prediction\n",
    "\n",
    "Have a look at this Keras example for [text prediction](https://keras.io/examples/generative/lstm_character_level_text_generation/) to see how you manipulate text data, pass these around as data for RNN layers, and train up a text predictor.\n",
    "\n",
    "Once you are comfortable with what is going on, try and find do it on your own dataset (find a digitised novel or something; a previous student of mine used [The Communist Manifesto](https://en.wikipedia.org/wiki/The_Communist_Manifesto) for a project computing information entropy, so you could try using that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dfaee9-698d-45e1-b622-ccc67d1d0519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
