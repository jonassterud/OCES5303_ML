{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a937b507-25cd-470c-aff9-6585667c880f",
   "metadata": {
    "id": "a937b507-25cd-470c-aff9-6585667c880f"
   },
   "source": [
    "*updated 08 Jan 2026, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 5303 \"AI and Machine Learning in Ocean Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES5303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24eef591-a0a8-494b-9a25-2be3d581160e",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. More `sklearn` functionalities and basic modelling\n",
    "\n",
    "As a precursor to neural networks here we are going to do a few other common data driven modelling approaches, still using the `sklearn` library. A few more modelling approaches and problems are introduced, and the more complex models require us to revisit the issue of model validation and overfitting.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Explore the uses and properties of a subset of linear models available in `sklearn`.\n",
    "> 2. Formulating dimension reduction and clustering again as optimisation problems.\n",
    "> 3. Deploying dimension reduction and clustering methods for sample problems.\n",
    "> 4. Given the inherent randomness and variability relating to the complexity of the models, highlight the need to evaluate robustness of model skill (which requires defining what is meant by \"skill\"), and demonstrate some inbuilt `sklearn` cross-validation routines.\n",
    "\n",
    "Going to load a whole load of basic things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f326d-dee7-40b8-ba35-8bec069a02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.feature_selection import r_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bd0f84-64a0-4183-a152-22fb2afbd091",
   "metadata": {
    "id": "99bd0f84-64a0-4183-a152-22fb2afbd091"
   },
   "source": [
    "---\n",
    "## a) Linear models\n",
    "\n",
    "Going to start with ***(multi-)linear models***, which includes linear regression as a special case. As a recap, given multiple features $X = (X^{(1)}, X^{(2)}, \\ldots)$ (denoted with superscripts; this is different to data sample which I denote by subscripts) and a **single** target $Y$, a linear model is of the form\n",
    "\\begin{equation*}\n",
    "    \\hat{Y} = a_0 + a_1 X^{(1)} + a_2 X^{(2)} + \\ldots a_N X^{(N)}\n",
    "\\end{equation*}\n",
    "where the ***model parameters*** or ***loading values*** are $\\{a_0, a_1, \\ldots, a_N\\}$. The name of the game is find the set of $\\{a_j\\}$ values to minimise a some ***objective/cost/loss function*** $J$ to be specified: the different linear models basically differ in the choice of $J$, which leads to slightly different properties.\n",
    "\n",
    "Suppose we take a loss function of the form\n",
    "\\begin{equation*}\n",
    "    J = \\frac{1}{2M}\\|\\hat{Y} - Y\\|_{L^2}^2 + \\alpha\\rho\\|a\\|_{L^1} + \\frac{\\alpha(1-\\rho)}{2}\\|a\\|_{L^2}^2,\n",
    "\\end{equation*}\n",
    "made up of the mismatch and two penalisations of the value of the loading values (or control variables). The four variants we are considering here are:\n",
    "\n",
    "* Standard ***linear regression***, which takes $\\alpha = 0$, i.e. there no penalisation (the value of $M$ is irrelevant in this case, but you could take it to be $M=1/2$).\n",
    "* ***Ridge optimisation***, which takes $\\rho=0$ (and $M=1/2$). Then $\\alpha$ becomes a model hyperparameter controlling the strength of penalisation, where the values of the coefficients are not allowed to be too big.\n",
    "* ***Lasso***, which takes $\\rho=1$ (and $M$ is the number of samples). This choice of penalisation promotes **sparsity** in the loading values, i.e. we don't just want their size controlled, we don't even want them if possible.\n",
    "* ***Elastic net***, which is a combination of ridge and lasso, and does both of the above at the same time.\n",
    "\n",
    "All the associated methodologies are done somehow, mostly using gradient descent type methods (see the lectures). It's easier to see what they do by using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0a729c-ab17-4f83-87e2-86cacc2a4ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe7bea-edcc-4f02-a01c-8999e8384d16",
   "metadata": {
    "id": "b1fe7bea-edcc-4f02-a01c-8999e8384d16"
   },
   "source": [
    "### Demonstration: Basic fitting\n",
    "\n",
    "To compare how the aforementioned linear models function, I am going to use artificial data\n",
    "\\begin{equation*}\n",
    "    Y = X^2 + \\epsilon, \\qquad \\epsilon\\sim\\mathcal{N}(0, \\sigma)\n",
    "\\end{equation*}\n",
    "where $\\epsilon$ is some random noise determined by magnitude of $\\sigma$. Here I don't scale the noise level with $X$, so as $X$ becomes big the effect of noise becomes smaller; however I am not going to take $X$ to be that big. The below shows the data (I am fixing the seed to make sure it is reproducible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e8740-54d7-438e-9afa-c1d98b42b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the artificial data\n",
    "\n",
    "sigma = 0.4\n",
    "n = 100\n",
    "X_vec = np.linspace(0, 4, n)  # including 0 here is potentially problematic, do it anyway...\n",
    "np.random.seed(69)\n",
    "Y = X_vec**2 + sigma * np.random.rand(n)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(X_vec, Y, 'ko', alpha=0.7)\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$Y$\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d559a489-1300-4202-a749-7e6f0974c9fb",
   "metadata": {
    "id": "d559a489-1300-4202-a749-7e6f0974c9fb"
   },
   "source": [
    "In this case I am not going to standarise the data nor do a train/test split, because this is really for demonstrating the differences in the behaviours in the linear model.\n",
    "\n",
    "In this case we know what the data is so it should be clear that we want a model to return $f = X^2$ to us. However I am going to be perverse and assume I don't actually know that, and instead give it a ton of features and ask the model to find me a best fit of those. The features I am going to choose are $(1, X, X^2, X^3 \\ldots X^{10})$, where the superscript without the brackets means I am raising it to the power or something. We would like to see whether we get $a_2 \\approx 1$, and $a_{j\\neq 2} \\approx 0$.\n",
    "\n",
    "To use the linear model formalism I am going to manually create the features via generating an $X$ that looks like\n",
    "\\begin{equation*}\n",
    "    \\begin{pmatrix}\n",
    "        x_0^0 & x_0^1 & x_0^2 & \\cdots & x_0^{10} \\\\\n",
    "        x_1^0 & x_1^1 & x_1^2 & \\cdots & x_1^{10} \\\\\n",
    "        \\vdots & \\vdots & & & \\vdots\\\\\n",
    "        x_{99}^0 & x_{99}^1 & x_{99}^2 & \\cdots & x_{99}^{10}\n",
    "    \\end{pmatrix},\n",
    "\\end{equation*}\n",
    "where $x_i^j$ denotes data at position $i$ raised to the power $j$ ($i$-th sample and $j$-th feature). If you remember from your linear algebra, we are trying to get\n",
    "\\begin{equation*}\n",
    "    \\begin{pmatrix}\n",
    "        x_0^0 & x_0^1 & x_0^2 & \\cdots & x_0^{10} \\\\\n",
    "        x_1^0 & x_1^1 & x_1^2 & \\cdots & x_1^{10} \\\\\n",
    "        \\vdots & \\vdots & & & \\vdots\\\\\n",
    "        x_{99}^0 & x_{99}^1 & x_{99}^2 & \\cdots & x_{99}^{10}\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "        a_0 \\\\ a_1 \\\\ \\vdots \\\\ a_{10}\n",
    "    \\end{pmatrix}\n",
    "    =\n",
    "    \\begin{pmatrix}\n",
    "        y_0 \\\\ y_1 \\\\ \\vdots \\\\ y_{99}\n",
    "    \\end{pmatrix}.\n",
    "\\end{equation*}\n",
    "Notice the problem here the matrices are not square, and we are in the over-determined regime. The shape above is good for `sklearn` because we want the array in `(n_samples, n_features)`: here we have 100 rows (100 samples), and 11 columns (11 features as degree 10 polynomial + a constant).\n",
    "\n",
    "Just going to use the default options from the linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585ae43-aed7-4153-b63d-4d798e30c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input features and do a load of fittings\n",
    "deg = 10\n",
    "X = np.zeros((n, deg+1))\n",
    "for j in range(deg+1):\n",
    "    X[:, j] = X_vec**j\n",
    "\n",
    "lin_reg = LinearRegression().fit(X, Y)\n",
    "ridge = Ridge().fit(X, Y)\n",
    "lasso = Lasso().fit(X, Y)\n",
    "e_net = ElasticNet().fit(X, Y)  # I can't get it to converge but it will return an answer\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(X_vec, Y, 'ko', alpha=0.7, label=\"data\")\n",
    "ax.plot(X_vec, lin_reg.predict(X),\n",
    "        label=f\"lin_reg, MSE = {mean_squared_error(Y, lin_reg.predict(X)):.4f}\")\n",
    "ax.plot(X_vec, ridge.predict(X),\n",
    "        label=f\"ridge,   MSE = {mean_squared_error(Y, ridge.predict(X)):.4f}\")\n",
    "ax.plot(X_vec, lasso.predict(X),\n",
    "        label=f\"lasso,   MSE = {mean_squared_error(Y, lasso.predict(X)):.4f}\")\n",
    "ax.plot(X_vec, e_net.predict(X),\n",
    "        label=f\"e_net,   MSE = {mean_squared_error(Y, e_net.predict(X)):.4f}\")\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$Y$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f499b6-b2a7-48a4-9425-e3b2f386be03",
   "metadata": {
    "id": "02f499b6-b2a7-48a4-9425-e3b2f386be03"
   },
   "source": [
    "On mine it says `ElasticNet` did not converge, although as you can see the skill is actually ok.\n",
    "\n",
    "It is of interest also to have a look at the coefficients obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59721a3-f65e-4d32-aad7-d3c6d0e98ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the coefficients of the models or the loading values?\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.bar(np.arange(deg+1), lin_reg.coef_)\n",
    "ax.set_title(\"linear regression\")\n",
    "ax.set_ylabel(r\"$a_j$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.bar(np.arange(deg+1), ridge.coef_)\n",
    "ax.set_title(\"ridge\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.bar(np.arange(deg+1), lasso.coef_)\n",
    "ax.set_title(\"lasso\")\n",
    "ax.set_xlabel(r\"deg\")\n",
    "ax.set_ylabel(r\"$a_j$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.bar(np.arange(deg+1), e_net.coef_)\n",
    "ax.set_title(\"elastic net\")\n",
    "ax.set_xlabel(r\"deg\")\n",
    "ax.grid()\n",
    "\n",
    "plt.tight_layout(pad=0.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a4832a-3c34-4324-9148-42bc94c2ae7e",
   "metadata": {
    "id": "b5a4832a-3c34-4324-9148-42bc94c2ae7e"
   },
   "source": [
    "Several things to notice here:\n",
    "\n",
    "* Linear regression and ridge both use $L^2$ in the loss, and the result model coefficients are noticeably non-zero across the board (smaller in ridge though). This is not seen in Lasso and elastic net, which include a $L^1$ term in the loss.\n",
    "* Ridge is really the only one that predicts the loading in the degree 2 term ($a_2$), although there is loading everywhere else.\n",
    "* Interestingly Lasso predicts a dominant loading at degree 4. This is actually plausible, because for small enough $X$ values (default here being from $0$ to $4$) $X^2$ and $X^4$ can be argued to not be that dissimilar to each other (and analogously argued for other even powers).\n",
    "\n",
    "Below shows how they perform poorly in extrapolation, demonstrating a signficant issue with over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c960e6b6-0836-4d12-b0f5-059f567056d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a single extrapolation\n",
    "x0 = 7.0\n",
    "X_extra = np.zeros((1, deg+1))\n",
    "for j in range(deg+1):\n",
    "    X_extra[:, j] = x0**j\n",
    "\n",
    "# compute some errors\n",
    "print(f\"target value at x0         = {x0**2:.4f}\")\n",
    "print(f\"lin_reg extrapolated value = {lin_reg.predict(X_extra)[0]:.6f}\")\n",
    "print(f\"ridge   extrapolated value = {ridge.predict(X_extra)[0]:.6f}\")\n",
    "print(f\"lasso   extrapolated value = {lasso.predict(X_extra)[0]:.6f}\")\n",
    "print(f\"e_net   extrapolated value = {e_net.predict(X_extra)[0]:.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade2273-0e96-4abf-8a1a-02fffe866e4a",
   "metadata": {
    "id": "0ade2273-0e96-4abf-8a1a-02fffe866e4a"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> See how the above behaviours change if you expand decreasing the training range (e.g. instead of `X_vec = (0, 4)` try `(0, 8)`, or `(4, 8)`, or randomly pick some values to do the model fitting in some specified region etc.)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Introduce some variability and do an ensemble of fittings (easiest is via `train_test_split`, although you can do the random picking within a region specified above also). Quantify the robustness of the above properties (e.g. do box-plots/histograms over the ensemble of the loading values, errors etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed137e9-3ecb-4660-b577-3466e7c47df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52af85f6-745b-4441-87fe-f02a86edd255",
   "metadata": {
    "id": "52af85f6-745b-4441-87fe-f02a86edd255"
   },
   "source": [
    "---\n",
    "## b) Dimension reduction (cf. data feature identification)\n",
    "\n",
    "An observation with the above linear model example is that you could in principle get reasonably good model skill without needing a lot of features (e.g. the $L^1$ penalised models). That's a good thing presumably because the resulting model has less complexity that should mitigating over-fitting to some degree, and it also speeds up the training stage in principle (because your problem size is smaller). Ways to identify the most useful \"features\" for model training/skill is thus of interest: this would be ***dimension reduction*** in that out of the possible space of results allowed by all data, you want to find a useful ***feature space*** that is presumably lower dimension and less complex than the original space.\n",
    "\n",
    "Asking for the *most* skillful one is probably quite a difficult question to answer. We will instead look for ways to simply find feature spaces that are simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6774f8-1521-4280-92b8-13c7e23edadf",
   "metadata": {
    "id": "4d6774f8-1521-4280-92b8-13c7e23edadf"
   },
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "(Or sometimes Empirical Orthogonal Functions (EOF)s particularly when space dimensions are involved.) \n",
    "\n",
    "This we already encountered in OCES 3301, and is essentially finding linear combinations of the features that maximises the variance explained in the data. Again, these are optimisation problems, just with a different choice of loss function.\n",
    "\n",
    "For example, for the `penguins` data, instead of the feature space being `(bill_length, bill_depth, flipper_length, body_mass)`, it could be that (say) `(bill_length + 2 body_mass, flipper_length + body_mass - bill_depth)` actually explains the data the best. This may not make sense scientifically, but that's not the point: PCA only seeks to maximise variance explained, variance is related to correlations, and correlation does not imply causation.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Have a think why there is no real reason to do a PCA as such for the polynomial fitting problem above.\n",
    "\n",
    "Although some of this was already done in OCES 3301, the below is included mostly for completeness.\n",
    "\n",
    "> NOTE: If you know this stuff, then PCA is just a SVD ranked by the magnitude of the singular values. The singular vectors are the features of interest and are orthogonal by construction (hence the O in EOF). When the matrix is square then this is related to diagonalisation of the covariance matrix in terms of its eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c163a-922b-424d-a6b6-23acf0a7a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# load the penguin data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954e353-dd8e-4055-9783-563dc30d8419",
   "metadata": {
    "id": "b954e353-dd8e-4055-9783-563dc30d8419"
   },
   "source": [
    "First going to do simple (and arguably unnecessary) case of two variables to two other variables, in which case we are simply dealing with a co-ordinate transformation.\n",
    "\n",
    "> NOTE: The thing with PCA is that it does depend quite strongly on choice of pre-processing. By default you should do data scaling before you do PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d54d39f-a984-4d40-9275-69f54bafda9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA of two variables to two other variables (i.e. a co-ordinate transformation)\n",
    "features = ['bill_length_mm', 'bill_depth_mm']\n",
    "X = df[features].values\n",
    "\n",
    "# scale and fit the data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# input in scaled space\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X_scaled[df[\"species\"] == species, 0],\n",
    "               X_scaled[df[\"species\"] == species, 1],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.annotate(\"\",\n",
    "            pca.mean_ + pca.components_[0] * 2 * np.sqrt(pca.explained_variance_[0]),\n",
    "            pca.mean_,\n",
    "            arrowprops=dict(color=\"green\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.annotate(\"\",\n",
    "            pca.mean_ + pca.components_[1] * 2 * np.sqrt(pca.explained_variance_[1]),\n",
    "            pca.mean_,\n",
    "            arrowprops=dict(color=\"blue\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(f\"scaled {features[0]}\")\n",
    "ax.set_ylabel(f\"scaled {features[1]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "\n",
    "# inputs in PCA basis\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X_pca[df[\"species\"] == species, 0],\n",
    "               X_pca[df[\"species\"] == species, 1],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.annotate(\"\",\n",
    "            [2 * np.sqrt(pca.explained_variance_[0]), 0],\n",
    "            [0, 0],\n",
    "            arrowprops=dict(color=\"green\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.annotate(\"\",\n",
    "            [0, 2 * np.sqrt(pca.explained_variance_[1])],\n",
    "            [0, 0],\n",
    "            arrowprops=dict(color=\"blue\", arrowstyle='->', linewidth=2, shrinkA=0, shrinkB=0))\n",
    "ax.set_xlabel(f\"PC 1 (var explained = {pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
    "ax.set_ylabel(f\"PC 2 (var explained = {pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
    "ax.grid(lw=0.5, zorder=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e9e4b-a766-4f8d-a1af-c2a499fa4dfc",
   "metadata": {
    "id": "ae9e9e4b-a766-4f8d-a1af-c2a499fa4dfc"
   },
   "source": [
    "Above looks like a rotation, a flip about the axis defined defined by the blue arrow, and then some mild scaling to me.\n",
    "\n",
    "A more interesting case is if I dump everything in, which would be a reduction of four dimensions to two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aac29e-747a-4b4f-807b-c72ff18db271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# throw everything into the PCA (in this case ignore species)\n",
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[features].values\n",
    "\n",
    "# scale data, fit PCA (do 2 componens here), transform data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X_pca[df[\"species\"] == species, 0],\n",
    "               X_pca[df[\"species\"] == species, 1],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"PC 1 (var explained = {pca.explained_variance_ratio_[0]*100:.2f}%)\")\n",
    "ax.set_ylabel(f\"PC 2 (var explained = {pca.explained_variance_ratio_[1]*100:.2f}%)\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9dceba-7f52-402a-b16b-544ae3d4cc35",
   "metadata": {
    "id": "cf9dceba-7f52-402a-b16b-544ae3d4cc35"
   },
   "source": [
    "Note the percentages in this case do not sum to 100%, because we have discarded two dimensions. We can query the percentages and also the feature combinations as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cf3cd6-59f2-4e23-9aa8-986ee5723e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print to screen the percentages and PC constituents\n",
    "\n",
    "for i in range(len(pca.explained_variance_ratio_)):\n",
    "    print(f\"PC {i+1} explains {pca.explained_variance_ratio_[i]*100:.2f}% of variance\")\n",
    "print(\" \")\n",
    "for i in range(len(pca.explained_variance_ratio_)):\n",
    "    print(f\"\"\"PC {i+1} = {pca.components_[i, 0]:.4f} * {features[0]} +\n",
    "       {pca.components_[i, 1]:.4f} * {features[1]} +\n",
    "       {pca.components_[i, 2]:.4f} * {features[2]} +\n",
    "       {pca.components_[i, 3]:.4f} * {features[3]}\"\"\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2603265b-e33c-45b7-9f69-54798fecd885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "543bbcb2-a76e-4b7e-ad18-246f2a2c1529",
   "metadata": {
    "id": "543bbcb2-a76e-4b7e-ad18-246f2a2c1529"
   },
   "source": [
    "### Locally Linear Embedding and $t$-SNE\n",
    "\n",
    "The original paper to ***Locally Linear Embedding*** is [here](https://www.science.org/doi/10.1126/science.290.5500.2323). This you can think of as PCAs done locally, and these local regions are patched together to form a best fit global reduction. Below just demonstrates how these are used with the penguin data, although it may make more sense after next lecture with a better example where it will significantly affect the results from clustering.\n",
    "\n",
    "> NOTE: This I think is a bit like in differential geometry where you find a whole load of local co-ordinate charts, and then patch it up to find a global atlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec968c-c999-45b4-84e1-171ea585ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[features].values\n",
    "\n",
    "lle = LocallyLinearEmbedding()  # default of 2 components (so 2 dimensions)\n",
    "X = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "X_lle = lle.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X_lle[df[\"species\"] == species, 0],\n",
    "               X_lle[df[\"species\"] == species, 1],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(r\"$\\tilde{X}_1$\")\n",
    "ax.set_ylabel(r\"$\\tilde{X}_2$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bfa00-1020-4f4b-ae07-71a3154113b6",
   "metadata": {
    "id": "882bfa00-1020-4f4b-ae07-71a3154113b6"
   },
   "source": [
    "This seems to suggest there is a projection that can strongly separate out the Gentoo penguins from others, which may help in classification tasks.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> The default option above is `standard`, but there are other options that can be specified (e.g. `ltsa`, `hessian`, `modified`). See [here](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding) for what those mean and try a few of those out. Might also be good to revisit after the next session.\n",
    "\n",
    "***$t$-distributed Stochastic Neighbor Embedding*** ($t$-SNE) tries to do something similar but instead assigns how close data is in the original space by Gaussian probabilities, and the closeness in the transformed space by Student $t$-distributions (cf. `05_hypothesis_testing` in OCES 3301). Together this aids in looking for a projection that is sensitive to local data structures. While the former method is better for continuously distributed samples, the method here might be better for local samples.\n",
    "\n",
    "> NOTE: $t$-SNE can be slow.\n",
    ">\n",
    "> The default has `TSNE(init=\"pca\")`, which helps with stability. `init=\"random\"` is the other option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3be5220-dc24-4612-8d1a-28e7b30b3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[features].values\n",
    "\n",
    "tsne = TSNE()  # default of 2 components (so 2 dimensions)\n",
    "X = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X_tsne[df[\"species\"] == species, 0],\n",
    "               X_tsne[df[\"species\"] == species, 1],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(r\"$\\tilde{X}_1$\")\n",
    "ax.set_ylabel(r\"$\\tilde{X}_2$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c6586-63f2-4bbc-888f-1018450c3c27",
   "metadata": {
    "id": "835c6586-63f2-4bbc-888f-1018450c3c27"
   },
   "source": [
    "So note here that we get fairly well separated clusters of points (with a little bit of mixing between Adelie and Chinstrap), which will presumably aid in clustering and/or classification tasks.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Switch off data standardisation and see what happens. Is this expected?\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Change over to `init=\"random\"` and see how the clustering changes. You may want to run multiple instances to check for robustness and/or wellness of separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c6faf1-47d3-4b02-bedc-c4fe18393a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e76ced1-6d78-486f-82b6-a7d54dcbf9d8",
   "metadata": {
    "id": "5e76ced1-6d78-486f-82b6-a7d54dcbf9d8"
   },
   "source": [
    "### Demonstration: Eigencat\n",
    "\n",
    "A fun application of PCA would be the [eigenpets](https://bioramble.wordpress.com/2015/09/01/pca-part-5-eigenpets/) example (cf. [eigenfaces](https://en.wikipedia.org/wiki/Eigenface)). Given a collection of cat images, what is the \"pattern\" that explains most of the data in terms of maximising the variance? This has uses in things like image/facial recognition: you might imagine certain species of cats have a particular signature in some modes. More generally, this might be useful in ***classification*** tasks where you are trying to identify species and the like. Having these features may also be helpful for reconstruction of broken images.\n",
    "\n",
    "The idea is quite simple: instead of a set of images being `(n_samples, width, height)`, we simply reshape/flatten it into `(n_samples, pixels)`; this is what was done for computing EOFs in OCES 3301. Then you dump it into the PCA routine as usual. We first have a load and plot out some of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77e74c-7e95-447c-ab66-40e8a27a348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't read the headers\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/cat.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X = df.values\n",
    "print(f\"data array shape is {X.shape}\")\n",
    "print(\" \")\n",
    "\n",
    "# generate a list of 25 indices (generate full list, shuffle, select first 25, so no repeats)\n",
    "ind = np.arange(80)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "# sample show (on-the-fly reshape data)\n",
    "fig = plt.figure(figsize=(8, 8.5))\n",
    "for i in range(25):\n",
    "    ax = plt.subplot(5, 5, i+1)\n",
    "    ax.imshow(np.reshape(X[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5372af57-d85f-45ff-9937-6e4b76adc2cb",
   "metadata": {
    "id": "5372af57-d85f-45ff-9937-6e4b76adc2cb"
   },
   "source": [
    "Here the raw data has already been flattened and is in the right shape (otherwise just transpose it with `.T`). We will PCA it accordingly, but I won't standardise it in this case (though probably should to remove the mean).\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Have a think why I made that choice, and possibly investigate later what consequences you may have if you do standardise in the usual way (uncomment two of the lines below), or with other choices of standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf06f3-e1ca-44d9-80bc-e5fae40fbc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if standardising then images are less sharp (Q: why?)\n",
    "# scale = StandardScaler()\n",
    "# X = scale.fit_transform(X)\n",
    "pca = PCA(n_components=40)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# should be (n_EOF, pixels)\n",
    "print(f\"PCA components with shape {pca.components_.shape}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba14882-e8c5-4da5-a67f-53d7c0939f0c",
   "metadata": {
    "id": "bba14882-e8c5-4da5-a67f-53d7c0939f0c"
   },
   "source": [
    "The transformed data should be in the shape `(n_EOF, pixels)`. We can reshape and plot them out accordingly.\n",
    "\n",
    "Note that I am calling the patterns the EOFs, while the loadings I am going call the PCs. In this case the mathematical expresion might be less ambiguous:\n",
    "\\begin{equation*}\n",
    "    \\mathrm{Image}(\\mathrm{pixels}) = \\sum_i \\mbox{PC}_i \\times \\mbox{EOF}_i(\\mathrm{pixels}),\n",
    "\\end{equation*}\n",
    "and each of the cat image differs in the $\\mbox{PC}_i$.\n",
    "\n",
    "> NOTE: If you are mathematically inclined, EOFs are just the basis of singular vectors and the PCs are my co-ordinates or coefficients in this basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5454abbd-53b1-43cc-8067-5ccfc56db960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eigencats\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "for i in range(15):\n",
    "    ax = plt.subplot(3, 5, i+1)\n",
    "    ax.imshow(np.reshape(pca.components_[i, :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"var = {pca.explained_variance_ratio_[i] * 100:.2f}%\")\n",
    "    ax.set_ylabel(f\"EOF {i+1}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd5797d-36f8-40c2-892a-d54c383b1eef",
   "metadata": {
    "id": "dbd5797d-36f8-40c2-892a-d54c383b1eef"
   },
   "source": [
    "Some observations:\n",
    "\n",
    "* Here you might think EOF1 probably measures colour: if the associated PC1 is positive then we are dealing with lighter coloured cats probably, and vice-versa for negative PC1s.\n",
    "* EOF2 and 3 might be about facial patterns.\n",
    "* EOF4 and 5 might be about \"chubby-ness\" of face?\n",
    "* Extra details on the later patterns.\n",
    "\n",
    "These are all speculations of course, because again PCA is statistical and only finds the correlations, whereas it is me who attributes the causality.\n",
    "\n",
    "Below are some selected examples of cats with increasing numbers of EOFs added, i.e. I am doing\n",
    "\\begin{equation*}\n",
    "    \\mathrm{Reconstructed\\ Image}(\\mathrm{pixels}) \\approx \\sum_i^m \\mbox{PC}_i \\times \\mbox{EOF}_i(\\mathrm{pixels})\n",
    "\\end{equation*}\n",
    "with increasing $m$. I am also going to plot out the corresponding loading values $\\mbox{PC}_i$ as a graph for the selected samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf690a8-5605-4e09-8fcc-1f31ca497fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample decomposition (X_pca above is the PC)\n",
    "\n",
    "cat_num = [33, 44, 61]\n",
    "pca_ind = [5, 10, 20, 30, 40]\n",
    "\n",
    "# do some plots to show decomposition\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "\n",
    "count = 0\n",
    "\n",
    "for j in range(3):\n",
    "    cat_ind = cat_num[j]\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(3, 6, (j*6)+i+1)\n",
    "        # plot original image\n",
    "        if i % 6 == 0:\n",
    "            ax.imshow(np.reshape(X[cat_ind, :], (64, 64)).T, cmap=\"gray\")\n",
    "            ax.set_title(f\"#{cat_num[j]}\")\n",
    "        # plot the reconstruction with increasing number of PCs\n",
    "        else:\n",
    "            # sum to get (scaled) reconstruction via sum_i (PC_i * EOF_i)\n",
    "            ind = pca_ind[i-1]\n",
    "            dum = np.sum(X_pca[cat_ind, :ind] * pca.components_[:ind, :].T, axis=-1)\n",
    "            ax.imshow(np.reshape(dum, (64, 64)).T, cmap=\"gray\")\n",
    "            ax.set_title(f\"to EOF = {pca_ind[i-1]}\")\n",
    "            count += 1\n",
    "\n",
    "        ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6kOQN_0TNd9V",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: the shape !!\n",
    "print('Shape of X: ', X.shape)\n",
    "print(\"Shape of X_pca: \", X_pca.shape)\n",
    "print(\"Shape of EOFs: \", pca.components_.shape)\n",
    "# reconstruction is similar to this (add mean for reverse shifting / centering)\n",
    "X_recon = np.dot(X_pca, pca.components_) + pca.mean_[np.newaxis, :]\n",
    "print(\"Matrix multiplication, Shape(X_pca * EOFs) = \", (X_recon).shape)\n",
    "\n",
    "# wont be able to exactly reconstruct the original images, try plotting to see\n",
    "# plt.imshow(np.reshape(X[0, :], (64, 64)).T, cmap=\"gray\")\n",
    "# plt.imshow(np.reshape(X_recon[0, :], (64, 64)).T, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d20ea3-a9ef-4222-9600-496e7becd9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the PC decomposition\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "for cat in cat_num:\n",
    "    ax.plot(np.arange(1, X_pca.shape[-1]+1), X_pca[cat, :], label=f\"cat #{cat}\")\n",
    "ax.set_xlabel(\"PC\"); ax.set_ylabel(\"magnitude\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db73c6b9-a735-4968-9b2d-2ef533114a00",
   "metadata": {
    "id": "db73c6b9-a735-4968-9b2d-2ef533114a00"
   },
   "source": [
    "Perhaps as expected we see that the black and white cat differs primarily in PC1. The tabby cat has a different signature on the other hand. This may aid in the classification of images via a machine learning algorithm in the following ways:\n",
    "\n",
    "* The different signatures might be distinguishing feature that allows separation of labels (see session 05 when we talk about classification tasks).\n",
    "* Instead of throwing the above images (which is $64^2 = 4096$ features, i.e. high dimension feature space), we are instead doing learning in a much lower dimension space (in this case 40), which may be good in mitigation of over-fitting.\n",
    "\n",
    "### A fun demonstration and also a warning\n",
    "\n",
    "I am going to go nuts and load an even larger dataset and do the same thing. Here I am going to do a PCA with 2000 components (again not standardising). This takes a bit of time (no more than 10 seconds on my laptop, around 20 seconds in Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a2085d-83d9-48c4-aa3a-32fcf4b5ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2000 images here; data ordering with the same convention\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat_bw_enlarged.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/cat_bw_enlarged.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path, header=None).T # make \"features\" the axis=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44289f6-f9e8-4f0a-81b3-a10b8ee0f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expanded eigencats\n",
    "X = df.values\n",
    "pca = PCA(n_components=2000)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "for i in range(15):\n",
    "    ax = plt.subplot(3, 5, i+1)\n",
    "    ax.imshow(np.reshape(pca.components_[i, :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"var = {pca.explained_variance_ratio_[i] * 100:.2f}%\")\n",
    "    ax.set_ylabel(f\"EOF {i+1}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cc10e0-9607-47f4-bb69-7ba9a07cb973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample decomposition (X_pca above is the PC)\n",
    "\n",
    "cat_num = [2, 5, 31]\n",
    "pca_ind = [50, 100, 200, 500, 1000]\n",
    "\n",
    "# do some plots to show decomposition\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "for j in range(3):\n",
    "    cat_ind = cat_num[j]\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(3, 6, (j*6)+i+1)\n",
    "        # plot original image\n",
    "        if i % 6 == 0:\n",
    "            ax.imshow(np.reshape(X[cat_ind, :], (64, 64)).T, cmap=\"gray\")\n",
    "            ax.set_title(f\"#{cat_num[j]}\")\n",
    "        # plot the reconstruction with increasing number of PCs\n",
    "        else:\n",
    "            # sum to get (scaled) reconstruction via sum_i (PC_i * EOF_i)\n",
    "            ind = pca_ind[i-1]\n",
    "            dum = np.sum(X_pca[cat_ind, :ind] * pca.components_[:ind, :].T, axis=-1)\n",
    "            ax.imshow(np.reshape(dum, (64, 64)).T, cmap=\"gray\")\n",
    "            ax.set_title(f\"to EOF = {pca_ind[i-1]}\")\n",
    "\n",
    "        ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d7d65-3afb-48c5-a269-b9c82f1421a6",
   "metadata": {
    "id": "b53d7d65-3afb-48c5-a269-b9c82f1421a6"
   },
   "source": [
    "So notice that this larger dataset is not as clean in that we have cat images that are:\n",
    "\n",
    "* Missing features (the first one isn't showing the ears).\n",
    "* Not centred (includes whole fact but rotated somewhat).\n",
    "* Not focused (showing whole body as well as other presumably irrelevant features).\n",
    "\n",
    "By enlarging the dataset with extra details I am massively increasing the dimension and extent of my feature space, which may or may not be a good thing. With this I can fit whatever I like to the trained set, as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x-SdN_e4-30O",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "targets_path = {\n",
    "    \"miffy_gormless\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/miffy_gormless.jpg\",\n",
    "    \"blauhaj\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/blauhaj.jpg\",\n",
    "    \"clippy\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/clippy.jpg\",\n",
    "}\n",
    "\n",
    "targets = {}\n",
    "\n",
    "for file_name, file_url in targets_path.items():\n",
    "    response = requests.get(file_url)\n",
    "    targets[file_name] = Image.open(BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff4fce-aabd-40a7-a127-6a189bc2a283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit some doofuses\n",
    "pca_ind = [50, 100, 200, 1000, 2000]\n",
    "\n",
    "# do some plots to show decomposition\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "j = 0\n",
    "for key in targets:\n",
    "    X = np.array(targets[key]).T.flatten()\n",
    "    X_pca = pca.transform(X.reshape(1, -1))\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(3, 6, (j*6)+i+1)\n",
    "        # plot original image\n",
    "        if i % 6 == 0:\n",
    "            ax.imshow(np.reshape(X, (64, 64)).T, cmap=\"gray\")\n",
    "            if j == 0:\n",
    "                ax.set_title(\"orig\")\n",
    "        # plot the reconstruction with increasing number of PCs\n",
    "        else:\n",
    "            # sum to get (scaled) reconstruction via sum_i (PC_i * EOF_i)\n",
    "            ind = pca_ind[i-1]\n",
    "            dum = np.sum(X_pca[0, :ind] * pca.components_[:ind, :].T, axis=-1)\n",
    "            ax.imshow(np.reshape(dum, (64, 64)).T, cmap=\"gray\")\n",
    "            ax.set_title(f\"to EOF = {pca_ind[i-1]}\")\n",
    "\n",
    "        ax.set_xticks([]); ax.set_yticks([]);\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64daea6a-9447-42bf-a42e-ef5ba82a0032",
   "metadata": {
    "id": "64daea6a-9447-42bf-a42e-ef5ba82a0032"
   },
   "source": [
    "With the expanded set I can fit:\n",
    "\n",
    "* A cat that wasn't in the sample.\n",
    "* An animal that's not a cat.\n",
    "* Not even an animal.\n",
    "\n",
    "The first is good but the latter are not. You can imagine I train a classifier, then it might identify the bottom one as a \"siamese cat\".\n",
    "\n",
    "A take home message here may be that more data is not always good: it does expose the models to the wider pdf, but it also allows for more possibility of hallucinations with the extra complexity. Just like \"practice makes perfect\" is really \"PERFECT practice makes perfect\", we should really have \"more GOOD data is good\".\n",
    "\n",
    "> NOTE: There are of course data choices that can be used to mitigate these hallucinations (a simple one would be to add `not cat` labels). There are also model design choices that can be taken as well, but those will be beyond the scope of this course.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Exercise in data processing: Find your favourite image and try and see if you can expand it with the enlarged eigenpets collection. (You will need to resize and grayscale it.)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Consider doing locally linear embedding or $t$-SNE instead of PCA in the above dataset(s). Probably don't do it on the extended cat set particularly with $t$-SNE, because that will probably kill your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3133a48e-7053-44cb-a3e7-9d314a57cd96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d56f34c-7818-42d4-8ea6-d9018a5a04d2",
   "metadata": {},
   "source": [
    "---\n",
    "## c) Clustering\n",
    "\n",
    "Given unlabelled data it may be of interest to label them according to how \"close\" the data is to each other. ***Clustering*** is a means to do this\". One reason to do clustering is that you may want to have different regression models or train regression models differently depending on the data. Piggy-backing on some previous material:\n",
    "\n",
    "1. From the `penguins` example you may remember that Gentoo has somewhat different properties compared to the other two. Models trained on everything is not expected to work that well (and it doesn't). In that case it is obvious to split by `species` (because that already comes with the data), but you could imagine you got unlabelled data instead. Then you'd want to cluster first and then possibly train the model based on the labelled data.\n",
    "2. You may want to reduce dimension to separate out data, then cluster, then train models accordingly. The labels also provide extra features that could be fed to the model.\n",
    "\n",
    "> NOTE: The above points are things you should probably consider doing certainly if you are going to use Argo data for assignment 1.\n",
    "\n",
    "Clustering algorithms work by having some measure of \"closeness\", whatever you want that to mean. Going to demonstrate roughly how clustering algorithms might work and some subtleties related to the measure of \"closeness\".\n",
    "\n",
    "> NOTE: This is again a problem in the choice of \"metric\".\n",
    "\n",
    "### $K$-means clustering\n",
    "\n",
    "A simple case for clustering that we can demonstrate reasonably easily is ***$K$-means clustering***. I am going to demonstrate how this works in 2d manually first, and the generalisation to arbitrary dimensions and use in `sklearn` should be fairly obvious.\n",
    "\n",
    "### 0. Create some data first\n",
    "\n",
    "Below I am going to artificially create three blobs of data (and kicked around by some noise). By eye it should be suggestive that there would be three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5083a8a-c801-44fd-ad6d-9d98f2a4bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create cluster centers (in a triangle in this case), then add noise to them\n",
    "X = np.concat((np.ones((30, 2)) * [ 0.0, 1.0],\n",
    "               np.ones((30, 2)) * [ 1.0, 0.0],\n",
    "               np.ones((30, 2)) * [-1.0, 0.0]))\n",
    "X += 0.25 * np.random.randn(X.shape[0], 2)  # made this small-ish so clusters show up\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(X[:, 0], X[:, 1], 'x')\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bb55e-f2b5-4e34-882f-ac67200e85af",
   "metadata": {},
   "source": [
    "### 1. Specify number of clusters $K$ and location of their initial centroid\n",
    "\n",
    "A user defined parameter (and thus a model hyper-parameter) is the number of clusters $K$ (hence $K$-means). Step 0 of plotting out the data suggests to me maybe I should choose $K = 3$, so I do that and in this case take an educated guess where the centres of the clusters (the **centroid**) are; we will consider the randomly initialised case later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c19c0-fae9-4e4b-8f62-d7d763e74fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise centroid guesses and plot where these are\n",
    "centroid = np.array([[0.0, 0.5], [0.5, 0.2], [-0.5, -0.2]]) # centroid[number, coord]\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "ax.plot(X[:, 0], X[:, 1], 'x')\n",
    "for i in range(centroid.shape[0]):\n",
    "    ax.plot(centroid[i, 0], centroid[i, 1], 's', markersize=10, label=f\"{i}\")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc3ce4-f731-4753-9cc2-c519f0d83064",
   "metadata": {},
   "source": [
    "### 2. Label the clusters according to how close they are to the centroids\n",
    "\n",
    "This depends on the choice of distance, and the usual (but not the only) choice is to take the $L^2$ distance (sometimes called ***Euclidean distance***). Below I do just that although I ignore the square root procedure: if you had units you would need a square root to get a distance, but the magnitude ordering is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f7b6d-afaa-4ee1-b8e0-25c11d8bd101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate by computing distances and assigning labels to points closest to centroids\n",
    "\n",
    "# L2 or root-mean-square distance (the root is not actually important here)\n",
    "dist = np.sum((X - centroid[0, :]) ** 2, axis=-1)  # sum along the co-ord dimension\n",
    "label = np.zeros(X.shape[0])  # set everything to belong to zero for now\n",
    "\n",
    "for i in range(1, centroid.shape[0]):\n",
    "    # compute new distance, and update baseline references as appropriate\n",
    "    dist_dum = np.sum((X - centroid[i, :]) ** 2, axis=-1)\n",
    "    label = np.where(dist_dum < dist, i, label)\n",
    "    dist = np.where(dist_dum < dist, dist_dum, dist)\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "for i in range(centroid.shape[0]):\n",
    "    ax.plot(X[label==i, 0], X[label==i, 1], f\"C{i+1}x\")\n",
    "    ax.plot(centroid[i, 0], centroid[i, 1], f\"C{i+1}s\", markersize=10, label=f\"{i}\")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46595f19-113b-4d16-8e22-67a305ced593",
   "metadata": {},
   "source": [
    "### 3. Update the centroid locations, and iterate\n",
    "\n",
    "Give the identified clusters, find the \"centres\" of those. I found it below by doing an optimistion problem (find the $L^2$ minimiser), although I am thinking you could probably just take an average (although something doesn't feel right with the procedre to me...)\n",
    "\n",
    "Then you iterate until some tolerance has been reached (e.g. the centroids don't move that much after a certain point, the classification hasn't changed after enough iterations etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a16635-faa2-43bb-b628-119eddc38284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute new centroid location from classification solving an optimisation problem\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# define the cost function\n",
    "def func(centroid, X):\n",
    "    return np.sum((X - centroid)**2)  # want the L2 minimiser\n",
    "\n",
    "# cycle through the data and update the centroids\n",
    "for i in range(centroid.shape[0]):\n",
    "    res = minimize(func, centroid[i, :], args=X[label==i, :])\n",
    "    centroid[i, :] = res.x\n",
    "\n",
    "# plot new locations\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "for i in range(centroid.shape[0]):\n",
    "    ax.plot(X[label==i, 0], X[label==i, 1], f\"C{i+1}x\")\n",
    "    ax.plot(centroid[i, 0], centroid[i, 1], f\"C{i+1}s\", markersize=10, label=f\"{i}\")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc952b8-8cd2-44fa-bf98-fc2ecf02fa05",
   "metadata": {},
   "source": [
    "### Doing this through `sklearn`\n",
    "\n",
    "A good exercise is to try writing your own $K$-means (mine wraps all of the above in about 30 lines of code). Failing that, we could use `sklearn`. Below code cells demonstrate the relevant syntax with the artificial data above and considers varying the initial guess via specifying `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144243d6-a902-4d6a-b70f-78373914826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "random_states = [1, 2, 4, 7]\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "for j in range(len(random_states)):\n",
    "    # initialise model, fit and then return labels\n",
    "    model = KMeans(n_clusters=3, random_state=random_states[j])\n",
    "    model.fit(X)\n",
    "    label = model.predict(X)  # could have used \"model.fit_predict\"\n",
    "\n",
    "    ax = plt.subplot(2, 2, j+1)\n",
    "    for i in range(model.n_clusters):\n",
    "        ax.plot(X[label==i, 0], X[label==i, 1], f\"C{i+1}x\")\n",
    "        ax.plot(model.cluster_centers_[i, 0], model.cluster_centers_[i, 1], f\"C{i+1}s\",\n",
    "                markersize=10, label=f\"{i}\")\n",
    "    if j > 1:\n",
    "        ax.set_xlabel(r\"$x$\")\n",
    "    if j % 2 == 0:\n",
    "        ax.set_ylabel(r\"$y$\");\n",
    "\n",
    "    ax.set_title(f\"random state = {random_states[j]}\")\n",
    "    ax.grid()\n",
    "\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc423d-468b-46c9-bc6c-f749b3fc6707",
   "metadata": {},
   "source": [
    "The thing to note here is that the clusters themselves are fairly robust although the labels differ. That's a mild annoyance if wanting to use this to compare with already labelled data, but a relabelling will fix that if need be.\n",
    "\n",
    "The algorithm generalises to high dimensions if you think of a piece of data as some point in some high dimensional space with co-ordinates given by each of the numbers in the features dimension, and taking the higher dimension analogue of the $L^2$ distance.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Have a think what would happen to the classification if two data points happen to have identical distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de6285-5c66-482c-959a-f32a1afe7a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68e439a4-20da-40c5-8057-8aca79e6d980",
   "metadata": {},
   "source": [
    "### Revisiting manifold methods (e.g. lle and $t$-SNE above)\n",
    "\n",
    "As noted one needs to then be a bit careful about the choice of distance. In the below example I have two half moons in 2d and an swiss-roll shaped data in 3d, using a tool already in `sklearn` (and is nicer than the examples I manually tried to generate). I am going to apply $K$-means to it as usual.\n",
    "\n",
    "> NOTE: The `make_s_curve` one is a good choice for 3d testing too.\n",
    "\n",
    "<img src=\"https://i.imgur.com/rOgacgN.png\" width=\"450\" alt='moon and swiss roll'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef28ea-3294-455f-a8b4-1a4fe9d3d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons, make_swiss_roll\n",
    "\n",
    "n_samples = 600\n",
    "X_moon, _ = make_moons(n_samples, noise=0.05)     # don't need labels\n",
    "X_swiss, swiss_color = make_swiss_roll(n_samples, noise=0.01)  # don't need swiss_color just yet\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# moon data should have 2 clusters\n",
    "ax = plt.subplot2grid((1, 3), (0, 0))\n",
    "label = KMeans(n_clusters=2).fit_predict(X_moon)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "# S data is one continuous thing, 3 clusters for now\n",
    "ax = plt.subplot2grid((1, 3), (0, 1), colspan=2, projection=\"3d\")\n",
    "label = KMeans(n_clusters=3, random_state=0).fit_predict(X_swiss)\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss[label==i, 0],\n",
    "            X_swiss[label==i, 1],\n",
    "            X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.view_init(azim=-75, elev=9)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "ax.set_anchor(\"W\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7472fdd-6486-46cd-a51b-4d3da8689e91",
   "metadata": {},
   "source": [
    "Both of these demonstrate cases where in this case the ambient measure of distance (in this case $L^2$ in $\\mathbb{R}^2$ and $\\mathbb{R}^3$) is not the relevant distance: the data has a distribution that lives on some surface (which I are going calling the ***manifold***), and it is the ***intrinsic*** measure of distance on the manifold that should be the relevant one, rather than the ***extrinsic*** one inherited from the data manifold being embedded in this case in and $\\mathbb{R}^2$ and $\\mathbb{R}^3$. One could argue for example that:\n",
    "\n",
    "* The two moons are really 1d curves living in $\\mathbb{R}^2$ up to noise.\n",
    "* The Swiss roll is really a 2d surface living in $\\mathbb{R}^3$ up to noise.\n",
    "\n",
    "This is where the previous dimension reduction techniques may be useful: you take the data and try and find a projection onto a lower dimension space to pull out the key features (cf. a co-ordinate \"transformation\"), then in the transformed space your usual \"distance\" may actually make sense, then you do clustering on that.\n",
    "\n",
    "Below demonstrates that with the moon data first, where we expect two clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c52c29b-0492-405c-955f-1e7a23417901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding, TSNE\n",
    "\n",
    "# find 1d projections of 2d data\n",
    "lle = LocallyLinearEmbedding(n_components=1, random_state=777)\n",
    "tsne = TSNE(n_components=1, random_state=777)\n",
    "X_moon_lle = lle.fit_transform(X_moon)\n",
    "X_moon_tsne = tsne.fit_transform(X_moon)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# predicting after LLE transformation\n",
    "label = KMeans(n_clusters=2).fit_predict(X_moon_lle)\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon_lle[label==i, 0], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "# predicting after TSNE transformation\n",
    "label = KMeans(n_clusters=2).fit_predict(X_moon_tsne)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon_tsne[label==i, 0], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "for i in range(2):\n",
    "    ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183ed7c-a9dc-41f4-94ec-cc35d3a3484b",
   "metadata": {},
   "source": [
    "So this one does actually work reasonably well as long as I do a 1d projection, rather than a 2d transformation (which I started off with for some stupid reason in hindsight). The $t$-SNE projection is more robust generally than the LLE one, at least from my empirical testing.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> I find that depending on the initialisation of the data (because the `random_state` is already fixed) the LLE approach is a bit brittle, while the $t$-SNE case is more robust. Consider doing an ensemble of these and test for the robustness.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try a 2d co-ordinate transformation instead. You will need to modify the plotting code somewhat.\n",
    "\n",
    "Below shows an analogous approach on swiss roll data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0d938c-a5d0-4607-8dfc-7c10873c59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 2d projections (because original data is in 3d)\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, random_state=777)\n",
    "tsne = TSNE(n_components=2, random_state=777)\n",
    "X_swiss_lle = lle.fit_transform(X_swiss)\n",
    "X_swiss_tsne = tsne.fit_transform(X_swiss)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "# predicting after LLE transformation\n",
    "label = KMeans(n_clusters=3).fit_predict(X_swiss_lle)\n",
    "ax = plt.subplot2grid((2, 3), (0, 0))\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss_lle[label==i, 0],\n",
    "            X_swiss_lle[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 3), (0, 1), colspan=2, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss[label==i, 0],\n",
    "            X_swiss[label==i, 1],\n",
    "            X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.view_init(azim=-75, elev=9)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "ax.set_anchor(\"W\")\n",
    "\n",
    "# predicting after LLE transformation\n",
    "label = KMeans(n_clusters=3).fit_predict(X_swiss_tsne)\n",
    "ax = plt.subplot2grid((2, 3), (1, 0))\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss_tsne[label==i, 0],\n",
    "            X_swiss_tsne[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 3), (1, 1), colspan=2, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.plot(X_swiss[label==i, 0],\n",
    "            X_swiss[label==i, 1],\n",
    "            X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "ax.view_init(azim=-75, elev=9)\n",
    "ax.set_box_aspect((1, 1, 1))\n",
    "ax.set_anchor(\"W\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f4195-7f94-41ad-acef-316514672c3d",
   "metadata": {},
   "source": [
    "In the swiss roll case it looks like the approaches are able to find segments of the data appropriately, but this is again slightly more brittle depending on initialisations.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Again, consider doing an ensemble of these and test for the robustness.\n",
    "\n",
    "### DBSCAN\n",
    "\n",
    "Instead of $K$-means there is an approach called `DBSCAN`, which asserts that clusters occur as high density of data points separated by gaps that are lower density. This is more a local approach and can also be flexible with the choice of metric (hence it's grouping here), although it does have potential drawbacks in that there are more model hyperparameters (`min_samples` and `eps` to give a measure what is meant by \"dense\"); the number of clusters that falls out is a result of those two choices (and thus we have less control on what drops out). Below shows the two demonstrations with the moons and swiss roll data in the original embedding space.\n",
    "\n",
    "> NOTE: The black dots are **noise points** and has a label of `-1`. These are points close enough to multiple identified clusters that DBSCAN is unable to classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91ee96-b361-432a-b7e6-d28e57705918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# moon data\n",
    "eps_vec = [0.5, 0.15, 0.05]  # keep default of min_samples = 5\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# moon data should have 2 clusters\n",
    "for j in range(len(eps_vec)):\n",
    "    ax = plt.subplot(1, 3, j+1)\n",
    "    label = DBSCAN(eps=eps_vec[j]).fit(X_moon).labels_\n",
    "    # identify number of labels\n",
    "    n_clusters = len(set(label)) - (1 if -1 in label else 0)\n",
    "    for i in range(n_clusters):\n",
    "        ax.plot(X_moon[label==i, 0], X_moon[label==i, 1], f\"C{i+1}x\", alpha=0.7)\n",
    "    ax.plot(X_moon[label==-1, 0], X_moon[label==-1, 1], f\"ko\", alpha=0.3)  # noise points\n",
    "    ax.grid()\n",
    "    ax.set_title(f\"$\\epsilon$ = {eps_vec[j]}\")\n",
    "    if j > 0:\n",
    "        ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5dadb1-c488-47ca-bc22-c2dd44ff4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# swiss roll data\n",
    "eps_vec = [3.5, 2.2, 1.5]  # keep default of min_samples = 5\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# swiss roll data is one continuous cluster\n",
    "for j in range(len(eps_vec)):\n",
    "    ax = plt.subplot(1, 3, j+1, projection=\"3d\")\n",
    "    label = DBSCAN(eps=eps_vec[j]).fit(X_swiss).labels_\n",
    "    # identify number of labels\n",
    "    n_clusters = len(set(label)) - (1 if -1 in label else 0)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        ax.plot(X_swiss[label==i, 0],\n",
    "                X_swiss[label==i, 1],\n",
    "                X_swiss[label==i, 2], f\"C{i+1}x\", alpha=0.7)\n",
    "    ax.plot(X_swiss[label==-1, 0],\n",
    "            X_swiss[label==-1, 1],\n",
    "            X_swiss[label==-1, 2], f\"ko\", alpha=0.3)  # noise points\n",
    "    ax.view_init(azim=-75, elev=9)\n",
    "    ax.set_box_aspect((1, 1, 1))\n",
    "    ax.set_anchor(\"W\")\n",
    "    ax.set_title(f\"$\\epsilon$ = {eps_vec[j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab399c4-7bcf-4965-b489-6ede44b7fcee",
   "metadata": {},
   "source": [
    "In the moon data if `eps` is too large then it just lumps everything together. If it's too small then there are a lot of noise points and tons of clusters. There is thus an optimum `eps` that gives the expected two.\n",
    "\n",
    "In the swiss roll data, it really is one big cluster so in this case `eps` being large-ish is actually ok. For moderate `eps` it is identifying clusters somewhat according to the unrolling of the swiss roll so that's good. If `eps` is too small there is too much noise as before.\n",
    "\n",
    "So cross-validation and exploration of hyperparameters with DBSCAN is important!\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> I kept `min_samples` as the default value of 5. Explore what happens when you change that.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try the above but also with appropriate dimensional reduction / co-ordinate transformation approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1fc95f-15ad-4f81-89f2-60266f1ba5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "240b5d98-0a8f-42fa-8582-415949a00799",
   "metadata": {},
   "source": [
    "### Demonstration: Penguins data\n",
    "\n",
    "We are going to do brute force $K$-means on the penguins data and compare the clustered results with the species ones. Note from the previous lecture that we think the $t$-SNE can do a good job projecting the 4d data down to 2d with a good separation, so we are also going to consider including that as an intermediate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473689d-376b-45b3-82ec-31f62457ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37dd1c-438b-400c-a64c-7295e8191a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out some data\n",
    "X = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values\n",
    "Y = df[\"species\"].values\n",
    "\n",
    "X = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "\n",
    "# instantiate model and fit with all data (no target data is needed since unsupervised)\n",
    "model = KMeans(n_clusters=3, random_state=4)\n",
    "Y_pred = model.fit_predict(X)\n",
    "\n",
    "# do a 3d plot to have a look to see what is going on\n",
    "key1, key2, key3 = \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# predicted clusters\n",
    "ax = plt.subplot(1, 2, 1, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.scatter(X[Y_pred==i, 0],\n",
    "               X[Y_pred==i, 1],\n",
    "               X[Y_pred==i, 2],\n",
    "               label=f\"cluster {i}\",\n",
    "               alpha=0.5)\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)\n",
    "\n",
    "# ground truth\n",
    "ax = plt.subplot(1, 2, 2, projection=\"3d\")\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X[df[\"species\"] == species, 0],\n",
    "               X[df[\"species\"] == species, 1],\n",
    "               X[df[\"species\"] == species, 2],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc228ba-b405-4450-a04d-91f3d2572ad2",
   "metadata": {},
   "source": [
    "By eye there is clearly some mismatch going on if you do this brute force. To be more quantitative about it, if we think in this case the colours are maching exactly (because I chose my `random_state` deliberately for this), then we can simply take `(Adelie, Gentoo, Chinstrap)` to `(0, 1, 2)`, and see how good the labels match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bad4b4-943c-48da-87cd-08aba1ab3c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight forward comparison of labels doesn't work, manually doing a remapping\n",
    "Y_remap = np.zeros(Y.shape)\n",
    "Y_remap[Y == \"Adelie\"] = 0\n",
    "Y_remap[Y == \"Gentoo\"] = 1\n",
    "Y_remap[Y == \"Chinstrap\"] = 2\n",
    "\n",
    "N = len(Y_pred)\n",
    "N_0, N_1, N_2 =len(Y_remap[Y_remap==0]), len(Y_remap[Y_remap==1]), len(Y_remap[Y_remap==2])\n",
    "skill_all = np.sum(Y_pred == Y_remap)\n",
    "skill_0   = np.sum(Y_pred[Y_remap==0] == Y_remap[Y_remap==0])\n",
    "skill_1   = np.sum(Y_pred[Y_remap==1] == Y_remap[Y_remap==1])\n",
    "skill_2   = np.sum(Y_pred[Y_remap==2] == Y_remap[Y_remap==2])\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"    cluster 0: {skill_0} correct out of {N_0} ({skill_0/N_0*100:.2f}%)\")\n",
    "print(f\"    cluster 1: {skill_1} correct out of {N_1} ({skill_1/N_1*100:.2f}%)\")\n",
    "print(f\"    cluster 2: {skill_2} correct out of {N_2} ({skill_2/N_2*100:.2f}%)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e771379e-e98e-417c-823d-f4fb795648eb",
   "metadata": {},
   "source": [
    "It's actually pretty good (it wasn't when I didn't standardise the data). \n",
    "\n",
    "The below does the same but includes a $t$-SNE step; for the same choice of `random_state` I need to do a slightly different label remapping here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bdc54-9077-44fd-88b0-935f22065a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above but with a TSNE step\n",
    "\n",
    "X = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values\n",
    "Y = df[\"species\"].values\n",
    "\n",
    "tsne = TSNE(random_state=4)  # default of 2 components (so 2 dimensions)\n",
    "X = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# instantiate model and fit with all data (no target data is needed since unsupervised)\n",
    "model = KMeans(n_clusters=3, random_state=4)\n",
    "Y_pred = model.fit_predict(X_tsne)\n",
    "\n",
    "# do a 3d plot to have a look to see what is going on\n",
    "key1, key2, key3 = \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"\n",
    "\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "# predicted clusters\n",
    "ax = plt.subplot(1, 2, 1, projection=\"3d\")\n",
    "for i in range(3):\n",
    "    ax.scatter(X[Y_pred==i, 0],\n",
    "               X[Y_pred==i, 1],\n",
    "               X[Y_pred==i, 2],\n",
    "               label=f\"cluster {i}\",\n",
    "               alpha=0.5)\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)\n",
    "\n",
    "# ground truth\n",
    "ax = plt.subplot(1, 2, 2, projection=\"3d\")\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(X[df[\"species\"] == species, 0],\n",
    "               X[df[\"species\"] == species, 1],\n",
    "               X[df[\"species\"] == species, 2],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{key1}\")\n",
    "ax.set_ylabel(f\"{key2}\")\n",
    "ax.set_zlabel(f\"{key3}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6e6dd-9bb0-4480-ae79-62af44819307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# straight forward comparison of labels doesn't work, manually doing a remapping\n",
    "Y_remap = np.zeros(Y.shape)\n",
    "Y_remap[Y == \"Adelie\"] = 2\n",
    "Y_remap[Y == \"Gentoo\"] = 1\n",
    "Y_remap[Y == \"Chinstrap\"] = 0\n",
    "\n",
    "N = len(Y_pred)\n",
    "N_0, N_1, N_2 =len(Y_remap[Y_remap==0]), len(Y_remap[Y_remap==1]), len(Y_remap[Y_remap==2])\n",
    "skill_all = np.sum(Y_pred == Y_remap)\n",
    "skill_0   = np.sum(Y_pred[Y_remap==0] == Y_remap[Y_remap==0])\n",
    "skill_1   = np.sum(Y_pred[Y_remap==1] == Y_remap[Y_remap==1])\n",
    "skill_2   = np.sum(Y_pred[Y_remap==2] == Y_remap[Y_remap==2])\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"    cluster 0: {skill_0} correct out of {N_0} ({skill_0/N_0*100:.2f}%)\")\n",
    "print(f\"    cluster 1: {skill_1} correct out of {N_1} ({skill_1/N_1*100:.2f}%)\")\n",
    "print(f\"    cluster 2: {skill_2} correct out of {N_2} ({skill_2/N_2*100:.2f}%)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6dcbb-15d4-41df-8f0f-1bcbe1aea39a",
   "metadata": {},
   "source": [
    "This provides a mild increase in the clustering skill, which is nice to see.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Explore what happens if I don't standardise. Why and where do you think the problem is? (The former you should know, the latter you might need to think a litte bit.)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> I didn't do `train_test_split`, but consider what happens if I do that, and see how good the label predictions are.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Do an ensemble of these and see how robust the scores are.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Explore above but with different hyperparameters, and evaluate the sensitivity of scores to hyperparameters.\n",
    "\n",
    "There are other `sklearn` metrics one could evaluate things on; look up what those mean if you like.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> What do the numbers below mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b557bf5-725e-4b3e-afc6-f4a51f08adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# X = df[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].values\n",
    "# X_scaled = StandardScaler().fit_transform(X)  # comment to switch of scaling if wanted\n",
    "\n",
    "# try X or X without standardisation, produce different results\n",
    "# X_tsne will be best because it is being properly scaled\n",
    "silhouette = metrics.silhouette_score(X_tsne, Y_pred)\n",
    "db_index = metrics.davies_bouldin_score(X_tsne, Y_pred)\n",
    "ch_index = metrics.calinski_harabasz_score(X_tsne, Y_pred)\n",
    "ari = metrics.adjusted_rand_score(Y_remap, Y_pred)\n",
    "ami = metrics.adjusted_mutual_info_score(Y_remap, Y_pred)\n",
    "\n",
    "# Print the metric scores\n",
    "print(f\"Silhouette Score: {silhouette:.2f}\")\n",
    "print(f\"Davies-Bouldin Index: {db_index:.2f}\")\n",
    "print(f\"Calinski-Harabasz Index: {ch_index:.2f}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {ari:.2f}\")\n",
    "print(f\"Adjusted Mutual Information (AMI): {ami:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327cb5f-69c0-4d99-9bd8-3dbb24665640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4af6743-e688-44fb-91b8-edb41c881c68",
   "metadata": {},
   "source": [
    "---\n",
    "## d) Validation and robustness\n",
    "\n",
    "Given the randomness in the data and train/test split, two things you may want to ask are:\n",
    "\n",
    "1. How robust is the model? (Did you get a model with good skill because you got lucky?)\n",
    "2. How generalisable is your model?\n",
    "\n",
    "ML models are generally prone to ***over-fitting***, in that it is easy to keep adding complexity into the model, which usually improves the skill over the training data, but fail miserably in the testing data. Going to illustrate a few things one could do to mitigate this a bit.\n",
    "\n",
    "If you go back to the first graph in the notebook where I coloured the data by `species`, you will notice that `Gentoo` is well-separated from `Adelie` and `Chinstrap`, so if I do a linear regression of those only those it will give me very different results. Below code demonstrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72586259-781a-4074-9766-7115b3069e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "ind = [3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b155119-0ade-434d-a6ca-01f8bdc0649a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show difference of regression of Gentoo and (Adelie, Chinstrap)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.axes()\n",
    "for species in np.sort(df[\"species\"].unique()):   # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_vars[ind[0]]],\n",
    "               df[df[\"species\"] == species][target_vars[ind[1]]],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "\n",
    "# fit gentoo data\n",
    "X = df[df[\"species\"] == \"Gentoo\"][target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[df[\"species\"] == \"Gentoo\"][target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "model = LinearRegression().fit(X, Y)\n",
    "ax.plot(X, model.predict(X), 'k--', label=f\"Gentoo LOBF, corr = {r_regression(X, Y.ravel())[0]:.4f}\")\n",
    "\n",
    "# fit other data (find NOT gentoos)\n",
    "X = df[df[\"species\"] != \"Gentoo\"][target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[df[\"species\"] != \"Gentoo\"][target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "model = LinearRegression().fit(X, Y)\n",
    "ax.plot(X, model.predict(X), 'm--', label=f\"Not Gentoo LOBF, corr = {r_regression(X, Y.ravel())[0]:.4f}\")\n",
    "\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "le = ax.legend()\n",
    "le.set_bbox_to_anchor([0.7, 0.8, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9aa4f-64bc-4474-9649-0815ded5010e",
   "metadata": {},
   "source": [
    "Notice the correlation of both models are positive (as it should be), and is of different sign to the correlation coefficient if all data is used. The example here is a bit contrived in that fitting to the whole data using a linear model is clearly not the best thing to do given the clear separation by species, but it demonstrates the point that, given the inherent randomness in data selection, there are likely variations in the model that results. If the model depends sensitively on the data then that's probably not a good thing to have.\n",
    "\n",
    "Well just like we don't (at least we shouldn't) do one trial and conclude from it (unless there is strong reason to believe the system is strongly deterministic, e.g. some physical systems), one way to investigate ***robustness/uncertainties*** in model is to do a few of these and take averages if need be. I am going to go further in this case and run a ton of `train_test_split`s, spit out the relevant model parameters, and plot the resulting histogram to get a sense of the pdf.\n",
    "\n",
    "> NOTE: I am effectively training an ***ensemble*** of models here. We will come back to ensembles in e.g. `07_forests` (***random forests*** and ***gradient boosting*** are ensemble methods that can be applied to ***decision trees***)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a09441-d66d-438c-970f-151ebe4823d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a ton of trials of LinearRegression (Y = aX + b) but changing the train_test_split\n",
    "X = df[target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "\n",
    "# number of trials and initialise arrays to dump statistics in\n",
    "n = 100\n",
    "stats = np.zeros((n, 6))  # a, b, MSE on test data, MAE on test data, r, R2\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(n):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    model.fit(X_train, Y_train)\n",
    "    stats[i, 0], stats[i, 1] = model.coef_[0, 0], model.intercept_[0]\n",
    "\n",
    "    Y_pred = model.predict(X_test)\n",
    "    stats[i, 2] = mean_squared_error(Y_test, Y_pred)\n",
    "    stats[i, 3] = mean_absolute_error(Y_test, Y_pred)\n",
    "\n",
    "    stats[i, 4] = model.score(X_test, Y_test)\n",
    "    stats[i, 5] = r_regression(X_train, Y_train.ravel())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dfae8-25b5-469b-a3d1-2a1a8dac4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out the above statistics\n",
    "labels = [r\"$a$\", r\"$b$\", \"MSE\", \"MAE\", r\"$r$\", r\"$R^2$\"]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 4))\n",
    "for j in range(6):\n",
    "    ax = plt.subplot(2, 3, j+1)\n",
    "    ax.hist(stats[:, j])\n",
    "    ax.set_title(labels[j])\n",
    "    ax.grid()\n",
    "    if j % 3 == 0:\n",
    "        ax.set_ylabel(\"freq\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138c716-33e7-49e1-bf04-f86351508fb7",
   "metadata": {},
   "source": [
    "You can compute the mean and standard deviations of these if you want to, but looking at the histogram the resulting model can be argued to be reasonably robust probably (this would actually be easier to see if the data was standardised). That's not surprising because linear regression is simple enough it probably can't do too many insane things. The same cannot be said of other models.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Do the above but scale the data, and convince yourself the same conclusions above effectively holds.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Compute the standard deviations and means (for raw or standardised data, or both if you really want to) and report the model values and statistics of those accordingly using appropriate `print` commands.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Explore what happens to the statistics reported above when you change the `test_size` value in `train_test_split`. (Have a think what you would expect if `test_size` were to ***increase*** substantially.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab03cc8-c528-41fc-9403-4201f5565cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac759190-c6c5-47ac-8fd0-0b1405d83c8d",
   "metadata": {},
   "source": [
    "### Over-fitting\n",
    "\n",
    "To demonstrate ***over-fitting*** and thus the need for cross-validation I am going to do **polynomial** regression instead but using `np.polyfit` (could also this through `sklearn.preprocessing.PolynomialFeatures`). In this case my (only) ***hyperparameter*** of the model would be the degree of fitting; linear regression is fitting to a polynomial of degree 1. The goal is to choose one that balances skill with some degree of robustness: in this case I want (relatively) lower MSE/MAE scores, and maybe that the spread of the model statistics is not completely wild.\n",
    "\n",
    "I am going to fix the training/testing split to reduce the degree of possible variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a805fe9-3370-4f93-9980-83a524bbae41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not going reshape because I don't need it when using np.polyfit\n",
    "\n",
    "X = df[target_vars[ind[0]]].values\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "\n",
    "# fix the seed for reproducibility purposes\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "n = 10\n",
    "MSE_train, MAE_train = np.zeros(n), np.zeros(n)\n",
    "MSE_test, MAE_test = np.zeros(n), np.zeros(n)\n",
    "for deg in range(n):\n",
    "    model = np.polyfit(X_train, Y_train, deg+1)\n",
    "    Y_pred = np.polyval(model, X_train)\n",
    "    MSE_train[deg] = mean_squared_error(Y_pred, Y_train)\n",
    "    MAE_train[deg] = mean_absolute_error(Y_pred, Y_train)\n",
    "    Y_pred = np.polyval(model, X_test)\n",
    "    MSE_test[deg] = mean_squared_error(Y_pred, Y_test)\n",
    "    MAE_test[deg] = mean_absolute_error(Y_pred, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa6c6d-20b7-4976-b0bd-1258795bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(np.arange(1, n+1), MSE_train, \"C0-x\", label=\"MSE\")\n",
    "ax.plot(np.arange(1, n+1), MAE_train, \"C1-x\", label=\"MAE\")\n",
    "ax.set_xlabel(\"polynomial degree\")\n",
    "ax.set_title(\"training data\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(np.arange(1, n+1), MSE_test, \"C0-x\", label=\"MSE\")\n",
    "ax.plot(np.arange(1, n+1), MAE_test, \"C1-x\", label=\"MAE\")\n",
    "ax.set_xlabel(\"polynomial degree\")\n",
    "ax.set_title(\"testing data\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a942e-0981-47f7-9d3f-fcc188436710",
   "metadata": {},
   "source": [
    "Above graph suggests that errors in the training are decreasing with increasing polynomial degree, although it is tapering off. This is not surprising as increased complexity generally allows for a better fit. On the other hand, the MSE error has reached some sort of minimum by degree 3, and increasing the degree doesn't do much for the skill of interest. Both of these are symptoms of over-fitting, where the model skill increases with complexity but then fails to generalise as well to unseen data.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> In the above case I am not really extrapolating as such because of how the data is distributed, but if I were extrapolating by feeding it an $X$ beyond the possible range of values I provide for the training then it would probably start returning insane values. Show that this is in fact the case (you don't need to go that far beyond the total range if your polynomial degree is high).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Evaluate the robustness of the above conclusion to the choice of random seed. The result should suggest to you that you may or may not want to therefore compute averages somehow to get some sense of the \"optimal\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf243490-34f1-4af9-a5b0-e9e32f1233fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b40a921a-f739-4dd2-9ae6-b6b551c4b02f",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "How do we go about selecting a model that has skill and is not too over-fitted? There isn't really one correct answer, but a main point is you probably need to multiple trains to evaluate dependence on the hyper-parameters etc., or take an average model of some sort.\n",
    "\n",
    "Training on all the data can be costly and one way to explore the dependence on hyper-parameters is ***$k$-fold*** cross-validation:\n",
    "\n",
    "* Split data into train and test sets as usual.\n",
    "* Further split training data set into $k$ groups, and train a model on all $k$ of those.\n",
    "* From that, make a decision what it means to be a \"best\" model (e.g. take the best model from the batch of $k$ models, take some average of the resulting parameters that describe the model, others...)\n",
    "* Test the \"best\" model on the testing data set to evaluate over-fitting etc.\n",
    "\n",
    "Doing this does not avoid over-fitting as such, but it does test for ***sensitivity***, which is an important thing to do given the inherent randomness in the procedures. Below code chooses the \"best\" model by:\n",
    "\n",
    "1. The one with the lowest `MSE_test` score (where \"test\" means the ones from the $k$-fold split).\n",
    "2. The avergage of all the trained models.\n",
    "\n",
    "The we evaluate the skill on the withheld test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ebe7e-ceee-4814-87cd-3d1223cb881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the procedure to split the training data further\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = df[target_vars[ind[0]]].values\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "# initialise a list and dump things in these\n",
    "k = 5\n",
    "deg = 5\n",
    "model = np.zeros((deg+1, k))\n",
    "MSE_train, MAE_train = np.zeros(k), np.zeros(k)  # don't actually use these but record anyway\n",
    "MSE_test, MAE_test = np.zeros(k), np.zeros(k)\n",
    "\n",
    "kf = KFold(n_splits=k)\n",
    "i = 0\n",
    "\n",
    "# only do k-fold split on the TRAINING data (the model still never sees the TESTING data)\n",
    "for train, test in kf.split(X_train):  # these generate a bunch of indices\n",
    "    model[:, i] = np.polyfit(X_train[train], Y_train[train], deg)\n",
    "    MSE_train[i] = mean_squared_error(np.polyval(model[:, i], X_train[train]), Y_train[train])\n",
    "    MAE_train[i] = mean_absolute_error(np.polyval(model[:, i], X_train[train]), Y_train[train])\n",
    "    MSE_test[i] = mean_squared_error(np.polyval(model[:, i], X_train[test]), Y_train[test])\n",
    "    MAE_test[i] = mean_absolute_error(np.polyval(model[:, i], X_train[test]), Y_train[test])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5b9f8-8c44-4bf8-ada1-f010ca05eca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model from lowest MSE_test, and from an average of those models\n",
    "model_best1 = model[:, np.where(MSE_test == np.min(MSE_test))[0][0]]\n",
    "model_best2 = np.mean(model, axis=-1)  # averaging over the \"model\" dimension\n",
    "\n",
    "# here we are passing (X_test, Y_test) in\n",
    "MSE_best1 = mean_squared_error(np.polyval(model_best1, X_test), Y_test)\n",
    "MSE_best2 = mean_squared_error(np.polyval(model_best2, X_test), Y_test)\n",
    "MAE_best1 = mean_absolute_error(np.polyval(model_best1, X_test), Y_test)\n",
    "MAE_best2 = mean_absolute_error(np.polyval(model_best2, X_test), Y_test)\n",
    "\n",
    "print(f\"model_best1 has:\")\n",
    "print(f\"  coeffs = {model_best1}\")\n",
    "print(f\"  MSE    = {MSE_best1:.6f}\")\n",
    "print(f\"  MAE    = {MAE_best1:.6f}\")\n",
    "print(\" \")\n",
    "print(f\"model_best2 has:\")\n",
    "print(f\"  coeffs = {model_best2}\")\n",
    "print(f\"  MSE    = {MSE_best2:.6f}\")\n",
    "print(f\"  MAE    = {MAE_best2:.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e25f41-a742-4f10-abb8-8a9ef168c698",
   "metadata": {},
   "source": [
    "Several things to be read from the above is that:\n",
    "\n",
    "* The best model has lower MSE, but the \"averaged\" model has marginally better performance in MAE, so which is better is somewhat arguable...\n",
    "* The resulting models are largely the same and has a large value in the last entry, in this case the leading coefficient of $X^5$. (This is probably not a good thing regarding over-fitting...)\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Play around with the choice of `k` and `deg`. I would expect the leading coefficient to still be large, which would indicate severe sensitivity if we are doing extrapolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a48d2-51ad-470a-a7a5-b8575b08338f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bdd6870-7826-4f06-aa00-983e0d033574",
   "metadata": {
    "id": "0bdd6870-7826-4f06-aa00-983e0d033574"
   },
   "source": [
    "We could also use some other in-built tools there to help with cross-validation (although it is good practice to do it once or twice by hand). Have a look at `cross_val_score`, `ShuffleSplit` as well as `Pipeline`. These can then be piggybacked for doing model selection and hyperparameter tuning, manually or using other `sklearn` objects. For length reasons I'm going to leave this for you to do in your own time instead. These can then be piggybacked for doing model selection and hyperparameter tuning, manually or using other `sklearn` objects. For length reasons I'm going to leave this for you to do in your own time instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899a05c2-a900-4976-99c3-184bd53c2d36",
   "metadata": {},
   "source": [
    "The thing to note is over-fitting is potentially more subtle in other models, so it is prudent to do ensembles and/or cross-validate; you should basically assume the ML models you create are probably over-fitted in general, and provide evidence to the degree of over-fitting as appropriate. Later notebooks will demonstrate cross validation and testing for robustness sporadically, but the comprehensive exploration will largely be left as exercises.\n",
    "\n",
    "> NOTE: There is an easier function `model_selection.cross_val_score` that could be used, but that requires an sklearn estimator object and doesn't play too well here for linear regression (partly because `LinearRegression` is too simple). We will use that briefly when we come to `03_linear_models_dim_reduction`.\n",
    ">\n",
    "### <span style=\"color:red\">Comments on over-fitting and robustness of model is expected of all the assignments that are to be handed in.</span>\n",
    "1. A portion of the marks under \"coding\" and marks under \"science\" will be given for (cross-)validation and related evaluations for model skill and robustness/sensitivity to hyper-parameters, data splitting etc. You automatically lose quite a lot of marks if you don't do it (cf. referencing: no one likes doing it as such, but it needs to be done).\n",
    "3. Saying you've done (cross-)validation but without providing numerical evidence will be treated as if no (cross-)validation was done. I need to see some evidence of this in code form.\n",
    "4. An ensemble calculation of about 5 to 10 would be minimally expected depending on the complexity of your problem (given you are not expected to be creating models that are large and thus slow to run)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507fe970-dc73-4333-8d53-aed3af443f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36dd3829-3f11-4514-96b0-8a37547bb9ab",
   "metadata": {
    "id": "36dd3829-3f11-4514-96b0-8a37547bb9ab"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Polynomial regression\n",
    "\n",
    "Consider doing polynomial regression using `sklearn.preprocessing.PolynomialFeatures`; this might be easier with [`pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) also. I basically did it manually above.\n",
    "\n",
    "(Essentially choosing a [basis for the features](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions), and is related to what we would do with [SINDy](https://pysindy.readthedocs.io/en/latest/), which we may visit in the bonus material.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97741a24-3b45-4ac2-b69b-8871e62c08ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdeb845-ddbe-4f5a-bc3d-46b784aecc87",
   "metadata": {
    "id": "4bdeb845-ddbe-4f5a-bc3d-46b784aecc87"
   },
   "source": [
    "## 2) Generalised Linear Models (GLMs)\n",
    "\n",
    "Have a look at [GLMs](https://scikit-learn.org/stable/modules/linear_model.html#generalized-linear-models), and try applying this in some of the examples above (or make up / apply to your own synthetic/real examples).\n",
    "\n",
    "(The reason we don't touch this is because this requires you knowing more things about other probability distribution funtcions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee7be1-77e5-46ba-85fb-640dc6945459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9dc012ff-3716-4bd2-b2a1-2b755f897cbd",
   "metadata": {
    "id": "9dc012ff-3716-4bd2-b2a1-2b755f897cbd"
   },
   "source": [
    "## 3) Bayesian regression\n",
    "\n",
    "Have a look at [Bayesian regession](https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression), and try applying this in some of the examples above (or make up / apply to your own synthetic/real examples).\n",
    "\n",
    "(The reason we don't touch this is because this requires you knowing the Bayesian formalism of probability/statistics.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9a04f-3fd0-40ac-892d-16432c5b361b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c884cb9-8e77-4103-8bb3-cc8b0ce82b5d",
   "metadata": {
    "id": "9c884cb9-8e77-4103-8bb3-cc8b0ce82b5d"
   },
   "source": [
    "## 4) Predicting cat faces, and on model interpretability\n",
    "\n",
    "Going to the raw images of cats, consider training linear models of the above ilk on half the face and using it to predict the other half. Analyse the skill on training data, testing data, the need for standardising the data, analysis of the model coefficients, dependence on the propotion of face exposed to model, cross-validation and etc.\n",
    "\n",
    "From a coding point of view it is potentially easier to use the left half of the face to predict the right half (idea below). You can try top and bottom also.\n",
    "\n",
    "<img src=\"https://i.imgur.com/G7xJJvu.jpeg\" width=\"500\" alt='cursed prediction'>\n",
    "\n",
    "One interest here is in the model coefficients: the loading values would provide a suggestion on which pixels may matter the most in the training of the model for the prediction problem, which aids in ***interpretability*** of the model. We will come back to do this when we get to `04_Neural_Networks`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafa621e-4c87-48a0-b6d8-6a3205a151a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a3ba38-5e5f-45e6-8ee1-868fac1a4cf0",
   "metadata": {
    "id": "a2a3ba38-5e5f-45e6-8ee1-868fac1a4cf0"
   },
   "source": [
    "## 5) Classification tasks and eigenpets\n",
    "\n",
    "There is the analogous dog data called `dog.csv`; try and do an eigenpets decomposition say that uses both cats and dog data.\n",
    "\n",
    "Then have a think how you might do ***classification*** of an image based on the features generated from eigenpets, and try and do that if you want to. One way to do it is simply say $Y=1$ is cat and $Y=-1$ is dog or analogously (i.e. you label the data), then you train a linear model based on the image and/or the eigenpet features. The predicted values will not be exactly $1$ or $-1$, but then you only need the sign of it (e.g. if predicted value is positive then it is a cat, and vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec0d51-c8b4-4a86-8eb1-deb8daf42570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "588d1795-3068-429c-a4ff-6348ec4919d9",
   "metadata": {
    "id": "588d1795-3068-429c-a4ff-6348ec4919d9"
   },
   "source": [
    "## 6) Argo data\n",
    "\n",
    "#### (This one is related to the upcoming assignment.)\n",
    "\n",
    "Consider training models on the Argo profile to predict one of the depth, temperature and salinity from the other two (as a start). Do the usual data scaling, evaluation for robustness, cross-validation etc.\n",
    "\n",
    "An extra thing to do would be to evalute whether the predictions are physically plausible:\n",
    "\n",
    "* It is entirely possible the prediction leads to a density that is in fact gravitationally unstable, when we don't think this is likely the case.\n",
    "* (Harder) The resulting density prediction contradicts the buoyancy frequency $N^2$ that is also present in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb135db-3202-489c-adf3-279e93ef49a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746f05ec-2b9a-438c-947e-7b43258cec64",
   "metadata": {
    "id": "746f05ec-2b9a-438c-947e-7b43258cec64"
   },
   "source": [
    "## 7) Hyperparameter tuning and model selection\n",
    "\n",
    "#### (May help with all upcoming assignments.)\n",
    "\n",
    "Instead of doing things manually, have a look at [here](https://scikit-learn.org/stable/modules/grid_search.html#) to see how you might do hyperparameter tuning of the model. Within the same manual there are suggestions on how you might model selection also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef06af-5a86-4d5a-8730-7cb52dbbe1be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fecccdf-df7d-4cd2-a993-0cdcfcd1eeb8",
   "metadata": {
    "id": "9fecccdf-df7d-4cd2-a993-0cdcfcd1eeb8"
   },
   "source": [
    "## 8) Spectral analysis\n",
    "\n",
    "If you have seen Fourier analysis before, consider doing dimension reduction / data compression via Fourier transform (e.g. `np.rfft` or similar); this is a fairly standard thing to do for acoustic data and signal processing for example (e.g. music identification software such as [Shazam](https://www.shazam.com/)), and maintains the feature orthogonality property.\n",
    "\n",
    "If you are feeling adventurous, you could try more general transforms (e.g. spherical harmonics, Bessel functions, wavelets).\n",
    "\n",
    "(There is then potentially something to be said about doing machine learning to predict things leveraging the data structure these spectral/wavelet spaces, which can be more stable and robust. See for example [this paper](https://www.nature.com/articles/s41467-021-21481-0) and maybe a video on YouTube by [Steve Brunton](https://www.youtube.com/watch?v=W8PybqAk6Ik).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190927b-4e29-400c-87ac-f3e39dfd4fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b17452a-ac02-4fc9-ba94-6e2b5eaf3878",
   "metadata": {
    "id": "6b17452a-ac02-4fc9-ba94-6e2b5eaf3878"
   },
   "source": [
    "## 9) Time-series forecasting\n",
    "\n",
    "Consider taking something like the `elnino34_sst.data` and train a linear model to do forecasting: given previous now and maybe some previous time data (these are your $X$s), predict the next time (these are your $Y$s). These are basically [ARIMA](https://en.wikipedia.org/wiki/Autoregressive_integrated_moving_average)-like models but with no noise terms.\n",
    "\n",
    "How many previous times you put in is up to you, but note that if you use the whole time series your model will be complex and you have very few values to regress over. A shorter time series may allow you to do train/test split.\n",
    "\n",
    "If you need to, I suppose you could \"cheat\" and generate multiple similar time series by taking the `elnino34_sst.data` and add noise to it, and call that a new time series; this is a bit like ***bootstrapping*** I would imagine.\n",
    "\n",
    "(You could also do something a bit more controlled by generating your time-series with a model of your choice. A particularly simple one would be a predator-prey or [Lotka-Volterra model](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations); if you look into the OCES 3301 repository there should already be an implementation there. A harder one would be a [Lorenz system](https://en.wikipedia.org/wiki/Lorenz_system).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b253332-a595-4aeb-aac2-f89af224e6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62ca7f88-0e20-4f6d-a8d1-f5978d00bc93",
   "metadata": {},
   "source": [
    "## 10) Other clustering algorithms\n",
    "\n",
    "There are a whole load of them at the [clustering](https://scikit-learn.org/stable/modules/clustering.html) page of `sklearn`. Have a look at them and see what you make of them, particularly of Gaussian Mixture Models (GMM), which I reference in the lecture but don't actually use it.\n",
    "\n",
    "(You can also explore the other [manifold learning](https://scikit-learn.org/stable/modules/manifold.html) methods.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536566c-6a13-4be2-8eed-898518ac296b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80df1448-aba5-416c-9cd0-9e010e02ca08",
   "metadata": {},
   "source": [
    "## 11) Primer for the assignment: using labels to improve models for `penguins`\n",
    "\n",
    "Use the identified clusters to train different linear models, or add the cluster labels in as an extra feature to train linear models. This is something you should do for the assignment with Argo data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba78c25b-18aa-4060-9075-cf21e2a171e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5281c10-7153-4f03-b5cb-8a838d9cc6a5",
   "metadata": {},
   "source": [
    "## 12) Clustering and/or manifold learning of cats (possibly also data cleaning)\n",
    "\n",
    "(May be useful for dealing with the penguins/turtle dataset and in classification tasks.)\n",
    "\n",
    "Consider applying clustering and/or manifold learning to cat images: this is a bit like what is done for the [digits dataset](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html), but larger dimension. I would start with the smaller `cats.csv` dataset first to see if you can identify clusters of data.\n",
    "\n",
    "If you are feeling brave and/or have some computational resources lying around, consider using the larger `cat_bw_enlarged.csv` dataset, and possibly use these approaches to detect ***outliers*** (there are definitely outliers or \"not good\" images in the dataset). This may be one way to use machine learning to help clean data, then feed the cleaner dataset back in for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682cf19-f1df-459c-a53f-37ca4ac9a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
