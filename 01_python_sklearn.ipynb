{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23bb91de-f8cc-44f9-9f33-284afa3ee0f9",
   "metadata": {
    "id": "f4dc7f5b-8740-4ce8-89f4-48a523f77203"
   },
   "source": [
    "*created 08 Jan 2026, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 5303 \"AI and Machine Learning in Ocean Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES5303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbaf749-1480-4de3-8fda-cd7b19e11746",
   "metadata": {
    "id": "2dbaf749-1480-4de3-8fda-cd7b19e11746"
   },
   "source": [
    "\n",
    "---\n",
    "# 01: Python recap, data handling and basic uses of `sklearn`\n",
    "\n",
    "The course here will use [Python](https://en.wikipedia.org/wiki/Python_(programming_language)) through [Jupyter notebooks](https://jupyter.org/). I chose Python because:\n",
    "* It's free (i.e. not [MATLAB](https://en.wikipedia.org/wiki/MATLAB) or SPSS)\n",
    "* It's not [R](https://en.wikipedia.org/wiki/R_(programming_language)) (I hate R syntax personally, but it will also work)\n",
    "* Python is used widely, has a lot of packages built in, pretty mature with userbase and support (an appropriate Google search help with debugging most of the time)\n",
    "* Familiarity to me\n",
    "\n",
    "I will openly admit I do not write Python in a Pythonic way: I started on MATLAB until MATLAB screwed up vector graphics outputs for me, so I rage quit and went to Python. The code provided here is certainly not the cleanest way to do it (this is sometimes by design), nor is it the most idiomatic way of doing it, but it should (mostly?) work and do the intended thing.\n",
    "\n",
    "## <span style=\"color:red\">!!! NOTE !!!</span>\n",
    "\n",
    "The content here largely assumes some familiarity with Python and some of the associated packages (see the list of packages to be loaded). If you are unfamiliar with those, you can have a look at the course content for OCES 3301 available at https://github.com/julianmak/OCES3301_data_analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b47164f-e5c5-4efb-8c17-77ea900d61cc",
   "metadata": {
    "id": "7b47164f-e5c5-4efb-8c17-77ea900d61cc"
   },
   "source": [
    "---\n",
    "# 1. Recapping Python through various data handling and using `sklearn`\n",
    "\n",
    "Just going to load and do basic manipulations of data that will be used for demonstration purposes.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. The present notebook is to check you can in fact load (almost) all the data that will be used for the course. If you are having trouble now then it really should to be fixed, because otherwise there will be issues for the remaining content...\n",
    "> 2. Demonstrates some Python loading/plotting approaches.\n",
    "> 3. Demonstrates some manipulations of array data, as well as `pandas` and `xarray` dataframes.\n",
    "> 4. Basic utility and syntax of the `scikit-learn` package.\n",
    "> 5. Highlight some inherent randomness in data-driven methods.\n",
    "> 6. Given the above, highlight the need to evaluate robustness of model skill (which requires defining what is meant by \"skill\")\n",
    "\n",
    "Going to load a bunch of relevant libraries first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d253bf34-1986-453d-966a-d93a7bacadc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "try:\n",
    "    import cftime\n",
    "except ModuleNotFoundError:\n",
    "    !pip install cftime\n",
    "    import cftime\n",
    "\n",
    "# optional: install the additional bits for xarray if you don't have it already (for Argo)\n",
    "# !pip install xarray[io]\n",
    "\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e25c1ee-46ab-4772-b1b3-ac3ca6a9ca90",
   "metadata": {
    "id": "9e25c1ee-46ab-4772-b1b3-ac3ca6a9ca90"
   },
   "source": [
    "---\n",
    "## a) Numerical data: python generated\n",
    "\n",
    "Name of the game for most of this course is to turn the data we read into numbers, manipulate those into a form that the Python data science + Machine Learning packages understands, and then feed them in the relevant algorithms. So it's probably useful to start with those.\n",
    "\n",
    "Below is a simple example of the function\n",
    "\\begin{equation*}\n",
    "    f = \\sin(t)\n",
    "\\end{equation*}\n",
    "modified in various ways (this is the same example used in `07_time_series` of OCES 3301). I am going to generate an ***array*** of numbers for $t$, and this is then fed into a function that spits out another array.\n",
    "\n",
    "> Note: For displaying to screen I will mostly use ***fstrings*** (e.g. `f\"STUFF\"` with the preface `f` before the string marks `\" \"`), although occasionally I will use `r` if I need some specific formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bb8576-053c-492b-92f1-41733a372a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_vec   = np.linspace(0, 2.0 * np.pi, 31)\n",
    "f       =   np.sin(t_vec)\n",
    "f_pos   = 2*np.sin(t_vec)\n",
    "f_neg   =  -np.sin(t_vec)\n",
    "f_shift =   np.sin(t_vec - np.pi / 2.0)\n",
    "\n",
    "print(f\"t_vec = {t_vec}\")\n",
    "print(\" \")\n",
    "print(f\"f = {f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66fe68-bf80-4241-a42f-5a6ec52f1dd5",
   "metadata": {
    "id": "8c66fe68-bf80-4241-a42f-5a6ec52f1dd5"
   },
   "source": [
    "The above as shown is not hugely useful since it is just a dump of numbers, but we can visualise this accordingly. Left plot shows it as a function of time (e.g. $[f_0, f_1, \\ldots]$ against $[t_0, t_1, \\ldots]$), right plots shows it as one function plotted against another (e.g. $[f_0, f_1, \\ldots]$ against $[g_0, g_1, \\ldots]$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ca625-03cb-49c4-b021-6ad4c791876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# award winning graph\n",
    "\n",
    "# 2x4 grid, firsrt graph takes up a 2x2 space located first at the upper left corner (0, 0)\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "ax = plt.subplot2grid((2, 4), (0, 0), colspan=2, rowspan=2)\n",
    "ax.plot(t_vec, f    ,   \"C0\", label=r\"$f$\")\n",
    "ax.plot(t_vec, f_pos,   \"C1\", label=r\"$f^+$\")\n",
    "ax.plot(t_vec, f_neg,   \"C2\", label=r\"$f^-$\")\n",
    "ax.plot(t_vec, f_shift, \"C3\", label=r\"$f_{\\rm shift}$\")\n",
    "ax.set_xlabel(r\"$t$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "# subsequent graphs are 1x1 but with a change in the location\n",
    "ax = plt.subplot2grid((2, 4), (0, 2))\n",
    "ax.scatter(f, f, color=\"C0\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 4), (0, 3))\n",
    "ax.scatter(f, f_pos, color=\"C1\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f^+$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 4), (1, 2))\n",
    "ax.scatter(f, f_neg, color=\"C2\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f^-$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot2grid((2, 4), (1, 3))\n",
    "ax.scatter(f, f_shift, color=\"C3\")\n",
    "ax.set_xlabel(r\"$f$\")\n",
    "ax.set_ylabel(r\"$f_{\\rm shift}$\")\n",
    "ax.grid()\n",
    "\n",
    "fig.tight_layout(pad=1.0) # give the graph a bit of padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0301b39b-1f75-4164-8e3d-aa62cc216a81",
   "metadata": {
    "id": "0301b39b-1f75-4164-8e3d-aa62cc216a81"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> This was previously used to demonstrate lag (linear) correlations. Convince yourself that the correlations of the right hand side subplots are (going clockwise) 1, 1, 0 and -1, corresponding accordingly to what you would suspect from looking at the entries in the left hand side subplot. Convince yourself that the **lagged correlation** looks like a sine (or cosine) curve.\n",
    "\n",
    "Can do stuff for multi-dimension data, but going to do this together with reading data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf30356-8b6d-4dd1-a569-5bb446ed5556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6c0a5d1-ea25-4e09-ad70-5014b9476e01",
   "metadata": {
    "id": "b6c0a5d1-ea25-4e09-ad70-5014b9476e01"
   },
   "source": [
    "---\n",
    "## b) Reading numerical data from file\n",
    "\n",
    "<img src=\"https://i.imgur.com/rKcpZzr.jpg\" width=\"400\" alt='cursed panda'>\n",
    "\n",
    "I am going to rely on `pandas` and/or `xarray` to read the data provided from file (or remotely via an internet connection if the files are small enough), and then do some manipulations and/or plotting with these; these can be done in principle via other means (see OCES 3301 for example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26f688-0fee-411f-9138-514416c240b1",
   "metadata": {
    "id": "ad26f688-0fee-411f-9138-514416c240b1"
   },
   "source": [
    "## El Nino 3.4 data\n",
    "\n",
    "This is a text file shown in the lecture slides and is just a text file. Going to load this remotely and spit out the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6668e9a5-e849-4498-9c39-e2ea4dfab5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"elnino34_sst.data\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/elnino34_sst.data\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "data = pd.read_csv(path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0db1c-b308-44ca-b7fc-7ea99b8ce0ee",
   "metadata": {
    "id": "49b0db1c-b308-44ca-b7fc-7ea99b8ce0ee"
   },
   "source": [
    "Generally `pandas` ***tries*** to read things assuming sensible layout etc., but that can fail if the data is not cleaned up (and most data is uncleaned). Here we have headers and misc. things we don't really need, leading to things being read into a single block.\n",
    "\n",
    "It is generally a good idea to have a look at the raw data file first to see what it consists and anticipate what things you might need to do. In this case, optional arguments needs to be provided (e.g. delimiter, separator, etc...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388efe1-2879-44f6-8c2b-13ef1012ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can give it a few more details to make it easier for pandas to help us\n",
    "df = pd.read_csv(path,\n",
    "          sep='\\s+',     # this used to be delim_whitespace=True,\n",
    "          names=[\"year\", \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"],\n",
    "          skipfooter=4,  # chop out some lines\n",
    "          skiprows=1,    # chop out some unnecessary lines\n",
    "          false_values=-99.99,\n",
    "          engine=\"python\")\n",
    "df = df.replace(-99.99, np.nan) # replace missing values with NaNs (not a number)\n",
    "df = df.set_index(\"year\")       # sets the index to be the year column\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6be92af-4dee-4a14-b153-42d5b23a0aac",
   "metadata": {
    "id": "a6be92af-4dee-4a14-b153-42d5b23a0aac"
   },
   "source": [
    "Notice there are `NaN`s in the data, which itself is not an issue. The pandas dataframe `df` has `72 rows x 12 columns`, which is 72 years of data every month over 12 months. We can do a quick dirty plot of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eda44e-7cb1-4279-85d1-ac16015d4a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not an award winning graph\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "df.plot(ylabel=r\"${}^\\circ\\mathrm{C}$\", ax=ax)  # pass some keywords in\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcddd5b6-96e1-4b24-ad17-370b93f2c7dc",
   "metadata": {
    "id": "bcddd5b6-96e1-4b24-ad17-370b93f2c7dc"
   },
   "source": [
    "In this case it is treating `months` (the header) as a category, i.e. plotting all the January temperatures as a function of `year`. This isn't necessary what we want: we probably want it as a time-series with increasing time.\n",
    "\n",
    "There are ways to do the reshaping in pandas, but for demonstration I am going to do this in native `numpy`:\n",
    "\n",
    "1. Load the pandas `df` frame into a numpy array with `df.values`.\n",
    "2. I want to keep the column ordering but remove the rows (e.g. have 1st row of 12, then 2nd row of 12 etc.), to be done via `.reshape(SIZE)`.\n",
    "   * Above can be done with `.flatten()` also.\n",
    "3. I am additionally going to compute the linear ***line of best fit*** (cf. `07_time_series` in OCES 3301), which I will need if I want to detrend the data to get the ***anomalies*** (although I don't use it here).\n",
    "\n",
    "> <span style=\"color:red\">**Q.**</span> by default `.reshape()` and `.flatten()` both use `order=C` as a default option, which gives the right thing in this case. Try this with `order=F` and convince yourself that is the wrong thing to do (the positions of `NaN`s would help)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bb5413-d327-40f6-8b33-9a0e08f63ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.values\n",
    "print(f\"data has shape {data.shape}\")\n",
    "data = data.reshape(data.size)  # data.size gives the total the number of entries\n",
    "print(f\"data now has shape {data.shape} after reshape or flatten\")\n",
    "print(\" \")\n",
    "\n",
    "# line of best fit via polyval (only because I don't want to load scipy)\n",
    "# need to remove NaNs first\n",
    "data_dum = data[~np.isnan(data)]  # find the NOT NaNs\n",
    "p = np.polyfit(np.arange(len(data_dum)), data_dum, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(data, label=\"data\")\n",
    "ax.plot(np.polyval(p, np.arange(len(data_dum))), label=\"LOBF\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_ylabel(r\"${}^\\circ\\mathrm{C}$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72512fd6-6208-48d3-9e4f-a385aef674a9",
   "metadata": {
    "id": "72512fd6-6208-48d3-9e4f-a385aef674a9"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> I was lazy and didn't provide the time array. Create a time array and do a proper regression (be careful of units). Then you can get a magnitude of a global warming trend from `p[0]` (which is the gradient of the straight line).\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Do the above but with `scipy` or `sklearn`.\n",
    "\n",
    "See extended exercises for more things to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b82d05-206d-4cf5-bd43-271be4167eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c55571e-e962-4400-a485-a9b57cfebd58",
   "metadata": {
    "id": "0c55571e-e962-4400-a485-a9b57cfebd58"
   },
   "source": [
    "## Penguin data\n",
    "\n",
    "<img src=\"https://www.boredpanda.com/blog/wp-content/uploads/2020/08/cats-standing-like-penguins-fb-png__700.jpg\" width=\"500\" alt='cursed penguins'>\n",
    "\n",
    "The [Palmer Penguins](https://cran.r-project.org/web/packages/palmerpenguins/readme/README.html) data was compiled as a replacement/alternative to the standard [iris data](https://en.wikipedia.org/wiki/Iris_flower_data_set) because of racism/eugenics reasons of Ronald Fisher (look it up if you are interested). A mildly touched up version is given here as `penguins.csv` (or see link below in the code); I removed some columns and some `NaN`s. We are going to be using this dataset quite a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b448c0c-e88c-42a7-8ac9-d2320e75b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e623544-2ad9-4948-bc93-9ae3b7af3203",
   "metadata": {
    "id": "2e623544-2ad9-4948-bc93-9ae3b7af3203"
   },
   "source": [
    "So this one is a text file but notice the headers are also loaded and are in fact useful because it tells you the data entries and units (which is more than can be said for a lot of data...) Notice also the `species` column is text while others are numbers; we will end up converting entries in `species` to numerical values in due course.\n",
    "\n",
    "Zeroth step of data analysis/exploration is to actually plot out the data first. Here are some random things I thought that could be done to demonstrate plotting/visualising and calling of things from `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da672049-34a8-4f77-90d0-046a5e41d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms of one of the randomly chosen variables cycling the species\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "target_var = np.random.choice(target_vars)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.hist(df[df[\"species\"] == species][target_var], label=species, alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(f\"{target_var}\")\n",
    "ax.set_ylabel(\"frequency\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30382b4d-324b-4841-91b5-ed84d7ae38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 3d plot of three random choices of variables\n",
    "\n",
    "from mpl_toolkits import mplot3d  # load a package for 3d plots\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "target_var = np.random.choice(target_vars, 3, replace=False)  # 3 unique choices\n",
    "\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = plt.axes(projection=\"3d\")\n",
    "\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_var[0]],\n",
    "               df[df[\"species\"] == species][target_var[1]],\n",
    "               df[df[\"species\"] == species][target_var[2]],\n",
    "               label=species\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_var[0]}\")\n",
    "ax.set_ylabel(f\"{target_var[1]}\")\n",
    "ax.set_zlabel(f\"{target_var[2]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend()\n",
    "ax.view_init(25, -45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793d451-7a00-4a79-99cd-5f8301b0961f",
   "metadata": {
    "id": "0793d451-7a00-4a79-99cd-5f8301b0961f"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> In the histogram plot I was deliberately lazy and didn't provide pre-defined bin edges (so `ax.hist` ended up choosing it). Pre-define the bin edges and do the binning of the data so that data from every species uses the same bins. You may want to use `np.histogram` instead, and then throw the outputs from there into `ax.hist` accordingly.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> The histogram plot shows frequency for now, but turn that into a probability either manually or using the inbuilt functionality.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Explore other combinations of scatter plots in both 2d and 3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39db4efc-7fc9-4ad7-a513-58a9e18b4ac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30ccd5f6-edfb-4ea5-b131-dd4f2ec8d373",
   "metadata": {
    "id": "30ccd5f6-edfb-4ea5-b131-dd4f2ec8d373"
   },
   "source": [
    "## Gridded data\n",
    "\n",
    "By gridded I mean these are data that have pre-defined co-ordinates that sits on a regular grid/array, such as (longitude, latitude) or similar. An example of this is satellite data: initially the data is per swarth, but given enough swarths some filling in can be done and the data put on a regular grid that is more useful for end users. One ongoing application of ML in oceanography would be instead of waiting for enough swarths, maybe you could use ML to fill in the gaps instead.\n",
    "\n",
    "The one I am showing here is from a simulation (sample from [NEMO ORCA0083-N01](https://gws-access.jasmin.ac.uk/public/nemo/)). The original dataset is REALLY big, so I downsized it quite significantly. The data is in the [NetCDF](https://en.wikipedia.org/wiki/NetCDF) format which is supposed to be self-describing (the one I made is not quite that). Going to open this with `xarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac697a-c86d-4e59-bdc7-13063d220991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# would do \"local\" for this one, because the filesize is not small (~45 MB)\n",
    "# could do this once and for all (as long as you save the file) with\n",
    "# !wget https://github.com/julianmak/OCES5303_ML_ocean/raw/refs/heads/main/current_speed.nc\n",
    "\n",
    "import fsspec  # for caching the file if using remote option\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"current_speed.nc\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES5303_ML_ocean/raw/refs/heads/main/current_speed.nc\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = xr.open_dataset(file)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf7fcd3-dde1-4bc1-b41d-b5282cd2fe21",
   "metadata": {
    "id": "cdf7fcd3-dde1-4bc1-b41d-b5282cd2fe21"
   },
   "source": [
    "This one is `(time, lat, lon)`, and although I didn't write the units in it is `m s-1`. Here we can select one time and plot out the data as a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db7fbc-92ad-4e94-a37f-ad7a77617c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using contourf here, could do pcolor also\n",
    "t_ind = 0\n",
    "\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "ax = plt.axes()\n",
    "cs = ax.contourf(df[\"lon\"], df[\"lat\"], df[\"speed\"][t_ind, :, :], 31)\n",
    "ax.set_xlabel(r\"lon $(^\\circ)$\")\n",
    "ax.set_ylabel(r\"lat $(^\\circ)$\")\n",
    "ax.set_title(df[\"time\"][0].values)  # load as string to remove other xarray descriptors\n",
    "cax = plt.colorbar(cs)\n",
    "cax.ax.set_title(r\"$\\mathrm{m}\\ \\mathrm{s}^{-1}$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb6a8b-cb6e-4420-8568-24c97d4a856b",
   "metadata": {
    "id": "21eb6a8b-cb6e-4420-8568-24c97d4a856b"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Plot out longitudinal or meridional slices instead.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Select one location and plot out the time series.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Make a movie out of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309631a-095e-4a95-9569-df85903ae800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e29aa638-7d17-4c22-b992-434ce0981f80",
   "metadata": {
    "id": "e29aa638-7d17-4c22-b992-434ce0981f80"
   },
   "source": [
    "## Argo data\n",
    "\n",
    "[Argo](https://argo.ucsd.edu/data/) is a system of autonomous floats that are put into the ocean, floating around with the currents, and periodically does vertical sections to take in-situ measurements of things like temperature, salinity, pressure, and so forth down to about 2000 m depth; see below for the schematic. There are increasing interest in [BGC-Argo](https://biogeochemical-argo.org/) that measure quantities relevant to biogeochemistry, and [deep Argo](https://argo.ucsd.edu/expansion/deep-argo-mission/) that go down to 4000 m; see OCES 3301 for more description.\n",
    "\n",
    "<img src=\"https://argo.ucsd.edu/wp-content/uploads/sites/361/2020/06/float_cycle_1-768x424.png\" width=\"600\" alt='Argo'>\n",
    "\n",
    "> NOTE: The namesake of Argo is related to the [JASON](https://en.wikipedia.org/wiki/Jason-1) satellites if you know your Greek mythology.\n",
    "\n",
    "The float data here are vertical sections at specific locations of space, and can be regarded as data that is more \"raw\" than the gridded data. The data provided here is in the `zarr` format that can be opened with `xarray`.\n",
    "\n",
    "Some care needs to be taken to obtain a copy of this. Would highly recommend not loading this remotely, because the content is big.\n",
    "\n",
    "> <span style=\"color:red\">!!! NOTE !!!</span> (JM 15 Apr 2025): If you are on Colab, you probably need to mount and do a separate upload of the data.\n",
    "> 1) Go to https://drive.google.com/drive/folders/1JJ0cpshu6-JE8wp93UsHuqy6V33rQy7s?usp=sharing\n",
    "> 2) Download the folder\n",
    "> 3) Upload that to your own instance of Colab\n",
    "> 4) Mount with `from google.colab import drive; drive.mount('/content/drive')` and then proceed as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uLabU4o-ndr7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount the drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff19374-bc25-40f9-b134-345d7e8d5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is slightly out-of-date and will fail with \"consolidated\" option but will load\n",
    "# silencing the warning\n",
    "\n",
    "## !!!!!!\n",
    "# make sure the data is somewhere in the drive\n",
    "# modify to the data path (to where you saved the data), and\n",
    "# give a full path, e.g.\n",
    "# argo_data_path = '/content/drive/MyDrive/oces5303_ml/GLOB_HOMOGENEOUS_variables.zarr'\n",
    "# df = xr.open_zarr(argo_data_path, consolidated=False)\n",
    "# df\n",
    "#\n",
    "# I have the data so on my laptop I do the following\n",
    "df = xr.open_zarr(\"GLOB_HOMOGENEOUS_variables.zarr\", consolidated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca22157-a8a7-4189-be96-a711ce0072e5",
   "metadata": {
    "id": "1ca22157-a8a7-4189-be96-a711ce0072e5"
   },
   "source": [
    "So note that the data is arranged as `(N_PROF, DEPTH)`, and `TIME`, `LATITUDE` and `LONGITUDE` are the variables tagged to `N_PROF`. Sample plot looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d67a7b-8943-4679-b833-399fa953f60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out what the observation data actually looks like in geographical space\n",
    "\n",
    "nl = 20 # change this index to plot different depths (as an index entry)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 4))\n",
    "\n",
    "# temperature\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cs = ax.scatter(df.LONGITUDE, df.LATITUDE, 10, df.TEMP[:,nl],\n",
    "                cmap=plt.get_cmap('Spectral_r'), zorder=3)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.set_title(f\"Temp at {df.DEPTH[nl].values} m\")\n",
    "\n",
    "# salinity\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cs = ax.scatter(df.LONGITUDE, df.LATITUDE, 2, df.PSAL[:,nl],\n",
    "                alpha=.5, cmap=plt.get_cmap('viridis', 5), zorder=3)\n",
    "ax.set_xlabel(r\"lon ($^\\circ$)\")\n",
    "ax.set_ylabel(r\"lat ($^\\circ$)\")\n",
    "plt.colorbar(cs)\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.set_title(f\"Salinity at {df.DEPTH[nl].values} m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c76ca2-4d63-4eaf-a9a4-3db628123365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out TS-diagrams at different depths\n",
    "\n",
    "nl = 0\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.plot(df.PSAL[:, nl], df.TEMP[:, nl], \"o\", markersize=2, label=\"total\")\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f\"TS diagram at {df.DEPTH[nl].values} m\")\n",
    "\n",
    "nl = 20\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.plot(df.PSAL[:, nl], df.TEMP[:, nl]  , \"o\", markersize=2, label=\"total\")\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "ax.set_ylabel(r'Temperature ($^\\circ\\ \\mathrm{C}$)')\n",
    "ax.set_xlabel(r'Salinity ($\\mathrm{g}/\\mathrm{kg}$)')\n",
    "ax.set_title(f\"TS diagram at {df.DEPTH[nl].values} m\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598e5e5-cb8c-446d-b3dc-eb15da75ebea",
   "metadata": {
    "id": "4598e5e5-cb8c-446d-b3dc-eb15da75ebea"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> So from the TS diagram you notice that are some outliers that probably should be removed (e.g. water that is too fresh is unlikely under typical oceanic conditions). Come up with criteria to drop these points from `df`, and do the plots again. This is an important part of data pre-processing before throwing it into the ML algorithmcs, following the ***Garbage In Garbage Out*** principle.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> Subset these by geographical locations (e.g. Atlantic Ocean using whatever defensible criterion you like), and either plot these separately, or plot them together with the labels. This is a thing that is worth looking into now because you will be doing something like this in assignment 1.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda3bc56-8459-45a2-b70c-e5ae0eb7fdcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cd4de47-d652-4091-975a-dca80ad3cb50",
   "metadata": {
    "id": "3cd4de47-d652-4091-975a-dca80ad3cb50"
   },
   "source": [
    "---\n",
    "\n",
    "## c) Images as numerical data\n",
    "\n",
    "Just like we can visualise data as an image, we can sometimes go the other way and get data out of an image:\n",
    "\n",
    "1) One possible example might be that you have chlorophyll concentration measurements, which gives some shades of green in the image. Then a useful thing might be the reverse: you can consider the case of measuring greeness from a satellite to infer for the chlorophyll concentration.\n",
    "2) I want to automatically classify species of fish or penguins or whatever from a long video segment.\n",
    "3) I have broken images that I may want to fill out.\n",
    "\n",
    "I am going to use some `jpg` files I have handy to demonstrate images as arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9234b0e2-aab8-479c-b597-807b913caf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    file = \"broccollie.jpeg\"\n",
    "elif option == \"remote\":\n",
    "    # do a local caching (downloads a file to cache)\n",
    "    print(\"loading data remotely\")\n",
    "    file_loc = \"https://github.com/julianmak/OCES5303_ML_ocean/raw/refs/heads/main/broccollie.jpeg\"\n",
    "    file = fsspec.open_local(f\"simplecache::{file_loc}\", filecache={'cache_storage': '/tmp/fsspec_cache'})\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "data = plt.imread(file)\n",
    "ax = plt.axes()\n",
    "ax.imshow(data)\n",
    "ax.set_title(f\"a broccolie of shape {data.shape}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bded57d-f69d-430a-ab2b-6f3bc1241691",
   "metadata": {
    "id": "9bded57d-f69d-430a-ab2b-6f3bc1241691"
   },
   "source": [
    "As can seen from querying the loaded array, the array is of size `(pixels, pixels, RGB)` where `RGB` is the strength of (red, green blue), and this goes from 0 to 255 for reasons you can look up if you want.\n",
    "\n",
    "It is just an array so all the usual things can be done to it. The example below converts the RGB image to grayscale using the formula\n",
    "\\begin{equation*}\n",
    "    \\mathrm{gray} = 0.299 \\mathrm{Red} + 0.587 \\mathrm{Green} + 0.114 \\mathrm{Blue}.\n",
    "\\end{equation*}\n",
    "The formula assumes the RGB values lie between 0 and 1 so some conversion is needed, but that's easy (just divide by 255).\n",
    "\n",
    "> NOTE: For plotting you could of course just plot is as grayscale, but this is for demonstrating how to manipulate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b66f2-090c-40b2-9a48-67de161c0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise data then convert\n",
    "data_bw = data / 255\n",
    "data_bw = 0.288 * data_bw[:, :, 0] + 0.587 * data_bw[:, :, 1] + 0.114 * data_bw[:, :, 2]\n",
    "\n",
    "ax = plt.axes()\n",
    "ax.imshow(data_bw, cmap=\"gray\")\n",
    "ax.set_title(f\"a broccollie of shape {data_bw.shape}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60cacd-eac1-4625-8549-1060c72c90d8",
   "metadata": {
    "id": "8d60cacd-eac1-4625-8549-1060c72c90d8"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> Consider passing these through blurring or sharpening fitlers. `scipy` has a few, or you can do it from scratch by specifying the convolution or deconvolution kernels accordingly (cf. `08_times_series` and `10_fun_with_maps` in OCES 3301, session 9 in this course later).\n",
    "\n",
    "Below case is a stack of images that are written into a `csv` file, where one dimension denotes all the pixels, and the other dimension denotes the sample number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a1e3f-2fd1-44b0-a9c9-dd91162c628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/cat.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "# going to transpose this so the shape is (image number, pixels)\n",
    "df = pd.read_csv(path, header=None).T\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6e674-5433-45f7-b897-bbe306fc35c5",
   "metadata": {
    "id": "0dc6e674-5433-45f7-b897-bbe306fc35c5"
   },
   "source": [
    "The file contains data for 80 images of cats of 64 by 64 pixels each ($64^2 = 4096$). This one requires unflattening/reshaping the arrays for the images to make sense. For ML applications with actually feed in flattened data into algorithms; see later notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c797c3-0ada-4660-9e0c-8bbeada08ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, reshape data and then plot one of the guys out\n",
    "\n",
    "cats = df.values\n",
    "ind = np.random.randint(cats.shape[0])\n",
    "\n",
    "# transpose back for image display purposes\n",
    "fig = plt.figure(figsize=(2, 2))\n",
    "ax = plt.axes()\n",
    "ax.imshow(np.reshape(cats[ind, :], (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_title(f\"cat {ind+1} / {cats.shape[0]}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a634ae3-448d-42d8-a398-a6e4d25f6a60",
   "metadata": {
    "id": "3a634ae3-448d-42d8-a398-a6e4d25f6a60"
   },
   "source": [
    "> <span style=\"color:red\">**Q.**</span> As a python exercise, try and use a loop to plot five of these but randomly choosing the image number to plot. Make sure the randomly selected indices are distinct.\n",
    ">\n",
    "> <span style=\"color:red\">**Q.**</span> You can try reshaping it in different ways (e.g. instead of `.reshape(64, 64)` try other numbers that multiple to `4096`), and convince yourself the choice taken here is the only sensible one.\n",
    ">\n",
    "Point here is that if you can deal with these sample images you can in principle deal with other images (e.g. fish/coral/snails/satellite/remote sensing images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78acfdc4-5aaa-45c0-a178-b9ca99007c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ceefae15-1982-4dae-bf97-8f840b8941c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## d) Regression as an optimisation problem\n",
    "\n",
    "For the below demonstration I am going to use the `penguins` data, which I load in the cell below and do a quick plot of for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e085a-62c7-49c3-b655-45e4bdf91b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8227c0-325d-460c-98af-039f33d4f320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 2d plot of two specific variables in the penguins data\n",
    "\n",
    "target_vars = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "ind = [3, 1]\n",
    "\n",
    "fig = plt.figure(figsize=(4, 4))\n",
    "ax = plt.axes()\n",
    "for species in np.sort(df[\"species\"].unique()):   # pick out all unique entries under `species`\n",
    "    ax.scatter(df[df[\"species\"] == species][target_vars[ind[0]]],\n",
    "               df[df[\"species\"] == species][target_vars[ind[1]]],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[1]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9024d9cd-29e5-4844-83d7-0584b886fb85",
   "metadata": {},
   "source": [
    "If we regard ML as finding a best model $f$ that maps inputs $X$ to outputs $Y$ (i.e. regression), then by the \"best model\" we mean some $f$ that minimises the ***mismatches*** between predictions $\\hat{Y}$ and given data $Y$. The problem then ultimately depends on what you define as the mismatch. A standard choice is the $L^p$ family of norms given by\n",
    "\\begin{equation*}\n",
    "    \\|\\hat{Y} - Y\\|_{L^p} = \\left(\\int |\\hat{Y} - Y|^p\\; \\mathrm{d}\\mu \\right)^{1/p}.\n",
    "\\end{equation*}\n",
    "The most commonly used ones of these are $L^2$ or ***mean squared error*** (MSE) and $L^1$ or ***mean absolute error*** defined by\n",
    "\\begin{equation*}\n",
    "    \\mathrm{MSE} = \\|\\hat{Y} - Y\\|^2_{L^2} = \\int |\\hat{Y} - Y|^2\\; \\mathrm{d}\\mu, \\qquad \\mathrm{MAE} = \\|\\hat{Y} - Y\\|_{L^1} = \\int |\\hat{Y} - Y|\\; \\mathrm{d}\\mu,\n",
    "\\end{equation*}\n",
    "where the integral is invariably replaced by sums in contexts encountered in this course.\n",
    "\n",
    "For the training of the ML model a choice of mismatch (i.e. the ***loss function***, or the ***objective function*** in the classical optimisation literature) would need to be specified, and $L^2$ or MSE is often used. On the other hand, we can also evalute a model's skill using MSE and MAE measures (or any other ones we think are relevant).\n",
    "\n",
    "> NOTE: Again, \"standard\" does not necessarily mean best.\n",
    ">\n",
    "> $\\mathrm{d}\\mu$ is related to a \"measure\"; not going to elaborate what that means except that it is a fundamental object in the theory of probability, and is more fundamental than the \"metric\".\n",
    ">\n",
    "> $L^2$ or MSE is often chosen to be the loss presumably because $L^2$ is differentiable and the resulting space is (presumably) a ***Hilbert space***, which has desirable properties. Not actually sure if this is the intention of most practicioners though...\n",
    "\n",
    "To see how this works we do need to choose a model, so I am going to choose Linear Regression from the `sklearn` library (`LinearRegression` below) partly to demostrate syntax for `sklearn`. Recall that linear regression does\n",
    "\\begin{equation*}\n",
    "    \\hat{Y} = aX + b,\n",
    "\\end{equation*}\n",
    "where the name of the game is to find numbers $a$ and $b$ such that the $L^2$ mismatches are minimised. Since it is constructed to be a $L^2$ mminimiser we expect the related MSE will be small, but note that this says nothing about MAE. We are going to predict $Y$ from $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89769fc8-841f-4e59-ac84-bdc486544d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# same syntax as above, (n_samples, n_features)\n",
    "X = df[target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "Y = df[target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(X, Y, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.plot(X, Y_pred, 'bx', label=\"predictions\")\n",
    "ax.set_xlabel(target_vars[ind[0]])\n",
    "ax.set_ylabel(target_vars[ind[1]])\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title(f\"fitted model: Y = {model.coef_[0, 0]:.6f} X + {model.intercept_[0]:.3f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf80c8-263b-45cd-9ecc-0efb37e000ab",
   "metadata": {},
   "source": [
    "The model is expected to suck in terms of skill because the data is clearly not linear. Below compares the relevant errors as done through `sklearn` as well as a native way of doing this using `numpy`, as well as computing some other relevant statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19d8f67-1626-459f-a47f-7df5b5ed0127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import r_regression\n",
    "\n",
    "# as computed by sklearn\n",
    "MSE_sklearn = mean_squared_error(Y, Y_pred)\n",
    "MAE_sklearn = mean_absolute_error(Y, Y_pred)\n",
    "\n",
    "# as computed by hand\n",
    "MSE_np = np.mean(np.abs(Y - Y_pred)**2)\n",
    "MAE_np = np.mean(np.abs(Y - Y_pred))\n",
    "\n",
    "print(f\"MSE from sklearn = {MSE_sklearn:.4f}\")\n",
    "print(f\"MSE from numpy   = {MSE_np:.4f}\")\n",
    "print(f\"MAE from sklearn = {MAE_sklearn:.4f}\")\n",
    "print(f\"MAE from numpy   = {MAE_np:.4f}\")\n",
    "print(\" \")\n",
    "print(f\"R^2 score of model = {model.score(X,Y):.4f}\")\n",
    "print(f\"correlation coeff of model = {r_regression(X, Y.ravel())[0]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de7a05-fff3-4d59-aa0b-acd374d9fda7",
   "metadata": {},
   "source": [
    "Coinvince yourself the $R^2$ score is consistent that there is not really a linear correlation going on, and the negative linear correlation coefficient is also consistent with the overall shape of the data.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Check that the $R^2$ and linear correlation coefficient computed above coincide with that computed from `scipy`.\n",
    "\n",
    "We will do more in terms of modelling with `sklearn` over the next two sessions. For now however we move on to some other useful and important utilities of `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f391c0-fd6a-48fe-acb8-6dde6d27436e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57867428-e481-49f2-ace9-860479e571c3",
   "metadata": {},
   "source": [
    "---\n",
    "## e) Dataset splitting and scaling/transformations\n",
    "\n",
    "You could argue it is better practice to withold some data for testing purposes, because otherwise the model sees \"everything\" so of course it is able to reproduce those things. The usual approach in ML is then to split the data into:\n",
    "\n",
    "* ***Training data***, which is passed in during model training\n",
    "* ***Testing data***, which is **not** exposed to model during training, and used to evalute the ***skill*** of a model (whatever you want skill to mean)\n",
    "* ***Validation data***, which is for tuning internal ***hyperparameters***, and is exposed to the model during training\n",
    "\n",
    "Mostly going to deal with the first two. Two \"obvious\" (!?) questions to ask for the dataset splitting would mainly concern whether you want a specific selection, and how much data you want in each dataset. There is some arbitrariness in this and answer might be context dependent...\n",
    "\n",
    "Without knowing what the data looks like, probably the most sensible thing to do is to just select data randomly (i.e. sampling from a uniform distribution). Doing a 80:20 training testing dataset split also seems reasonable. You can do this by hand, or use the `train_test_split` in `sklearn` to do this also. Below demostrates the syntax of that and provides some illustration of what is going on.\n",
    "\n",
    "> NOTE: Below I've actually loaded the arrays into memory, but I could have passed in `pandas` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71a202a-03a6-45c5-9ed3-9b8a85170c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split syntax demonstration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# select data based on choice of \"ind\" above:\n",
    "X = df[target_vars[ind[0]]].values  # actually load the data into memory\n",
    "\n",
    "# going to subset data twice to demonstrate a point\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    X_train, X_test = train_test_split(X, test_size=0.2) # 20% of data in test\n",
    "    ax.plot(X_train, f'C{i}o', alpha=0.7, label=f\"split {i}\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_ylabel(f\"{target_vars[ind[0]]}\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65c6fd-0709-4370-b44e-a9b23da0f5bb",
   "metadata": {},
   "source": [
    "It is not that clear above, but the splitting is random and the data selected is not overlapping. It is probably clearer in the case where I select two variables and do this as a scatter plot. The below code demonstrates also how you can pass in multiple arrays to be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92173481-87e5-44a9-9a1a-fa19ba5c216b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data based on choice of \"ind\" above:\n",
    "X = df[target_vars[ind[0]]].values  # actually load the data into memory\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "\n",
    "# going to subset data twice to demonstrate a point\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(2):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "    ax.plot(X_train, Y_train, f'C0o', alpha=0.7, label=\"training data\")\n",
    "    ax.plot(X_test, Y_test, f'C1^', label=\"testing data\")\n",
    "    ax.set_xlabel(f\"{target_vars[ind[1]]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(f\"{target_vars[ind[0]]}\")\n",
    "    ax.grid(lw=0.5, zorder=0)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891f285b-6cf0-40ac-9815-090fc8e41ef1",
   "metadata": {},
   "source": [
    "For reproducibity (e.g. in an assignment say), you can specify `random_state=SEED` in the `train_test_split` to force it to split in a specific way (basically specifying the random seed). The below basically shows two identical images.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Check the split arrays are in fact exactly identical in the contents and the order when the random seed is specified (there should be no permutations in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77721185-af2c-4a40-adbd-a1a3e65965c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why 42 ???\n",
    "seed = 42\n",
    "\n",
    "# going to subset data twice to demonstrate a point\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "for i in range(2):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=seed)\n",
    "    ax = plt.subplot(1, 2, i+1)\n",
    "    ax.plot(X_train, Y_train, f'C0o', alpha=0.7, label=\"training data\")\n",
    "    ax.plot(X_test, Y_test, f'C1^', label=\"testing data\")\n",
    "    ax.set_xlabel(f\"{target_vars[ind[1]]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(f\"{target_vars[ind[0]]}\")\n",
    "    ax.grid(lw=0.5, zorder=0)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717610a-935e-4de3-87a4-24bd9a052196",
   "metadata": {},
   "source": [
    "If you have't changed my default settings then one thing you can argue is that the units of the selected data are not even the same (one is a length and the other is a mass), so how can we even sensibly/meaningfully compare them? Further, given a ML model depends on data input, would this discrepancy not cause problems for the model?\n",
    "\n",
    "Quick answer to the above are that \"you can't/shouldn't\" and \"yes it will do\", although the question (and thus the related answer) is probably much more subtle... Without going into too much detail, it really is context dependent, but a standard (!?) thing to do is to normalise and scale the data distributions accordingly.\n",
    "\n",
    "> NOTE: \"Standard\" does not mean it is the \"best\" all the time.\n",
    ">\n",
    "> I like to try and think of problems from a geometry point of view, and the way I'd phrase the above problem is effectly asking what is the ***metric*** of interest. From a machine learnig point of view, see [information geometry](https://en.wikipedia.org/wiki/Information_geometry) and the textbook by the late [David MacKay](https://www.inference.org.uk/mackay/itila/book.html) for an overview of this.\n",
    "\n",
    "One way to do this is to ***assume*** (the \"assume\" part is important) the data follows a Gaussian/normal distribution, i.e. for data (random variables) $X$ and $Y$, we have $X \\sim \\mathcal{N}(\\mu_X, \\sigma_X)$ and $Y \\sim \\mathcal{N}(\\mu_Y, \\sigma_Y)$. While each of the data sets would have their own mean and standard deviations $\\mu$ and $\\sigma$, we can remove the individual the mean and scale by the standard deviation so the transformed data follow a standardised Gaussian distribution, i.e. $\\tilde{X}, \\tilde{Y}\\sim \\mathcal{N}(0, 1)$. Then we can compare them at least from the distribution point of view. The `StandardScaler()` below basically does this. I am going throw the data in individually first to demonstrate some basic syntax.\n",
    "\n",
    "> NOTE: `sklearn` expects data arrays to have the shape `(n_samples, n_features)`. The 1d arrays I throw in will fail although the warning will ask for a `.reshape`; the one we want is `.reshape(-1, 1)` in this case (it basically just adds an extra dimension to it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682929d-944e-4007-8283-7624eabd7d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstration of standard scaler: comparisons of PDFs (i.e. histograms)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df[target_vars[ind[0]]].values  # actually load the data into memory\n",
    "Y = df[target_vars[ind[1]]].values\n",
    "\n",
    "# sklearn syntax: expects (n_samples, n_features), reshape below is needed\n",
    "print(f\"shape before = {X.shape}\")\n",
    "X = X.reshape(-1, 1)\n",
    "print(f\"shape after  = {X.shape}\")\n",
    "print(\" \")\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "# initialise scaler, then fit, and then transform\n",
    "scaler_X = StandardScaler()\n",
    "scaler_X.fit(X)   # data is n samples 1 feature\n",
    "X_scale = scaler_X.transform(X)\n",
    "scaler_Y = StandardScaler()\n",
    "scaler_Y.fit(Y)\n",
    "Y_scale = scaler_Y.transform(Y)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# raw unscaled histograms\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.hist(X, density=True, color=\"C0\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.hist(Y, density=True, color=\"C1\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$Y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.hist(X_scale, density=True, color=\"C0\", alpha=0.7, label=r\"$\\tilde{X}$\")\n",
    "ax.hist(Y_scale, density=True, color=\"C1\", alpha=0.7, label=r\"$\\tilde{Y}$\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(r\"$\\tilde{X}$ and $\\tilde{Y}$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27862c38-e7d8-4fde-9b79-35f68dddd3c4",
   "metadata": {},
   "source": [
    "Hopefully you are convinced that it would be ridiculous to try and plot the left and middle one together on the same plot given the disparity of scales.\n",
    "\n",
    "The syntax below does multiple things at the same time.\n",
    "\n",
    "> NOTE: `sklearn` expects data arrays to have the shape `(n_samples, n_features)`, so I may need to be a bit careful about how I jam the two arrays together. I avoid it completely by just reloading the relevant data, but the commented code shows another way to do it (I can think of at least five other ways to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c439f50-21cc-451a-ac33-e9f7324319b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data\n",
    "data = df[[target_vars[ind[0]], target_vars[ind[1]]]].values\n",
    "print(f\"data shape is {data.shape}, already of right form\")\n",
    "print(\" \")\n",
    "\n",
    "# could also stack the two arrays together\n",
    "# X = df[target_vars[ind[0]]].values.reshape(-1, 1)\n",
    "# Y = df[target_vars[ind[1]]].values.reshape(-1, 1)\n",
    "# data = np.concatenate((X, Y), axis=-1)  # stack at the feature axis which is the last one\n",
    "\n",
    "# don't even bother defining the scaler object\n",
    "data_scale = StandardScaler().fit_transform(data)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "# raw unscaled histograms\n",
    "ax = plt.subplot(1, 3, 1)\n",
    "ax.hist(data[:, 0], density=True, color=\"C0\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 2)\n",
    "ax.hist(data[:, 1], density=True, color=\"C1\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$Y$\")\n",
    "ax.grid()\n",
    "\n",
    "ax = plt.subplot(1, 3, 3)\n",
    "ax.hist(data_scale[:, 0], density=True, color=\"C0\", alpha=0.7, label=r\"$\\tilde{X}$\")\n",
    "ax.hist(data_scale[:, 1], density=True, color=\"C1\", alpha=0.7, label=r\"$\\tilde{Y}$\")\n",
    "ax.grid()\n",
    "ax.set_xlabel(r\"$\\tilde{X}$ and $\\tilde{Y}$\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad2884b-6f69-4108-aa48-f34a4a048348",
   "metadata": {},
   "source": [
    "There are other ways to scale the data; see extended exercises later.\n",
    "\n",
    "Given the randomness in the data and train/test split, two things you may want to ask are:\n",
    "\n",
    "1. How robust is the model? (Did you get a model with good skill because you got lucky?)\n",
    "2. How generalisable is your model?\n",
    "\n",
    "We will deal with these a bit more in the next session when we have more possibility for variation in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e25ed7c-3a91-4a80-9e24-8839042b7a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3273f3e7-88b4-4b78-a898-fb8f5103c9f9",
   "metadata": {
    "id": "3273f3e7-88b4-4b78-a898-fb8f5103c9f9"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Delay embedding (Takens' embedding)\n",
    "\n",
    "Taking the sine curve example, consider doing the plots like the right hand side, but instead do e.g.\n",
    "\\begin{equation*}\n",
    "    [f_1, f_2, f_3, f_4, \\ldots, f_N]\n",
    "\\end{equation*}\n",
    "against\n",
    "\\begin{equation*}\n",
    "    [f_0, f_1, f_2, f_3, \\ldots, f_{N-1}],\n",
    "\\end{equation*}\n",
    "i.e. shift the array by one index (or more if you want). You will need to be careful about array sizes and calling array entries that are less than zero (this will lead to wrap around, e.g. `f[-1] = f[N]` and `f[-2] = f[N-1]`; the sine curve example is periodic so it doesn't matter, but it might matter for more general cases).\n",
    "\n",
    "You may want to consider writing this as a subroutine that takes in an array and spits out two arrays, one with a shifted index, and both having the intended array size.\n",
    "\n",
    "If you plot it out you should get ellipses with different eccentricities depending on the time-lag. Convince yourself that actually makes sense.\n",
    "\n",
    "The above is related to [Takens' theorem](https://en.wikipedia.org/wiki/Takens%27s_theorem) and we may or may not come back to this in the bonus lectures, e.g. [Empirical Dynamic Modelling (EDM)](https://en.wikipedia.org/wiki/Empirical_dynamic_modeling), [Topological Data Analysis (TDA)](https://en.wikipedia.org/wiki/Topological_data_analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfdd77-2557-4ea6-bf7b-90e2aeba6a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline arrays for doing lag embedding with\n",
    "t_vec = np.linspace(0, 2.0 * np.pi, 61)\n",
    "f     = np.sin(t_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1bb4f5-a5e5-466b-956c-61c0a54d840a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5dd5675-7364-4421-979f-d366d095141c",
   "metadata": {
    "id": "b5dd5675-7364-4421-979f-d366d095141c"
   },
   "source": [
    "## 2) El Nino data manipulation\n",
    "\n",
    "Starting from the El Nino 3.4 time series data, probably detrend it to get the anomalies. Provide a threshold criteria to classify El Nino and La Nina events using an analogous criterion to e.g. https://ggweather.com/enso/oni.htm. You may or may not want to compute some smoothing / running averages.\n",
    "\n",
    "Could also compute the power spectrum or similar to obtain the magnitude of frequencies; see OCES 3301 on how this might be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab8f883-7798-49f1-937e-f3617c88ca72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dbc855b-12a7-4d21-ad61-70a56404f2a7",
   "metadata": {
    "id": "5dbc855b-12a7-4d21-ad61-70a56404f2a7"
   },
   "source": [
    "## 3) Turtle + penguin data\n",
    "\n",
    "Have a look and see what is in this dataset: https://www.kaggle.com/datasets/abbymorgan/penguins-vs-turtles. This may be used for one of the marked assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ffb70-ccd9-477e-9de1-a23235503195",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c83aeaa-0a79-40f0-86c4-c5c0d785bf6d",
   "metadata": {
    "id": "0c83aeaa-0a79-40f0-86c4-c5c0d785bf6d"
   },
   "source": [
    "## 4) Satellite data\n",
    "\n",
    "Obtain some satellite grid and/or track data. Would recommend having a look at the [Sentinel Data Space](https://dataspace.copernicus.eu/data-collections/copernicus-sentinel-data). This may or may not be a useful place to get data for the extended project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1fe5a-47bd-4adf-8b84-24ad915bcd6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4be5544b-2d52-46d2-b24d-50410752693f",
   "metadata": {},
   "source": [
    "## 5) Other ways of scaling\n",
    "\n",
    "Have a look at [here](https://scikit-learn.org/stable/api/sklearn.preprocessing.html) and explore other ways of scaling the data.\n",
    "\n",
    "You could also try scaling things according to non-Gaussian pdfs, but you may need to do this yourself. Could also consider ***non-dimensionalisation*** based on specific processes (e.g. if you've done my OCES 2003 you would have seen things like the Rossby/Reynolds/Rayleigh/Ekman number; this may show up in the extra material of this course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c2ca89-eebd-4f5d-a7c0-75887bdc65bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224c482a-9189-4fdf-bd55-4f37ed77ce72",
   "metadata": {},
   "source": [
    "## 6) $L^1$ minimising linear regression\n",
    "\n",
    "This was previously posted as a (hard-ish) problem to do in OCES 3301. Linear regression by default do $L^2$ (mean squared error or MSE) minimisation, but that says nothing about the $L^1$ (mean absolute error or MAE), or indeed other norms. See how you would create a $L^1$ minimising linear regression predictor, and show the resulting object does seem to minimise $L^1$, reduces weighting on the outliers, but says nothing about the $L^2$ or other errors.\n",
    "\n",
    "I would do it (and have done it beofre) through `scipy.optimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cfb4e-c20a-4cf2-9b55-d3182a197aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b392f5d7-41e7-4d6a-a4fc-cf89effb2414",
   "metadata": {},
   "source": [
    "## 7) Some datasets in `sklearn`\n",
    "\n",
    "Have a look and play around with the (already) processed data in `sklearn`; see [here](https://scikit-learn.org/stable/datasets.html) for some descriptions.\n",
    "\n",
    "A notable example from there is the famous Mauna Loa CO2 data that gives the Keeling curve; see [here](https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_co2.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-co2-py) for a related tutorial that provides instructions on how to read some data. (You can have a look at ***Gaussian Processes*** emulator if you like, although I am not touching on it in this course.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f78967-2993-4015-aff0-a0c66839505f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
