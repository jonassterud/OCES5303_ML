{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a9e8ad7-e2a7-41bc-9ec6-734f29fe4207",
   "metadata": {
    "id": "7a9e8ad7-e2a7-41bc-9ec6-734f29fe4207"
   },
   "source": [
    "*created 09 Jan 2026, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 5303 \"AI and Machine Learning in Ocean Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES5303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021411e-ee13-4aad-a43f-20de846a775b",
   "metadata": {
    "id": "a021411e-ee13-4aad-a43f-20de846a775b"
   },
   "source": [
    "---\n",
    "# 3. Classification tasks and random forests\n",
    "\n",
    "So far we have dealt with ***regression***, where we assume the target is continuously varying. ***Classification*** would be the discrete version of that, i.e. predict the discrete labels given the data. Examples of this might be:\n",
    "\n",
    "* Given attributes from the `penguins` data, predict the species.\n",
    "* Classify whether an image is that of a cat or a dog.\n",
    "* Predicting whether an Argo temperature/sainity profile is from a particular geographical location if I label it accordingly (e.g. from the Atlantic).\n",
    "\n",
    "We are also going to introduce the use of ***decision trees*** and ensemble methods, of which the two algorithms of interest are ***random forests*** and ***gradient boosting***.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Demonstrate that classification is closerly related to finding separation between labelled data (and separation again depends on what you mean by distance).\n",
    "> 2. Introduce other loss functions, some of which are closely related to ***activation functions*** that we will encounter when dealing with neural networks.\n",
    "> 3. Go into a little detail about the anatomy of decision trees, and introduce ensemble based methods for decision trees.\n",
    "> 4. Consider an example of decision tree as a classifier and a regressor.\n",
    "\n",
    "For most of the below I am going to use some artificial data for the code demonstration, before performing similar calculations with the `penguins` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3622477f-25e0-4a6f-bc8f-2cdc8ac25946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7722be69-0afd-424f-bdb0-2ec65512fee4",
   "metadata": {
    "id": "7722be69-0afd-424f-bdb0-2ec65512fee4"
   },
   "source": [
    "---\n",
    "## a) Classification tasks.\n",
    "\n",
    "### Linear/Quadratic Discriminant Analysis (L/QDA)\n",
    "\n",
    "The idea of classification is to find a separator between two sets of data, then you can say if my data lies to one side it is class $A$ etc. The main idea of ***LDA*** and ***QDA*** is that you find a linear and quadratic separator between the data. Without going into the subtleties, the two methods here have closed form solutions and no hyperparameters to tune, and are generally quite robust. For some more details, you could start with [here](https://scikit-learn.org/stable/modules/lda_qda.html).\n",
    "\n",
    "I am going to leverage the moon data for this. The creation mechanism already spits out the labels (I just happened to suppress it last time); you could have make label yourself through the procedures in the last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97335299-a8de-49a3-ba0a-8cb73b8a0805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create moon data, cluster and then label them\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# use different colours for reasons later\n",
    "colors = \"rb\"  # red blue\n",
    "\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=0)     # labels are numeric: 0 and 1\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    ax.plot(X[Y==i, 0], X[Y==i, 1], f\"{colors[i]}x\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.set_xlim([-1.5, 2.5]); ax.set_ylim([-0.8, 1.2])\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6f1b5-ab50-4186-8c77-b32d0ddd0a89",
   "metadata": {
    "id": "2ba6f1b5-ab50-4186-8c77-b32d0ddd0a89"
   },
   "source": [
    "Going to first demonstrate LDA. We do train/test split, but we don't really need to standarise for this dataset. The predictions return labels, and the check on the score is simply how many labels did it get correct (`np.sum(Y_pred == Y_test)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cd9cdb-c99b-43c8-af24-dc1b0949ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ea8ce-ba9b-4eac-9af1-e9f76cc42622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit models\n",
    "model = LinearDiscriminantAnalysis()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4c3c25-114a-4e3f-bfd3-f400cd4cae9b",
   "metadata": {
    "id": "1a4c3c25-114a-4e3f-bfd3-f400cd4cae9b"
   },
   "source": [
    "We have broad diagnostics of skill but it may be of interest to actually see what is the boundary the model decided to find. Here we can leverage `sklearn.inspection.DecisionBoundaryDisplay` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599bab4-739f-464e-9cf3-dc8d209c0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# do a prediction but on full data\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    ax.plot(X[Y_pred==i, 0], X[Y_pred==i, 1], f\"{colors[i]}x\", alpha=0.7)\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict_proba\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict_proba\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    alpha=1.0,\n",
    "    levels=[0.5],  # because the labels are 0 or 1, and (0 + 1)/2 = 0.5\n",
    ")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.set_xlim([-1.5, 2.5]); ax.set_ylim([-0.8, 1.2])\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b244252-c570-4d39-ab25-ed95fdeb71b7",
   "metadata": {
    "id": "8b244252-c570-4d39-ab25-ed95fdeb71b7"
   },
   "source": [
    "So everything above the line is predicted as red and below is blue, and there are some parts of the moon that gets predicted wrong. This is not entirely surprising because for this case there is no linear line in 2d that cuts the data cleanly in half.\n",
    "\n",
    "Before we move on to QDA, note that you can use LDA as a way to do dimension reduction. For this problem the data is embedded in 2d, so the reduction goes to 1d. Within the lower dimension the idea is to find the best separator, which in this case is some horizontal line. Given the data mixing, there is no clean separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b42252-3a58-46ba-a2a3-00759f3025b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduction of dimenion by 1 in this case\n",
    "X_transform = model.transform(X_train)\n",
    "\n",
    "# plot out the transforms + projected data with labels\n",
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    ax.plot(X_transform[Y_train == i], f\"{colors[i]}x\", alpha=0.7)\n",
    "ax.set_xlabel(r\"$\\hat{x}$\")\n",
    "ax.set_title(r\"LDA projection of moon data\")\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedff8e0-e507-4550-af49-e3ac07652602",
   "metadata": {
    "id": "cedff8e0-e507-4550-af49-e3ac07652602"
   },
   "source": [
    "A similar thing can be done with QDA, although there is no dimension reduction procedure here with the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e928799-7f55-4e01-a0f4-ea34729ca0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# fit models\n",
    "model = QuadraticDiscriminantAnalysis()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(\" \")\n",
    "\n",
    "# do a prediction but on full data\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    ax.plot(X[Y_pred==i, 0], X[Y_pred==i, 1], f\"{colors[i]}x\", alpha=0.7)\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict_proba\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict_proba\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    alpha=1.0,\n",
    "    levels=[0.5],  # because the labels are 0 or 1, and (0 + 1)/2 = 0.5\n",
    ")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.set_xlim([-1.5, 2.5]); ax.set_ylim([-0.8, 1.2])\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7209d9-d47c-40a3-831f-f727415b726f",
   "metadata": {
    "id": "3b7209d9-d47c-40a3-831f-f727415b726f"
   },
   "source": [
    "Here the boundary is a quadratic curve that gives a marginally better performance than LDA. This is perhaps not surprising because the separator has more complexity and is slightly more flexible.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> I didn't standardise the data, and I am fairly sure it doesn't actually do anything for this case. Convince yourself that statement is probably true numerically or otherwise.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Here you probably don't need to check for robustness and cross-validation as such, because the data and the methods are simple enough, but give that a go to see how the prediction scores change.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try something similar for the swiss roll or S curve data embedded in 3d. The way I would do it is \"unroll\" the data somehow (use a dimension reduction technique to 2d), then artificially draw a line in the plane and call one half something and another half something else, then you do the classification. You could also try and have multiple labels (which we will see later with the penguins data).\n",
    ">\n",
    "> Note that the `DecisionBoundaryDisplay` routine will complain if the predictor requires more than two inputs (because plotting a surface of separation is hard). You could of course include dimension reduction techniques in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3f1819-0e51-4e8c-8399-c2cfb6854b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e47de41-8bd9-4ef2-b18a-c7a2badc2e25",
   "metadata": {
    "id": "0e47de41-8bd9-4ef2-b18a-c7a2badc2e25"
   },
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "If we take the moon data example above then you can argue there is no way I can separate the data unless I have enough complexity, which may then give me over-fitting. But that's only if I am stuck 2d; see below for an artificial example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f60112-65c1-4731-a6e0-ac3433df4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different colours for reasons later\n",
    "colors = \"rb\"  # red blue\n",
    "\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=0)     # labels are numeric: 0 and 1\n",
    "\n",
    "# artificially lift the data into 3d\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "\n",
    "# lift the data into 3d and shift the z-coordinate but some constant\n",
    "X_3d = np.zeros((X.shape[0], X.shape[1]+1))  # make a bigger array\n",
    "X_3d[:, :-1] = X                             # dump in the old data\n",
    "X_3d[:, 2] = np.where(Y==0, 0.0, 1.0)        # add a z-coord related to label\n",
    "\n",
    "elev_range = [90, 45, 10]\n",
    "for j in range(3):\n",
    "    ax = plt.subplot2grid((1, 3), (0, j), projection=\"3d\")\n",
    "    for i in range(2):\n",
    "        ax.plot(X_3d[Y==i, 0], X_3d[Y==i, 1], X_3d[Y==i, 2], f\"{colors[i]}x\", alpha=0.7)\n",
    "    ax.view_init(azim=-75, elev=elev_range[j])\n",
    "    ax.set_box_aspect((1, 1, 1))\n",
    "    ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\"); ax.set_zlabel(r\"$z$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce91a7d-4d54-409d-b599-19e8439f407f",
   "metadata": {
    "id": "7ce91a7d-4d54-409d-b599-19e8439f407f"
   },
   "source": [
    "If we do that then the separator is \"obvious\" to find. You can of course argue that I can arbitrarily promote the data to higher dimensions (effectively as a co-ordinate transformation), then I can always find a good separator (i.e. high-dimensional hyperplane) for it.\n",
    "\n",
    "The idea of ***Support Vector Machine*** (SVM) is essentially to find a maximal separator between the labelled data, allowing for promotion to higher dimension space, but penalising that latter procedure. The result is again an optimisation problem.\n",
    "\n",
    "> NOTE: The details are a bit complicated in that you actually solve a ***dual problem*** using the ***kernel trick***. Not going to elaborate on what that is.\n",
    "\n",
    "In `sklearn` SVM is in the module `svm`, and we can try and do the above but with default SVM. We are going to use `SVC` which is the basic one for classification (C for classification; SVR is for regression). The default there uses the regularisation parameter `C=1` and `kernel=rbf` (Radial Basis Function). Have a look at [here](https://scikit-learn.org/stable/modules/svm.html) to see what those mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c57ca0b-5e18-4611-b76c-760b82c12182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=0)     # labels are numeric: 0 and 1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4167)\n",
    "\n",
    "# fit models\n",
    "# NOTE: Try kernel='linear', what is the result?\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "\n",
    "# do a prediction but on full data\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    ax.plot(X[Y_pred==i, 0], X[Y_pred==i, 1], f\"{colors[i]}x\", alpha=0.7)\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict\",  # SVM has no probability associated with predictions\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    alpha=1.0,\n",
    "    levels=[0.5],  # because the labels are 0 or 1, and (0 + 1)/2 = 0.5\n",
    ")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.set_xlim([-1.5, 2.5]); ax.set_ylim([-0.8, 1.2])\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b904ca30-2de5-4cbd-94d8-18dc51a1d98f",
   "metadata": {
    "id": "b904ca30-2de5-4cbd-94d8-18dc51a1d98f"
   },
   "source": [
    "More or less what we expected! In this case we can get a perfect score because the data is in fact well-separated (but that's because the data noise level is fairly low).\n",
    "\n",
    "In the below case I am going to do a whole load of different variants. I will be using this multiple times, so I am going to wrap it up in a subroutine that takes in the input and output data and does some plots.\n",
    "\n",
    "> NOTE: Unlike the one above where I use different data for training and testing, the ones below use all the data for fitting, because it is only there to demonstrate differences in behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cbeb8c-26ad-4336-8026-3cd2da9a792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_SVM_variants(X, Y, models, title, C=1.0):\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for j in range(len(models)):\n",
    "\n",
    "        model = models[j]\n",
    "\n",
    "        # fit models\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        # basic skill diagnostics\n",
    "        Y_pred = model.predict(X)\n",
    "        N = len(Y)\n",
    "        skill_all = np.sum(Y_pred == Y)\n",
    "\n",
    "        # do a prediction but on full data\n",
    "        ax = plt.subplot(2, 2, j+1)\n",
    "        for i in range(2):\n",
    "            ax.plot(X[Y_pred==i, 0], X[Y_pred==i, 1], f\"{colors[i]}x\", alpha=0.7)\n",
    "        DecisionBoundaryDisplay.from_estimator(model, X,\n",
    "            response_method=\"predict\",  # SVM has no probability associated with predictions\n",
    "            plot_method=\"pcolormesh\",\n",
    "            ax=ax,\n",
    "            cmap=\"RdBu\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "        DecisionBoundaryDisplay.from_estimator(model, X,\n",
    "            response_method=\"predict\",\n",
    "            plot_method=\"contour\",\n",
    "            ax=ax,\n",
    "            alpha=1.0,\n",
    "            levels=[0.5],  # because the labels are 0 or 1, and (0 + 1)/2 = 0.5\n",
    "        )\n",
    "        if j % 2 == 0:\n",
    "            ax.set_ylabel(r\"$y$\")\n",
    "        if j > 1:\n",
    "            ax.set_xlabel(r\"$x$\")\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "        if (j == 1 or j == 3):\n",
    "            ax.set_yticklabels([])\n",
    "        ax.set_title(f\"{titles[j]}, full skill = {skill_all/N*100:.2f}%\")\n",
    "        ax.grid()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4e698b-3383-4f60-b916-728b9d6f8ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refresh data\n",
    "\n",
    "random_state = 0\n",
    "\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=random_state)\n",
    "\n",
    "# cycle through an array of models\n",
    "C = 1.0  # regularisation parameter (1.0 is the default, smaller is larger regularisation)\n",
    "models = (\n",
    "    svm.SVC(kernel='linear', C=C, random_state=random_state),\n",
    "    svm.SVC(kernel='rbf', gamma=0.7, C=C, random_state=random_state),\n",
    "    svm.SVC(kernel='poly', degree=3, C=C, random_state=random_state),\n",
    "    svm.LinearSVC(C=C, random_state=random_state),\n",
    ")\n",
    "titles = (\"linear kernel\", r\"RBF with $\\gamma$\", \"degree 3 kernel\", \"linear SVC\")\n",
    "\n",
    "fig = plot_SVM_variants(X, Y, models, titles, C=C);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabd511-c25f-4079-920a-01fa2990432a",
   "metadata": {
    "id": "fbabd511-c25f-4079-920a-01fa2990432a"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> Try the whole thing again but this time increase the noise in the data generation part (then we won't have a good separation).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try something similar for the swiss roll or S curve data embedded in 3d, with massaging of the data as before.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Explore different model parameters to see what that does, and cross-validate things where relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768bfdf-ab7b-4ba3-b864-4807670c023d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "429517e1-fd5e-4d9c-bcdc-a5a8705b5d4e",
   "metadata": {
    "id": "429517e1-fd5e-4d9c-bcdc-a5a8705b5d4e"
   },
   "source": [
    "### Things to be aware of: data scaling\n",
    "\n",
    "SVM is not scale-invariant and can be sensitive to scaling in the data. In the below case I artificially stretch the data in one co-ordinate by quite a large factor, and you can see the data starts behaving a bit strangely (particularly the `rbf` that previously had a 100% accuracy).\n",
    "\n",
    "Standardisation is recommended in general. Try it for the penguins data later particularly if the `body_mass_g` feature is included, because that has completely different magnitudes compared to the others (and the fact that it doesn't even have comparable units...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d412568-b1f8-40c1-9293-242f0bb34cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificially scaled data\n",
    "\n",
    "random_state = 0\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=random_state)\n",
    "\n",
    "factor = 100\n",
    "X[:, 0] *= factor  # stretch it in one direction by a factor\n",
    "\n",
    "# try scaling here\n",
    "# scalar = StandardScaler()\n",
    "# X = scalar.fit_transform(X)\n",
    "\n",
    "fig = plot_SVM_variants(X, Y, models, titles, C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb474f22-4b09-4ba1-b3ae-111ea7a3a309",
   "metadata": {
    "id": "cb474f22-4b09-4ba1-b3ae-111ea7a3a309"
   },
   "source": [
    "### Things to be aware of: unbalanced datasets\n",
    "\n",
    "SVM has issues if the dataset is unbalanced, by which I mean there is noticeably more data in one class than the other. In the below code I am going to artificially remove some data from one class and then do SVM on those again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a22c1e-fc1d-46bd-bb4e-a074c1de041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifically unbalanced data\n",
    "random_state = 0\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=random_state)\n",
    "\n",
    "# define amount of data to keep, find index, choose an integer selection, and overwrite array\n",
    "factor = 0.05\n",
    "inds_full = np.where(Y==1)[0]\n",
    "np.random.seed(random_state)  # force it to be deterministic\n",
    "inds = np.random.choice(inds_full, int(factor * len(inds_full)), replace=False)\n",
    "X = np.concatenate((X[Y==0, :], X[inds, :]), axis=0)\n",
    "Y = np.concatenate((Y[Y==0], Y[inds]))\n",
    "\n",
    "fig = plot_SVM_variants(X, Y, models, titles, C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38966c-0dbe-4d68-9b61-6f4ec75e6f7e",
   "metadata": {
    "id": "0b38966c-0dbe-4d68-9b61-6f4ec75e6f7e"
   },
   "source": [
    "Several possible ways to deal with this are (can be used in combination, and not all of them will fix everything):\n",
    "\n",
    "* Remove some data from the larger class (but this is removing data which can be a problem with smaller datasets)\n",
    "* ***Bootstrap*** to bulk out the class that is low on samples (but requires interpolation and/or knowing something about the pdf of the data)\n",
    "* Pass `class_weight=\"balanced\"` when initialising the model\n",
    "* Increase the regularisation `C` when initialising the model (in this case you want to DECREASE the value of `C`)\n",
    "\n",
    "Below considers the latter two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147ce86-a4b6-4920-842e-95d9ebd9bd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying model parameters for unbalanced dataset\n",
    "random_state = 0\n",
    "C = 0.75  # regularisation parameter (1.0 is the default, smaller is larger regularisation)\n",
    "models = (\n",
    "    svm.SVC(kernel='linear', C=C, \n",
    "            random_state=random_state, class_weight=\"balanced\"),\n",
    "    svm.SVC(kernel='rbf', gamma=0.7, C=C, \n",
    "            random_state=random_state, class_weight=\"balanced\"),\n",
    "    svm.SVC(kernel='poly', degree=3, C=C, \n",
    "            random_state=random_state, class_weight=\"balanced\"),\n",
    "    svm.LinearSVC(C=C, \n",
    "                  random_state=random_state, class_weight=\"balanced\"),\n",
    ")\n",
    "\n",
    "fig = plot_SVM_variants(X, Y, models, titles, C=C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66989f45-2b4b-4fb1-b815-b500277b9896",
   "metadata": {
    "id": "66989f45-2b4b-4fb1-b815-b500277b9896"
   },
   "source": [
    "The rebalancing seems to improve the SVM using `rbf`, but only has marginal response in the other ones.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Try the whole thing again but this time increase the noise in the data generation part (then we won't have a good separation).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try something similar for the swiss roll or S curve data embedded in 3d, with massaging of the data as before.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Explore different model parameters to see what that does, and cross-validate things where relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8183803-4879-4a9c-ac0a-c6bc2c692576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be5750cc-0df4-429f-af33-3616851c32c9",
   "metadata": {
    "id": "be5750cc-0df4-429f-af33-3616851c32c9"
   },
   "source": [
    "---\n",
    "## b) Detour: Stochastic Gradient Descent (SGD) with different choices of loss and penalisations\n",
    "\n",
    "So here I want to take a slight detour because some of the details will be relevant to neural networks later. We talked about ***gradient descent*** before as a means to solve the optimisation problem earlier: recally that for the landscape defined by the loss function $J$, around $J(\\theta_n)$, you want to probe for the gradient information, and update to get $\\theta_{n+1}$ depending on the direction where $J$ descends fastest. Formally to get the full gradient you would need to compute $J$ in some $n$-dimensional ball around $\\theta$ to find the optimal direction.\n",
    "\n",
    "The idea then is that you could consider some random directions instead of every direction, and use that to approximate the gradient. You potentially take a less direct route to the minimum along the $J$ landscape, but it is cheaper, and you could in principle use the information in the previous iterations to help you along the way. If the number of directions is 1, it's called ***stochastic gradient descent***; if it's more than 1 it's called ***mini-batch*** gradient descent (but also sometimes just labelled as \"stochastic\").\n",
    "\n",
    "This kind of method is useful particularly for things like neural networks where there are a lot of entries in $\\theta$ to adjust, so the dimensionality can be so large that full gradient descent is not computationally feasible. These type of methods also have some suggestion that it can sample the space better and not get stuck in a local minimum.\n",
    "\n",
    "SGD itself is a method and not a classifier as such: when you choose the loss function (and appropriate penalisation) you define the type of classifier/regressor. For classifiers the kind of loss we want should be one-sided: you should incur a massive cost if you get the classification wrong. Below are some of the these, and I am going to plot these out first before describing them.\n",
    "\n",
    "> NOTE: You can look through the code to see the mathematical form of these if you want; not going to type those out here. (I basically took the below graph from  [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html), although I renamed the variables.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b5f96-9984-4572-aa00-a7b501123da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole bunch of loss functions\n",
    "\n",
    "def modified_huber_loss(Y_true, Y_pred):\n",
    "    z = Y_pred * Y_true\n",
    "    loss = -4 * z\n",
    "    loss[z >= -1] = (1 - z[z >= -1]) ** 2\n",
    "    loss[z >= 1.0] = 0\n",
    "    return loss\n",
    "\n",
    "X = np.linspace(-4, 4, 101)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot([-4, 0, 0, 4], [1, 1, 0, 0], label=\"zero-one\")\n",
    "ax.plot(X, np.where(X < 1, 1 - X, 0), label=\"hinge\")\n",
    "ax.plot(X, -np.minimum(X, 0), label=\"perceptron\")\n",
    "ax.plot(X, np.log2(1 + np.exp(-X)), label=\"log\")\n",
    "ax.plot(X, np.where(X < 1, 1 - X, 0) ** 2, label=\"squared hinge\")\n",
    "ax.plot(X, modified_huber_loss(X, 1), \"--\", label=\"modified Huber\")\n",
    "ax.set_ylim((0, 8))\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$f(x)$\")\n",
    "ax.set_ylabel(r\"$\\mathcal{L}(f(x))$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46dbbca-ddc7-459d-8b4d-99e0e094a851",
   "metadata": {
    "id": "e46dbbca-ddc7-459d-8b4d-99e0e094a851"
   },
   "source": [
    "We are making the decision here that anything positive is correct, and negative is bad (you can flip these accordingly of course). Respectively, we have:\n",
    "\n",
    "* **Zero-one**: No cost if correct, other incur a cost of one. This is not really implemented, and is not considered further.\n",
    "* **Hinge**: It's called \"hinge\" because it looks like a door hinge (the bit that connects the door to the wall). You start incurring a cost from 1, and this scales up linearly as you get more and more wrong.\n",
    "* **Perceptron**: It's called \"perceptron\" because it is used in ***perceptrons***, which we will encounter later when we deal with neural networks. This is like hinge but it starts activating at 0 instead of 1.\n",
    "* **Log**: Because it follows a (shifted) logarithm. Starts activating sooner but scales up faster than hinge and perceptron (because log blows up to infinity faster than linear functions).\n",
    "* **Squared hinge**: Because you take the hinge loss function and square it.\n",
    "* **Modified Huber**: Bit like the squared hinge but is more smooth, has some numerical advantages.\n",
    "\n",
    "> NOTE: The main reason I wanted to digress into this here is that the loss functions defined here where flipped about the $y$-axis because possible candidates of ***activation functions***, which is important for neural networks later. For example, flipping the perceptron loss above would lead to ***ReLU***, which is usually the default activation for neural networks.\n",
    "\n",
    "Note that the loss functions above are not convex and by itself may not have a minimum. A regularisation is usually needed; common choices are just the $L^1$, $L^2$ and elastic nets (a combination of both).\n",
    "\n",
    "For a SGD classifer we would import this from `sklearn.linear_model.SGDClassifier`, and passing the keyword `loss=` in. The default regulariser is $L^2$ (`penalty=\"l2\"`), which is fine for regression but not useful for classification. Below are some specific cases:\n",
    "\n",
    "* `loss=\"hinge\"` and `penalty=\"l2\"` (the default if you just do `SDGClassifier()`: This reduces to linear SVM above.\n",
    "* `loss=\"modified_huber\"` is a bit like the above, but punishing outliers more. Comes with probability measures of the decision (cf. allows for `response_method=\"predict_proba\"` in `DecisionBoundaryDisplay.from_estimator`)\n",
    "* `loss=\"log_loss\"` gives ***Logistic Regression***, which is really a classifier, although it has regression in the name. Sometimes called ***logit*** or ***Maximum Entropy*** classifier; not going to go into why that is.\n",
    "\n",
    "Below I am just going to one for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95351115-b8e6-4e5e-ad7b-c8031a22a0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "n_samples = 600\n",
    "X, Y = make_moons(n_samples, noise=0.05, random_state=0)     # labels are numeric: 0 and 1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4167)\n",
    "\n",
    "# fit models\n",
    "model = SGDClassifier(loss=\"log_loss\")  # just logistic regression, with L2 penalisation\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(\" \")\n",
    "\n",
    "# do a prediction but on full data\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(2):\n",
    "    ax.plot(X[Y_pred==i, 0], X[Y_pred==i, 1], f\"{colors[i]}x\", alpha=0.7)\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    cmap=\"RdBu\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    alpha=1.0,\n",
    "    levels=[0.5],  # because the labels are 0 or 1, and (0 + 1)/2 = 0.5\n",
    ")\n",
    "ax.set_xlabel(r\"$x$\"); ax.set_ylabel(r\"$y$\");\n",
    "ax.set_xlim([-1.5, 2.5]); ax.set_ylim([-0.8, 1.2])\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401d0a7-8c68-43a5-b003-0c5b39c3141f",
   "metadata": {
    "id": "4401d0a7-8c68-43a5-b003-0c5b39c3141f"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> You could also do logistic regression via `sklearn.linear_model.LogisticRegression`, and that has somewhat more options available. Convince yourself those two more or less do the same things.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try some other combinations (e.g. linear SVM with $L^1$ penalisation, which promotes sparsity). You have to be a bit careful that convergence of the solver may become an issue.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> One thing of interest to later is the ***learning rate*** that you can specify in the classifier; this is effectively the size of steps the solver takes to iterate towards a minimum. Try the options beyond the default.\n",
    ">\n",
    "> The learning rate is a possible model hyper-parameter that one might need to consider for cross-validation and helping for model convergence particularly when we deal with neural networks later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50afe9-5a0b-463f-9c6b-f7a55d4dad20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "634d5191-c153-4137-ba01-9cb908aee0c6",
   "metadata": {
    "id": "634d5191-c153-4137-ba01-9cb908aee0c6"
   },
   "source": [
    "## Demonstration: Penguins data\n",
    "\n",
    "The example here considers doing classification on penguins data (which has three labels). We are going to do the case where there are two features (so I can show decision boundaries), and a case where I throw in all four features.\n",
    "\n",
    "Recall that in `penguins` the `species` feature is text. We can convert that to numerical values via `sklearn.preprocessing.LabelEncoder`; going to load `StandardScaler` at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307caf81-8f7c-4cb6-992f-40a67645a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the penguin data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# choose two particularly different features\n",
    "feature_names = [\"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# turn target from text to numerical values\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df[\"species\"])\n",
    "\n",
    "print(f\"{encoder.classes_} mapped to {encoder.transform(encoder.classes_)}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b9d98-6721-4ec9-b853-6b657e5b4792",
   "metadata": {
    "id": "c43b9d98-6721-4ec9-b853-6b657e5b4792"
   },
   "source": [
    "I am going to do this wrong first by using `SVM` by not scaling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b881430d-b74e-4c91-8df0-c7ba5bd45ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4167)\n",
    "\n",
    "# fit a model\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "\n",
    "# plot out the predictions (circles should lie on top of crosses if completely correct)\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.axes()\n",
    "ax.plot(Y_pred, 'bx', label=\"predictions\")\n",
    "ax.plot(Y_test, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(encoder.classes_)\n",
    "ax.set_title(r\"SVM WITHOUT data scaling (on test set)\")\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984a412-6052-473e-ae8f-ec2026a7c608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a prediction but on full data\n",
    "Y_pred = model.predict(X)\n",
    "\n",
    "colors = \"ryb\"  # red yellow blue\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for j in range(3): # 3 species\n",
    "    idx = np.where(Y == j)\n",
    "    ax.scatter(X[idx, 0], X[idx, 1], c=colors[j], edgecolor=\"k\",\n",
    "               s=16, label=encoder.classes_[j])\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    alpha=0.3,\n",
    ")\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_train,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    alpha=1.0,\n",
    "    levels=[0.5, 1.5],  # because the labels are 0 1 2\n",
    ")\n",
    "ax.set_xlabel(f\"{feature_names[0]}\"); ax.set_ylabel(f\"{feature_names[1]}\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6b6028-6490-4e53-8a2d-fd98451a565b",
   "metadata": {
    "id": "cb6b6028-6490-4e53-8a2d-fd98451a565b"
   },
   "source": [
    "So somehow it finds a boundary for `Chinstraps` to be absolutely tiny to the point it basically doesn't exist, so it makes no predictions for it. Lets see what happens if we standardise the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe269d1c-a3d5-4ecb-98ff-5b97ecf0a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise data X the proceed as usual\n",
    "\n",
    "X = df[feature_names].values\n",
    "# X = StandardScaler().fit_transform(X) # don't need to do Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4167)\n",
    "# JL: standardisation after train test split,\n",
    "#     standardisation before split will have data leakage, i.e. test data is somehow seen / leak into training\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)     # dont fit test set, only transform using fitted scalar\n",
    "\n",
    "# fit a model\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(\" \")\n",
    "\n",
    "# plot out the predictions (circles should lie on top of crosses if completely correct)\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.axes()\n",
    "ax.plot(Y_pred, 'bx', label=\"predictions\")\n",
    "ax.plot(Y_test, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(encoder.classes_)\n",
    "ax.set_title(r\"SVM with data scaling (on test set)\")\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ce723-15ed-4584-9f35-ff18c78473ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a prediction but on SCALED full data\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "Y_pred = model.predict(X_scaled)\n",
    "\n",
    "colors = \"ryb\"  # red yellow blue\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for j in range(3): # 3 species\n",
    "    idx = np.where(Y == j)\n",
    "    ax.scatter(X_scaled[idx, 0], X_scaled[idx, 1], c=colors[j], edgecolor=\"k\",\n",
    "               s=16, label=encoder.classes_[j])\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_scaled,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"pcolormesh\",\n",
    "    ax=ax,\n",
    "    cmap=plt.cm.RdYlBu,\n",
    "    alpha=0.3,\n",
    ")\n",
    "DecisionBoundaryDisplay.from_estimator(model, X_scaled,\n",
    "    response_method=\"predict\",\n",
    "    plot_method=\"contour\",\n",
    "    ax=ax,\n",
    "    alpha=1.0,\n",
    "    levels=[0.5, 1.5],  # because the labels are 0 1 2\n",
    ")\n",
    "ax.set_xlabel(f\"scaled {feature_names[0]}\"); ax.set_ylabel(f\"scaled {feature_names[1]}\");\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8f0ac2-b785-4798-a15f-b97f95ba64c2",
   "metadata": {
    "id": "9b8f0ac2-b785-4798-a15f-b97f95ba64c2"
   },
   "source": [
    "So here it is at least the separators are carving out a region for predicting `Chinstraps`, although the accuracy is not that high.\n",
    "\n",
    "What if we throw all the data in? We can evalute the scores although we can't really plot the boundaries anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ef09a-8007-4be7-93ce-1b22592ee820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as above but for all features\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "# X = StandardScaler().fit_transform(X) # don't need to do Y\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4167)\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)\n",
    "\n",
    "# fit a model\n",
    "model = svm.SVC()\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(\" \")\n",
    "\n",
    "# plot out the predictions (circles should lie on top of crosses if completely correct)\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.axes()\n",
    "ax.plot(Y_pred, 'bx', label=\"predictions\")\n",
    "ax.plot(Y_test, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.set_yticks([0, 1, 2])\n",
    "ax.set_yticklabels(encoder.classes_)\n",
    "ax.set_title(r\"SVM with data scaling (on test set)\")\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ea6dc-8b35-48e4-bbb6-8604a53f9820",
   "metadata": {
    "id": "f49ea6dc-8b35-48e4-bbb6-8604a53f9820"
   },
   "source": [
    "> <span style=\"color:red\">Q.</span> There is something to be said about decreasing the complexity by minimising the number of features. Consider keep the number of input features at two, but see which combination gives the best score.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> As above, but what if you do dimension reduction techniques on it first?\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try other options under SVM, and/or types of classifers (with various options).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Do robustness tests and cross-validation accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab8da0-0935-457d-ae27-c6c13c33d041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c4d417b-ae70-4f29-a1a9-f5d78c3b3727",
   "metadata": {},
   "source": [
    "---\n",
    "## c) Decision trees and ensemble methods\n",
    "\n",
    "An example of a ***decision tree*** is as below:\n",
    "\n",
    "<img src=\"https://i.imgur.com/UayMNei.png\" width=\"450\" alt='cat decision tree'>\n",
    "\n",
    "From a manual point of view it's not too hard to see how we create decision trees, but how would a machine do this? The goal of this notebook is to spend a bit of time introducing some concepts behind how a single decision tree is created, and how you might use these as classifiers and/or regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2497ff2-0085-4d96-9f74-d0065be193cb",
   "metadata": {},
   "source": [
    "### Anatomy of a decision tree\n",
    "\n",
    "Going to introduce some terminology so I can use it later:\n",
    "\n",
    "* Root: This would be the start of the tree (the \"is that for me?\" box above).\n",
    "* Leaves: The things at the end (the \"I don't want it\" and \"That's for me\" box above).\n",
    "* Node: The intermediate blocks connecting the root to the leaves (in this case strictly speaking there isn't one; leaves can be pure nodes with no subsequent connectors).\n",
    "* Branches: The connectors detailing the decisions (here it really should be the \"yes\" and \"no\").\n",
    "* Levels/depth: Distance from the node and maximum number of levels (I would call above depth 1, arguably it could be 2).\n",
    "* Parent/child: Usually refer to nodes that are directly connected. If a tree is going down like the above, the parent node would be the higher one (at a *lower* level; here the root node is the parent and the leaves are the child nodes).\n",
    "\n",
    "### Recap in probability, and the concept of information entropy\n",
    "\n",
    "(I want to spend a bit of time on this because similar ideas and terminology crop up when dealing with neural networks.)\n",
    "\n",
    "Recall that a ***probability*** $p_i$ is a value between 0 to 1 assigned to an event $X_i$ occurring, with the condition that the sum of all probabilities should be 1, i.e. $\\sum p_i = 1$. With the penguin data, this might be the probability of me randomly picking a sample out of the full dataset and the chances of me picking `Adelie`, `Chinstrap` or `Gentoo`. I am going to assume the process of drawing a sample is fair (i.e. follows a uniform distribution), then the probabilities are simply the number of samples of $X_i$ divided by the total sample size $N$. Let's actually do that in code form (because I want to use the result later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eb8e91-2d17-4605-a35f-1318c560819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of drawing a particular species\n",
    "\n",
    "# pandas can actually do this in one go\n",
    "freq = df[\"species\"].value_counts()\n",
    "p = df[\"species\"].value_counts(normalize=True)  # pandas can actually do this in one go\n",
    "\n",
    "print(freq)\n",
    "print(f\"total number of samples = {np.sum(freq)}\")\n",
    "print(\" \")\n",
    "print(p)\n",
    "print(f\"total probability = {np.sum(p)}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c45785-b0e9-4967-bdd2-283e6199d645",
   "metadata": {},
   "source": [
    "Thing to notice for later is that the dataset is unbalanced, with fewer samples in `Chinstrap`.\n",
    "\n",
    "If instead you want to find the probability in picking out a continuous variable like `body_mass_g`, then it doesn't make sense to talk about a probability of picking out a sample that has 4000 g, because it is almost surely not going to happen. Instead you talk about the probability of picking out a sample between (say) 3900 and 4100 g, i.e. you create ***bins*** or categories for the data to fall into first, then you proceed as before. This is of course just the same as creating a histogram, and in this case it is the area of the histogram (or pdf) that should sum (or integrate) to 1.\n",
    "\n",
    "A quantity of interest is called the ***information entropy*** or the ***Shannon entropy***, defined as\n",
    "\\begin{equation*}\n",
    "    H = \\sum_{i=0}^N H_i = -\\sum_{i=0}^N p_i \\log_a p_i,\n",
    "\\end{equation*}\n",
    "where I have not been very specific about the choice of base $a$. The original definition uses $a=2$, although $a=10$ and $a=e$ (so $\\log \\to \\ln$) will also work.\n",
    "\n",
    "> NOTE: It doesn't really matter because almost all the time it's the \"shape\" or functional behaviour of $H$ that is of importance, rather than its value. I am actually going to use the natural log (i.e. $a=e$), and I am simply going to denote that $\\log$ (rather than use $\\ln$). You can call the related logs in `numpy` via `np.log2`, `np.log10` and `np.log`.\n",
    "\n",
    "This quantity shows up in ecology as the Shannon index and is supposed to be a measure of species diversity. The way you may want to think about entropy is the amount of surprise or information you get from doing a random sample from the dataset: $H=0$ is no surprise and perfect information, while high $H$ means the converse. To see this, consider the two extremes:\n",
    "\n",
    "* If there is only one species, $p_0 = 1$ by construction, but $\\log 1 = 0$ for any base, so $H = 0$. Drawing a sample here provides zero surprise and information, because you can't get any other possibility anyway.\n",
    "* Convince yourself that $H$ is maximised for a uniform distribution, in which case $p_i = 1/(N+1)$, so $\\log 1/(N+1) = \\log 1 - \\log (N+1) = -\\log(N+1)$ for any base, so $H = (N+1)/(N+1)\\times \\log(N+1) = \\log(N+1)$. Diversity is thus maximum, and drawing a sample is maximally surprising because there is no bias in the dataset.\n",
    "\n",
    "The penguins data is not balanced and so the calculated Shannon entropy is not maximised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ed9c5-6c4c-4db3-ab28-cea9809d3cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shannon index for all penguin data based on species\n",
    "\n",
    "H = (-p * np.log(p)).sum()\n",
    "print(f\"H = {H:.6f} in base e\")\n",
    "\n",
    "# pick out number of unique entries\n",
    "num_unique = len(df[\"species\"].unique())\n",
    "print(f\"maximum possible Shannon entropy = {np.log(num_unique):.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533382ac-f668-4f31-b3b7-88f83d64e6fa",
   "metadata": {},
   "source": [
    "We can play the same game for continuous data; going to do this for the case `body_mass_g` feature as it serves to be an intermediate step to the next part in terms of some `pandas` syntax. The first part shows which bins it is actually going to dump the data in, and the second does the binning and computation of the probabilities, then entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b85110-5ace-4420-9f60-789bb7679e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cut the data and return bins\n",
    "out = pd.cut(df[\"body_mass_g\"], 5)\n",
    "bins = out.unique()  # find the number of unique entries based on intervals\n",
    "print(out.value_counts())\n",
    "print(\" \")\n",
    "\n",
    "p = out.value_counts(normalize=True)\n",
    "H = (-p * np.log(p)).sum()\n",
    "print(f\"H = {H:.6f} in base e\")\n",
    "\n",
    "# pick out number of unique entries\n",
    "num_unique = len(bins)\n",
    "print(f\"maximum possible Shannon entropy = {np.log(num_unique):.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48444dd-08aa-4570-ae3c-ded58891ab8a",
   "metadata": {},
   "source": [
    "### Information gain\n",
    "\n",
    "The value of the entropy by itself is not that useful by itself. The question to ask here is that suppose I already chose a feature to segment the data (say `species`), then given a choice of choosing the next feature to segment data on, which one should I choose? One way to do this is to choose the feature that gives you the maximum ***information gain***: if high entropy is low information, I want to choose the feature that lowers the resulting entropy as much as possible. Information gain is calculated as\n",
    "\\begin{equation*}\n",
    "    \\mbox{IG} = H_p - \\sum_{i=0}^N p_{c,i} H_{c,i},\n",
    "\\end{equation*}\n",
    "where $H_p$ is the entropy of the parent class, and $p_c$ and $H_c$ are the probabilities and entropy associated with the child classes.\n",
    "\n",
    "> NOTE: IG can be written in terms of the ***Kullback-Liebler*** (KL) divergence, which is an important concept in information geometry and used quite widely in ML. Not going to elaborate on what and how though; think of it as a distance between pdfs.\n",
    ">\n",
    "> IG is also close in form to ***cross entropy***, but also not going to elaborate how or why. ***Cross entropy*** is used for classification problems.\n",
    "\n",
    "Probably easier to explain with an example:\n",
    "\n",
    "1. If we take `species` as the parent feature, then we already calculated $H_p$ once above.\n",
    "2. If I take `body_mass_g` as the child feature to further segment on, then I need to bin the data accordingly (also done above).\n",
    "3. Find the number of `species` in those `body_mass_g` bins, and compute the occurrences and entropies accordingly. The below does this by creating the bins, finding the indices corresponding to the unique bins, pick out the data, and then computing the entropies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b750e59-0c6f-4de7-a15f-de01d312debc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# computing IG\n",
    "\n",
    "# compute entropy of parent first\n",
    "p = df[\"species\"].value_counts(normalize=True)\n",
    "H = (-p * np.log(p)).sum()\n",
    "\n",
    "# compute intermediate variables\n",
    "out = pd.cut(df[\"body_mass_g\"], 5)\n",
    "bins = out.unique()\n",
    "\n",
    "H_i, n_i = np.zeros(len(bins)), np.zeros(len(bins))  # using n later to compute p_i\n",
    "for i in range(len(bins)):\n",
    "    ind = (out == bins[i])\n",
    "    n_i[i] = df[ind][\"species\"].value_counts().sum()\n",
    "    p_i = df[ind][\"species\"].value_counts(normalize=True)\n",
    "    H_i[i] = (-p_i * np.log(p_i)).sum()\n",
    "\n",
    "# compute information gain\n",
    "IG = H - np.sum(H_i * n_i / np.sum(n_i))  # n_i / sum(n_i) = p_i\n",
    "print(f\"IG(parent = species, child = body_mass_g) = {IG:.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f4d662-fc50-4a1c-ab5b-7f251b184ceb",
   "metadata": {},
   "source": [
    "Positive IG here means the entropy is being reduced, which is expected. We should then check this for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8385765-dd6e-49ef-bc65-ff5f7181ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute IGs\n",
    "\n",
    "# compute entropy of parent first\n",
    "p = df[\"species\"].value_counts(normalize=True)\n",
    "H = (-p * np.log(p)).sum()\n",
    "\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "\n",
    "for child_feature in feature_names:\n",
    "\n",
    "    # compute intermediate variables\n",
    "    out = pd.cut(df[child_feature], 5)\n",
    "    bins = out.unique()\n",
    "\n",
    "    H_i, n_i = np.zeros(len(bins)), np.zeros(len(bins))  # using n later to compute p_i\n",
    "    for i in range(len(bins)):\n",
    "        ind = (out == bins[i])\n",
    "        n_i[i] = df[ind][\"species\"].value_counts().sum()\n",
    "        p_i = df[ind][\"species\"].value_counts(normalize=True)\n",
    "        H_i[i] = (-p_i * np.log(p_i)).sum()\n",
    "\n",
    "    # compute information gain\n",
    "    IG = H - np.sum(H_i * n_i / np.sum(n_i))\n",
    "    print(f\"IG(parent = species, child = {child_feature:<17}) = {IG:.6f}\")\n",
    "\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b80793-7458-49a7-a4cc-682c23611117",
   "metadata": {},
   "source": [
    "Note that the IG is the largest for `flipper_length_mm`, i.e. it's reducing the entropy the most. It is then suggesting that, given we chose `species` as a parent, we probably want to segment on `flipper_length_mm` next. Thus this is one measure on how the entropy can guide the creation of branches and leaves in decision trees.\n",
    "\n",
    "> NOTE: This is all relative and ordering matters: choosing `flipper_length_mm` as parent feature then `species` as child feature will not give the same IG.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Modify code accordingly to include both continuous and discrete labels as parent/child features to come for the IG. I would do the continuous ones first before I do the discrete ones; you probably need to convert the labels to numerical values first, and then dump that into the `df` as part of a `pandas` dataframe so you can use the subroutines above.\n",
    "\n",
    "To close, there is a similar quantity to the information entropy called the ***Gini index*** or ***Gini impurity***, which instead has the form\n",
    "\\begin{equation*}\n",
    "    G = \\sum_i p_i(1 - p_i).\n",
    "\\end{equation*}\n",
    "The Gini index measures \"impurities\" in the dataset: the case with a single feature would have $p_i = 1$, so $G = 0$ and is completely \"pure\", and corresponds also to the $H=0$ case. $G=1$ would be the maximally mixed case. Both can be used to construct something like an information gain measure like the above, although the Gini index is more computationally efficient for binary decisions such as the trees we are considering. Information entropy and related ideas have more links with neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70ae586-6a53-434e-9080-19569c770648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87f3edf1-fc71-4d12-894e-ffe5959c837d",
   "metadata": {},
   "source": [
    "---\n",
    "### Trees as classifiers\n",
    "\n",
    "So for classifiers we are going to take target $Y$ to the `species` feature. Going to load a whole load of things and to demonstrate some features that one could dig into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081ad4b5-e023-47f0-bdd3-d9a1977fbe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "# X = StandardScaler().fit_transform(X)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df[\"species\"])\n",
    "\n",
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=4167)\n",
    "scalar = StandardScaler()\n",
    "X_train = scalar.fit_transform(X_train)\n",
    "X_test = scalar.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e81d1f-2ed8-4fe7-a071-e843afd7efd7",
   "metadata": {},
   "source": [
    "Going to fit a deliberately shallow tree (by specifying `max_depth=2`), and force it to use the information entropy criterion with `criterion=\"entropy\"`. I am also going to query the feature importance in the fitted model with `model.feature_importances_`.\n",
    "\n",
    "> NOTE: I am overriding the defaults are `max_depth=None` and `criterion=\"gini\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6af180-f7dc-4cef-a37d-d7cf3300d207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a decision tree (deliberately shallow for showing)\n",
    "model = DecisionTreeClassifier(max_depth=2, criterion=\"entropy\")\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf2bc3-a65c-4789-ab77-e97652524822",
   "metadata": {},
   "source": [
    "The model does pretty well on the testing dataset. It also seems to be finding that `flipper_length_mm` is the most important feature, which is potentially consistent with our IG calculation above.\n",
    "\n",
    "Note also that (on mine at least) the tree doesn't actually use the `body_mass_g` feature at all, because `body_mass_g` has zero feature importance. This is partly because I forced `max_depth=2` (so I can't have that much complexity anyway), and it basically managed to classify all the data with three features. This would be an early termination criterion: if all data is accounted for, stop any further creation of nodes.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Why am I only using \"potentially\"? What is wrong with my assertion/statement here? (How is IG defined?)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Convince yourself that for this case it doesn't matter really if I scale the data or not. In other words, decision trees are ***scale-invariant*** (but I am still scaling because it possibly lessens the computational burden a bit).\n",
    "\n",
    "The thing with decision trees (and somewhat unlike most neural networks) you could actually look at the tree that gets formed so the model is arguably interpretable, although of course if your model is sufficiently complex that may be a mute point anyway. In the above I deliberately forced the tree to be simple so the display coming up is a bit easier to see. I can use `plot_tree` to actually plot a figure, or `export_text` to spit out the entries in the decision tree.\n",
    "\n",
    "> NOTE: I passed in the `features_names=` argument so that it would display the feature names, otherwise it would display $X[2]$ etc., which refers to the third feature (in this case because Python counts from zero), which is `flipper_length_mm`.\n",
    ">\n",
    "> The colours in the trees denote the classes, as can be seen from the text description of the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a5fe9-a199-45ba-99d1-07fa8d84408a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree, export_text\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "plot_tree(model, filled=True, proportion=True, feature_names=feature_names);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62de0b-ee0d-477a-ab49-2feae77c17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export as text\n",
    "print(export_text(model, feature_names=feature_names))\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbd1bf-a319-4931-abb9-499e670f401d",
   "metadata": {},
   "source": [
    "If we do not force the maximum depth then we get the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a992ed-efc5-4b3a-8595-241dfc1a0c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a decision tree (deliberately shallow for showing)\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")\n",
    "\n",
    "print(export_text(model, feature_names=feature_names))\n",
    "print(\" \")\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = plt.axes()\n",
    "plot_tree(model, filled=True, proportion=True, feature_names=feature_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11664e65-56aa-43a7-bd9e-3590789b3ce9",
   "metadata": {},
   "source": [
    "Note in this case class 2 (which is `Gentoo`) has a termination node that is quite high up on the tree close to the root node. If you remember this is consistent with our observations that `Gentoo` data seems to live in a different part of the feature space compared to the other two classes.\n",
    "\n",
    "### Decision boundaries\n",
    "\n",
    "We can plot out the decision boundaries easily if we only models using two features. Below takes three specific combinations just to demonstrate what these look like.\n",
    "\n",
    "> NOTE: Note I've standardised the data so the units are not quite right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfea4e5-1337-4bba-8dc2-a81758a462fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise the decision boundary\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "combos = [[1, 0], [2, 0], [3, 0]]  # pre-define some pairs of features\n",
    "colors = \"ryb\"  # red yellow blue\n",
    "\n",
    "fig = plt.figure(figsize=(9, 3))\n",
    "\n",
    "for i in range(3):\n",
    "    # train a new model per cycle\n",
    "    model = DecisionTreeClassifier().fit(X_train[:, combos[i]], Y_train)\n",
    "\n",
    "    ax = plt.subplot(1, 3, i+1)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        model,\n",
    "        X_train[:, combos[i]],\n",
    "        cmap=plt.cm.RdYlBu,\n",
    "        response_method=\"predict\",\n",
    "        ax=ax\n",
    "        )\n",
    "    # plot the actual data points in\n",
    "    for j in range(3): # 3 species\n",
    "        idx = np.where(Y_train == j)\n",
    "        ax.scatter(X_train[idx, combos[i][0]], X_train[idx, combos[i][1]],\n",
    "                   c=colors[j], edgecolor=\"k\", s=10, label=f\"{encoder.classes_[j]}\")\n",
    "    ax.set_xlabel(f\"{feature_names[i+1]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(f\"{feature_names[0]}\")\n",
    "ax.legend()\n",
    "\n",
    "fig.suptitle(r\"boundaries based on pairs of features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d71a91-227d-463e-b2a1-b4e416101f58",
   "metadata": {},
   "source": [
    "### Dependence on `max_depth`\n",
    "\n",
    "If we don't specify `max_depth` then the trees goes on until the leave nodes are \"pure\", i.e. either entropy or Gini index is zero. It is sometimes a good idea to limit `max_depth` as a control to possiblities of over-fitting. Below code changes that parameter to see how the skill varies as a function of `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b596176-b2ea-41f7-968c-d902814db800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependence of skill with varying max_depth\n",
    "\n",
    "for max_depth in range(1, 6):\n",
    "    # fit a decision tree with fixed seed\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                                   max_depth=max_depth, \n",
    "                                   random_state=167)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # basic skill diagnostics\n",
    "    Y_pred = model.predict(X_test)\n",
    "    N = len(Y_test)\n",
    "    skill_all = np.sum(Y_pred == Y_test)\n",
    "    print(f\"max_depth = {max_depth} overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "    print(f\"fitted feature importance [1 = max]:\")\n",
    "    for i in range(len(feature_names)):\n",
    "        print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f0092a-42f8-4d42-86d8-3b6b81acb17d",
   "metadata": {},
   "source": [
    "It looks like the model is reaching its peak performance around `max_depth=2` or `max_depth=3`.\n",
    "\n",
    "### Dependence on `criterion`\n",
    "\n",
    "Below does the same as above but using the (default) Gini criterion instead. Performance ends up being about similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0977b25-25dd-4870-99d1-8f52deb4abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependence of skill with varying max_depth\n",
    "\n",
    "for max_depth in range(1, 6):\n",
    "    # fit a decision tree with fixed seed\n",
    "    model = DecisionTreeClassifier(max_depth=max_depth, random_state=167)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # basic skill diagnostics\n",
    "    Y_pred = model.predict(X_test)\n",
    "    N = len(Y_test)\n",
    "    skill_all = np.sum(Y_pred == Y_test)\n",
    "    print(f\"max_depth = {max_depth} overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "    print(f\"fitted feature importance [1 = max]:\")\n",
    "    for i in range(len(feature_names)):\n",
    "        print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf3bd7-69e5-435f-9f1b-dfe66415db1c",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">Q.</span> There are a whole load of other parameters one could specify for decision trees (e.g. see [here](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)). Have a play around with those.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> I can think you pass further options into `plot_tree` to beautify it accordingly, have a look at doing that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782c939a-756b-4790-8940-0f8ee0b26038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fef105a5-61d3-44c2-925f-b51810dc8bb8",
   "metadata": {},
   "source": [
    "## Ensemble methods\n",
    "\n",
    "### TL;DR: ***Random forests*** and ***gradient boosting*** is just ***ensembles*** of decision trees.\n",
    "\n",
    "<img src=\"https://i.imgur.com/vlEyC0x.jpeg\" width=\"500\" alt='forests of randoms'>\n",
    "\n",
    "Loading data here again in case you want to run the below without the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f97c29b-0af6-4d71-b6ff-bfc297238a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the penguin data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce88ffaf-6f9c-43c8-bdd1-419b725e9966",
   "metadata": {},
   "source": [
    "### Random forests\n",
    "\n",
    "The idea here is that a single decision tree is prone to over-fitting, and the structure of the tree itself can demonstrate quite a bit of variability in the eventual classification/regression depending on initialisation and/or choice of data. One way to get around this is to train up a collection of trees (i.e. a \"forest\") and take an average of the decisions, and call that final product your classifer/regressor. Because of the aveaging operation you might suspect the resulting model to be more robust, display less variance and be less prone to over-fitting.\n",
    "\n",
    "The way this roughly works is as follows the pictorial below (with exaggerations):\n",
    "\n",
    "<img src=\"https://i.imgur.com/bOy1l4N.jpeg\" width=\"1200\" alt='schematic'>\n",
    "\n",
    "### 1) Bootstrap sampling\n",
    "\n",
    "In your training set you create sub-samples of the training data via ***bootstrap sampling***, a bit like when you did $K$-fold cross-validation. The difference here is you **allow for replacement** of data. As an example, if the whole dataset is $[a, b, c, d, e]$ and I allow sampling four of these to form one of my sub-training sets, then the following are permissiable under bootstrap sampling but not in $K$-fold splitting:\n",
    "\n",
    "* $[a, b, a, c]$\n",
    "* $[a, b, b, a]$\n",
    "* $[a, a, a, a]$\n",
    "\n",
    "Note that $[a, e, c, d]$ would be permissable under both with and without replacement sampling.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> In reality if you were doing $K$-folds you probably wouldn't get $[a, e, c, d]$ as one of the folds anyway, why is that?\n",
    "\n",
    "### 2) Bagging (or boostrap aggregation)\n",
    "\n",
    "***Bagging*** is just you training models based on the dataset obtained from bootstrap aggregation from the above. The ***random*** part in the \"random forest\" comes from the fact that the bootstrap sampling is going to introduce randomness in the created models.\n",
    "\n",
    "Note the ensemble members are in general going to be weak learners, because the exposed sub-sample data size will likely be low. The idea is that you have multiple weak learners that aggregate into a robust stronger learner. See the [original paper](https://link.springer.com/article/10.1023/A:1018054314350) for why this should work.\n",
    "\n",
    "### 3) Averaging / voting\n",
    "\n",
    "Once you have the ensemble members, then when you use the ensemble to make a prediction, all of the members make a prediction, and the result is averaged for a regressor, or a majority vote is taken for a classifer.\n",
    "\n",
    "Note because this is an ensemble method, some measures of probability in the predictions are in principle given.\n",
    "\n",
    "> NOTE: The things described above can in principle be applied to other methods (e.g. linear models, neural networks), although it is most commonly used with decision trees.\n",
    ">\n",
    "> In `sklearn` the averaging and voting is simple in that the weights are uniform (no biases). You also only get votes (as a probability) in the classifier. Could in principle bully the code to deal with uneven weights, but this is not demonstrated here.\n",
    "\n",
    "Below code demonstrates the usage of random forests with the penguin data, first using the `RandomForestClassifer` object. I am deliberately forcing the resulting trees to be shallow. with `max_depth=2`, and I create 20 trees here (`n_estimators=20`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddcc7c-4e0e-4b10-87de-30e53b126a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "# aim: predict species from features (exactly as in trees basically)\n",
    "feature_names = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[feature_names].values\n",
    "\n",
    "# turn target from text to numerical values\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df[\"species\"])\n",
    "\n",
    "# withhold 20% of data that model training does not see, and use that to test skill\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=4167)\n",
    "\n",
    "# use an ensemble of 20 trees (but same max_depth and criterion as before)\n",
    "model = RandomForestClassifier(max_depth=2, n_estimators=20, criterion=\"entropy\",\n",
    "                              random_state=4167)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d9027-b2bb-4746-b2a9-97cb57a7b647",
   "metadata": {},
   "source": [
    "Note in contrast to the decision tree example in the previous section, when I specify `max_depth=2` I end up not using the `body_mass_g` feature. Here the feature importance shows up as a non-zero (but small) value, indicating that some of the ensemble members did pick that feature up to do segmentation on.\n",
    "\n",
    "We can visualise some of the ensemble members below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a5d5e-a08b-48ca-80b7-d046d2e6387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise some of trees\n",
    "\n",
    "# randomly choose 4 of the \"n_estimators\" to plot\n",
    "inds = np.arange(20)\n",
    "np.random.shuffle(inds)\n",
    "ind = inds[:4]  # pick out the first four\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    ax = plt.subplot(2, 2, i+1)\n",
    "    plot_tree(model.estimators_[ind[i]], filled=True, proportion=True, ax=ax,\n",
    "             feature_names=feature_names, class_names=encoder.classes_)\n",
    "    ax.set_title(f\"tree #{ind[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66b609-4eb5-4891-a404-12cfe4ec6513",
   "metadata": {},
   "source": [
    "The `RandomForestClassifier` object comes with probability measures that we can probe, using `model.predict_proba()`. `X_test` should be the input in this case, and the outputs are the probabilities over the ensemble (of size `n_estimators`) of predicting class `0`, `1` or `2` (corresponding to `Adelie`, `Chinstrap` and `Gentoo`).\n",
    "\n",
    "In the below I plot the probabilities, and the classifier returns the predicted label associated with the largest probability. I also label the actual labels by the bars with the brightest bar. Notice then the tallest bars do not correspond to the brightest, i.e. a wrong classification, although it is mostly the case. This observation is consistent with the summary statistics above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c232998b-ba62-4314-be00-9d91d15b76db",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict_proba(X_test)\n",
    "x = np.arange(len(Y_test))\n",
    "width = 0.2\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "\n",
    "# do a quick plot for legend purposes but never actually show it\n",
    "x_temp = -10\n",
    "for j in range(3):\n",
    "    ax.bar(x_temp+(j-1)*width, probs[0, j], width=width, color=f\"C{j}\",\n",
    "           label=f\"{encoder.classes_[j]}\")\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([10.6, 20.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([40.6, 50.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlabel(\"test data point #\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606bd92b-3f69-4e87-aced-2f2ca9e224af",
   "metadata": {},
   "source": [
    "### Out-Of-Bag score\n",
    "\n",
    "This is a measure of how we might expect the models to generalise to new data. First, ***out-of-bag*** is the complement of the bagged samples: following example above, if your whole dataset is $[a,b,c,d,e]$, and the bagging gives a subset of $[a, c, b, a]$, then the out-of-bag part is\n",
    "\\begin{equation*}\n",
    "    \\mbox{OOB} = [a,b,c,d,e] \\setminus [a,c,b,a] = [a,b,c,d,e] - [a,b,c] = [d, e],\n",
    "\\end{equation*}\n",
    "where $\\setminus$ is to mean be a set operation and ignoring duplicates.\n",
    "\n",
    "The out-of-bag score is then the average skill of the ensemble members in predicting things out-of-bag. To enable this metric we pass `oob_score=True` in (it is `False` by default because then you do fewer calculations). A value of 1 means the members all predicted with 100% accuracy, and a value closer to 1 would suggest models are likely going to generalise better to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d427bc8-57ea-4dbb-86e2-6a0d73d814fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use an ensemble of 20 trees (but same max_depth and criterion as before)\n",
    "model = RandomForestClassifier(max_depth=2, n_estimators=20, criterion=\"entropy\",\n",
    "                              random_state=4167, oob_score=True)\n",
    "model.fit(X_train, Y_train)\n",
    "print(f\"model OOB accuracy = {model.oob_score_*100:.4f}%\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cf607-24a1-45da-bb66-4e647e9764a7",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">Q.</span> For me the problematic test set data seems to be at index 13, 41 and 42. Have a look and those associated data points and see if there is a reason they are particularly hard for the prediction step (e.g. are they in a region where the data is particularly \"mixed\"?)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try some of the above for `RandomForestRegressor`, e.g. do the prediction problem I did last time. The thing you don't have access to are the probabilities of the predictions (the returned value is an average of all ensemble members).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Consider changing other things such as pruning parameter `ccp_alpha`, `max_depth`, `min_samples_leaf`, `criterion` and see how performance differs. Could also do cross-validation and hyper-parameter tuning accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e101060-a688-4a3a-98e3-2db205fa5106",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a40834f-36ca-40e8-bd66-f83beb69e3bc",
   "metadata": {},
   "source": [
    "### Gradient boosting\n",
    "\n",
    "The idea of ensemble methods is that you train a whole load of weak models, but the variance of the models are reduced through some sort of averaging procedure, leading to ensemble predictions that are more robust, but it may or may not say anything about the bias. The idea of ***boosting*** is to target the bias by targeting the problematic predictions, and considers the following pipeline:\n",
    "\n",
    "1. Train an ensemble of models again, and identify the particularly weak models, i.e. the ones that got misclassified or had particularly large prediction errors.\n",
    "2. Take those biases and compute residuals/mismatches/losses, change the weighting of the **data** to improve those particularly weak models, with the aim to \"boost\" the performance of the overall ensemble.\n",
    "5. Re-train the model, repeat the processing of weight updates, and iterate until you get some convergence of the overall ensemble.\n",
    "\n",
    "In that sense it's an optimisation problem (again!), hence my usage of familiar terms. The ***gradient*** part is that it uses (stochastic) gradient descent type methods to solve the optimisation problem associated with ***boosting***.\n",
    "\n",
    "The demonstration below applies `GradientBoostingClassifier` to the same problem as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca741e-bb31-498e-9a75-5f742d024ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# use an ensemble of 20 trees (but same max_depth and criterion as before)\n",
    "model = GradientBoostingClassifier(max_depth=2, n_estimators=20,\n",
    "                                   random_state=4167)\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# basic skill diagnostics\n",
    "Y_pred = model.predict(X_test)\n",
    "N = len(Y_test)\n",
    "skill_all = np.sum(Y_pred == Y_test)\n",
    "print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "print(f\"fitted feature importance [1 = max]:\")\n",
    "for i in range(len(feature_names)):\n",
    "    print(f\"    {feature_names[i]:<20} = {model.feature_importances_[i]:.4f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a9234a-d906-4567-9de9-98007b3a9a97",
   "metadata": {},
   "source": [
    "So in this case with basically the same settings as the `RandomForestClassifer` the skill on the test set is comparable to the random forests. What is slightly different are the associated probabilities in the resulting trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f26c24-3eef-4422-a77c-0f201d01cb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the associated probabilities\n",
    "probs = model.predict_proba(X_test)\n",
    "x = np.arange(len(Y_test))\n",
    "width = 0.2\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "\n",
    "# do a quick plot for legend purposes but never actually show it\n",
    "x_temp = -10\n",
    "for j in range(3):\n",
    "    ax.bar(x_temp+(j-1)*width, probs[0, j], width=width, color=f\"C{j}\",\n",
    "           label=f\"{encoder.classes_[j]}\")\n",
    "\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([10.6, 20.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "for i in range(len(Y_test)):\n",
    "    x_temp = x[i]\n",
    "    truth = Y_test[i]\n",
    "    for j in range(3):\n",
    "        if j == truth:\n",
    "            alpha=1.0\n",
    "        else:\n",
    "            alpha=0.5\n",
    "        ax.bar(x_temp+(j-1)*width, probs[i, j], width=width, alpha=alpha,\n",
    "               color=f\"C{j}\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xlim([40.6, 50.4])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_xlabel(\"test data point #\")\n",
    "ax.set_ylabel(r\"$p$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf0afe-0ab8-42b7-b4fd-ef10fb169c7c",
   "metadata": {},
   "source": [
    "The thing that is probably true in this case is that where the bars were already tall in the random forest case it is even taller here.\n",
    "\n",
    "Interestingly, both classifers are failing at the same prediction points (for me it's index 13, 41 and 42). It is perhaps of interest to have a look at what is going on there.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> For me the problematic test set data seems to be at index 13, 41 and 42. Have a look and those associated data points and see if there is a reason they are particularly hard for the prediction step (e.g. are they in a region where the data is particularly \"mixed\"?)\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try some of the above for `GradientBoostingRegressor`, e.g. do the prediction problem I did last time. The only thing you don't have access to are the probabilities of the predictions (the returned value is an average of all ensemble members).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Consider changing other things such as pruning parameter `ccp_alpha`, `max_depth`, `min_samples_leaf`, `criterion` and see how performance differs. Could also do cross-validation and hyper-parameter tuning accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfb67a-e356-4de0-baf4-55987acc7337",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f8895c8-6338-46df-aa02-6f2383920c78",
   "metadata": {
    "id": "6f8895c8-6338-46df-aa02-6f2383920c78"
   },
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Other classifiers\n",
    "\n",
    "There are other classifiers I didn't go through because they either follow the same principles we have encountered before, or they are completely different (e.g. dealing quite a bit with probability). These include:\n",
    "\n",
    "* Ridge classification ($L^2$ penalisation as in linear models; can also do regression).\n",
    "* Logistic classification (it's called `LogisticRegression` in `sklearn` though; it is part of `linear_model`, and is a special case of Generalised Linear Models).\n",
    "* Nearest Neighbours (somewhat related to clustering things we have seen; can also do regression).\n",
    "* Gaussian Processes (probabilistic classifiers; can also do regression).\n",
    "* Naive Bayes and its extensions (as above).\n",
    "\n",
    "Have a look at some of these.\n",
    "\n",
    "(I personally think the Gaussian Processes and Bayesian approaches are the most interesting, because they can provide soft boundaries and measures of uncertainty. I guess just using them is fine, but understanding what they do with take a bit more work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155cc9a0-b8fc-475a-8d04-f1b4edc7805b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab159ed9-caf8-4f46-9745-9f62696e5738",
   "metadata": {
    "id": "ab159ed9-caf8-4f46-9745-9f62696e5738"
   },
   "source": [
    "## 2) Classifying cats and dogs\n",
    "\n",
    "#### (This one may be part of an upcoming assignment.)\n",
    "\n",
    "Try and do cats and dogs classification. You may or may not want to use dimension reduction approaches first.\n",
    "\n",
    "You may also want to revisit again after all the subsequent lectures (decision trees, random forests, neural networks). This is a hard test in general and skill is not expected to be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5ce21d-a8fe-4db5-87cb-f751dc7266b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ba34e7b-7c6c-4121-a985-0a67799df08d",
   "metadata": {
    "id": "9ba34e7b-7c6c-4121-a985-0a67799df08d"
   },
   "source": [
    "## 3) More with SVM\n",
    "\n",
    "You can use `svm.SVR` for doing regression, try that with some previous examples encountered.\n",
    "\n",
    "There are also other variants of SVM (`NuSVC` and `NuSVR`), have a look and see what those are, apply them to some examples considered here and see how the results compare accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc73d9d-e3f2-4dbe-b22b-02f500b92671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "484f8418-d7de-4b82-b3d9-ac9449c984eb",
   "metadata": {
    "id": "484f8418-d7de-4b82-b3d9-ac9449c984eb"
   },
   "source": [
    "## 4) Regression\n",
    "\n",
    "You can do regression with the SGD and random forests / gradient boosting stuff too. Have a look on the manual on what kind of loss functions you can use etc. and apply them to some examples considered here or previously also (e.g. penguins data, time series forecasting with `elnino34_sst.data`, Lotka-Volterra, Lorenz, or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7297ef1-2b26-46e9-874e-542b585d7bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3673720-407f-425b-aa1f-316922694a02",
   "metadata": {},
   "source": [
    "## 5) Image classification and/or extrapolation\n",
    "\n",
    "This is a hard problem. Going to the raw images of cats and/or dogs, consider training\n",
    "\n",
    "1. Classifiers for cats and dogs,\n",
    "2. Regressors on half the face and using it to predict the other half. Analyse the skill on training data, testing data, the need for standardising the data, analysis of the model coefficients, dependence on the propotion of face exposed to model, cross-validation and etc.\n",
    "\n",
    "From a coding point of view it is potentially easier to use the left half of the face to predict the right half (idea below). You can try top and bottom also.\n",
    "\n",
    "Just be aware if you use the full image it might be slow (I never managed to get `GradientBoostingRegressor` to converge for the regression problem, although you could try `HistGradientBoostingRegressor` instead). You may or may not want to do dimension reduction on these first, which will probably help speed up the classification problem quite a bit; not sure about the regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e541aa-c775-4575-8385-2b2edebad27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d7c107c-c0af-496e-bdfe-e7768e0a40e3",
   "metadata": {},
   "source": [
    "## 6) Extreme Gradient Boosting with XGBoost\n",
    "\n",
    "One significant advancement in machine learning is the advent of ***extreme gradient boosting***. Look up what extreme gradient boosting means and what it is doing. The Python package `xgboost` has gained a lot of popularity. See if you can use `xgboost` for the `penguins` data; see documentation [here](https://xgboost.readthedocs.io/en/stable/get_started.html).\n",
    "\n",
    "Google Colab has `xgboost` installed by default. **Be very careful with hyperparameters (`n_estimators`, `max_depth` etc.)! Control your model wisely and do not go insane with parameters!**\n",
    "\n",
    "Notice any advantages / disadventages of `xgboost`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa5ba4d-58bc-4caa-8a4e-6c56b3e2b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost\n",
    "# from xgboost import XGBClassifier\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
