{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4807e94-963f-41df-b5d1-086e3df6c76b",
   "metadata": {},
   "source": [
    "*updated 16 Jan 2026, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 5303 \"AI and Machine Learning in Ocean Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES5303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844859c7-e5a4-4df2-b2dc-b4386bd70af8",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Introduction to neural networks and MLPs\n",
    "\n",
    "***Neural Networks*** form a large part of modern day machine learning. Recall that the problem of regression or supervised learning is to find $N$ in $N(X) = Y$ given inputs and outputs $X$ and $Y$. [***Universal approximation theorems***](https://en.wikipedia.org/wiki/Universal_approximation_theorem) essentially say that with enough complexity these neural networks can approximate functions $f$ with sensible properties arbitrarily well. We are more or less going to focus on neural networks for the rest of the course. \n",
    "\n",
    "The first of these sessions will introduce the anatomy of what actually goes into neural networks and the principles behind how these networks are trained, illustrating these with particularly simple network architectures. The remaining lectures can then be regarded as theme and variations on this: the details differ, but the ideas remain largely the same. We are going to use the [***PyTorch***](https://pytorch.org/) interface, and various capabilities with PyTorch will be introduced in steps (rather than all of it one go, because there is quite a lot of it).\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Basic anatomy of a neural network as an approximation of an operator.\n",
    "> 2. Training of neural networks via back-propagation and (stochastic) gradient descent.\n",
    "> 3. Perceptrons and Multi-Layer Perceptrons as a basis of neural networks.\n",
    "> 4. Basic usage of `PyTorch` for building and training of neural networks.\n",
    "\n",
    "Note that the implementation in this first session will be quite \"raw\", in that we do the things in a reasonably rudimentary way. There are better ways of doing it through `PyTorch` functionalities that we will gradually introduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928ddb6-de42-4475-8edf-389eb0ff6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33787ae-84ab-484f-9dcb-8695b103dfd9",
   "metadata": {},
   "source": [
    "---\n",
    "## a) Background concepts\n",
    "\n",
    "Below is a schematic of a neural network $N$ I cooked up for illustrating the key parts of a neural network and what it does.\n",
    "\n",
    "<img src=\"https://i.imgur.com/LFTTTa5.jpeg\" width=\"600\" alt='schematic neural network'>\n",
    "\n",
    "The basic idea of a neural network is a thing that takes input $X$ and predicts a $\\hat{Y}$. In my case this neural network has one single ***hidden layer*** (the components between the two purple lines) consisting of two nodes denoted $h_1$ and $h_2$. The $h_1$ and $h_2$ do elementary operations on the $X = (x_1, x_2)$ as\n",
    "\\begin{equation*}\n",
    "    h_1 = f(w_1 x_1 + w_3 x_2 + b_1), \\qquad h_2 = f(w_2 x_1 + w_4 x_2 + b_2).\n",
    "\\end{equation*}\n",
    "\n",
    "We have:\n",
    "* $w_i$ are the ***weights*** associated with the nodes that multiplies to the input(s).\n",
    "* $b_i$ are the ***biases*** that get added.\n",
    "* $f$ is called an ***activation function*** that eats some numbers and spits out another number.\n",
    "\n",
    "> NOTE: In some manuals these may be referred to as ***affine transformations***.\n",
    "\n",
    "In the end $N$ eats some numbers and spits some stuff out: this would be the ***feed-forward*** (the terminology is presumbly drawn from *control theory* in the engineering field).\n",
    "\n",
    "To see a feed-forward in action, I'll do one of these calculations manually; the below will be a code version of this. Suppose $X = [-1, 1]$, and my weight vector is $w = (1, 2, 3, 4)$, my biases are $b = (0, 1)$, and I my activation function is trivial, i.e. I have the identify function $f(x) = x$ for all values. Then at $h_1$, I have the following intermediate steps:\n",
    "\n",
    "1. $x_1 w_1 + x_2 w_3 + b_1 = (-1)(1) + (1)(3) + 0 = -1 + 3 + 0 = 2$\n",
    "2. $f(x_1 w_1 + x_2 w_3 + b_1) = f(2) = 2$\n",
    "\n",
    "So $h_1(X) = 2$. Convince yourself that $h_2(X) = 3$, thus $N(X) = \\hat{Y} = 2 + 3 = 5$. A coded up version of it looks like the below: I lumped $w$ and $b$ into an array variable called $\\theta$ (`theta`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82efbc9a-99e9-4cd6-92c1-7db768a50811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_nn(X, theta, activation=\"unity\"):\n",
    "    # pull out relevant numbers\n",
    "    x_1, x_2 = X\n",
    "    w_1, w_2, w_3, w_4 = theta[:4]\n",
    "    b_1, b_2 = theta[4:]\n",
    "\n",
    "    # compute arguments (note this could be written as a matrix multiplication)\n",
    "    h_1 = w_1 * x_1 + w_3 * x_2 + b_1\n",
    "    h_2 = w_2 * x_1 + w_4 * x_2 + b_2\n",
    "\n",
    "    # pass through activation function\n",
    "    match activation:\n",
    "        case \"unity\":\n",
    "            h_1, h_2 = h_1, h_2\n",
    "        case \"tanh\":\n",
    "            h_1, h_2 = np.tanh(h_1), np.tanh(h_2)\n",
    "\n",
    "    # return final output\n",
    "    return h_1 + h_2\n",
    "\n",
    "X = (-1, 1)\n",
    "theta = (1,2,3,4,0,1)\n",
    "\n",
    "print(f\"output of simple_nn(X) is {simple_nn(X, theta):.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe97fa-9a81-424b-b855-69624d287832",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "While I didn't do it above, try and convince yourself that as long as I deal don't have a non-trivial ***activation function*** then most things I wrote above can in fact be written as a matrix multiplication. That's not entirely surprising in hindsight, because my nodes and each of the hidden layers are just doing sums and additions, i.e. linear operations, and linear operations (or affine transformations) can be represented by matrices. Each hidden layer $\\ell$ is represented by a matrix $A_\\ell$, but\n",
    "\\begin{equation*}\n",
    "    A_1(X) = a_1, \\quad A_2(a_1) = a_2, \\quad \\ldots A_M(a_{M-1}) = \\hat{Y}\n",
    "\\end{equation*}\n",
    "is really just\n",
    "\\begin{equation*}\n",
    "    A_M(\\ldots A_3(A_2(A_1(X)))) = A(X) = \\hat{Y},\n",
    "\\end{equation*}\n",
    "so our task just boils down to finding $A$.\n",
    "\n",
    "However, we don't want to just stick with linear operations, because that is somewhat limited. One way that neural networks add in nonlinearity is through ***activation functions*** $f$. Then we have instead\n",
    "\\begin{equation*}\n",
    "    A_1(X) = f(a_1), \\quad A_2(f(a_1)) = f(a_2), \\quad \\ldots A_M(f(a_{M-1})) = \\hat{Y}\n",
    "\\end{equation*}\n",
    "but the chain $A_M(\\ldots A_3(A_2(A_1))) = A$ no longer holds because there is nonlinearity. The universal approximation theorems applies to this case, so we can build a wider variety of operators from simple components.\n",
    "\n",
    "Common activation functions are illustrated below. Some of these you have seen already in another form: they are just flipped versions of the one-sided loss functions from when we were doing classification. Indeed, the code below I literally just copied and pasted from a previous lecture and modified/deleted a few lines..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d84f2b-7dec-42b9-a13b-f525aaa33e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample activation functions\n",
    "\n",
    "slope = 0.1\n",
    "\n",
    "X = np.linspace(-4, 4, 101)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(X, 1.0 / (1.0 + np.exp(-X)), label=\"sigmoid/logistic\")\n",
    "ax.plot(X, np.tanh(X), label=\"tanh\")\n",
    "ax.plot(X, np.maximum(X, 0), label=\"ReLU\")\n",
    "ax.plot(X, slope * np.minimum(X, 0) + np.maximum(X, 0), label=f\"leaky ReLU (slope={slope})\",\n",
    "        zorder=-1)  # force it to be below other plots\n",
    "ax.set_ylim((-1.5, 4))\n",
    "ax.legend()\n",
    "ax.set_xlabel(r\"$X$\")\n",
    "ax.set_ylabel(r\"$f(X)$\")\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3758e7-c015-423a-8fd0-04e33f7fcf8c",
   "metadata": {},
   "source": [
    "The idea of activations are that they are at least piecewise-differentiable (for reasons to be detailed later), and control the value of the outcome. The examples given above are:\n",
    "\n",
    "* ***Sigmoid*** generally refers to something that is $S$-shaped, and one example is the ***logistic function*** demonstrated above. This example is bounded between [0, 1].\n",
    "* ***Hyperbolic-tangent*** (or tanh) is like the above. The standard form is bounded between [-1, 1].\n",
    "* ***Rectified Linear Unit*** or (ReLU) is like the flipped version of the hinge loss from before, and the standard form is bounded below by zero.\n",
    "* Leaky ReLU is like ReLU but has an extra part with a slope in the negative part.\n",
    "\n",
    "The thing with sigmoid and ReLU is that if the inputs are (sufficiently) negative then you just get zero; this is refered to as a neuron not \"firing\" (because it spits out nothing), and partly the reason why these are called activation functions.\n",
    "\n",
    "In the subroutine above I implemented the `tanh` activation function. The intermediate steps we have are\n",
    "\\begin{equation*}\n",
    "    h_1(X) = \\tanh(2), \\qquad h_2(X) = \\tanh(3),\n",
    "\\end{equation*}\n",
    "so $\\mathcal{N}(X) \\approx 1.959...$, which is also what is returned by a call of the subroutine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965125b-aab7-4af2-b5d8-c575c0b39eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (-1, 1)\n",
    "theta = (1,2,3,4,0,1)\n",
    "\n",
    "# this one needs to be single quotes in not match the double quotes...\n",
    "print(f\"output of simple_nn(X) is now {simple_nn(X, theta, activation='tanh'):.6f}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8411cb-c34f-4af8-a394-83699425c5e7",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">Q.</span> Put in sigmoid and ReLU activations and see what outputs you get there (you should do these by hand to check you implemented it correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989402cb-e8bc-48cc-9b2f-330608942671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73f33649-c726-406f-aa4f-342c9651b7ba",
   "metadata": {},
   "source": [
    "### Back-propagation\n",
    "\n",
    "So now we can do the feed-forward, how do we \"train\" the neural network $N$? Well again we need a measure of what it means for $N$ to do \"well\": we need to define a loss function $J$, possibly with penalisations added in accordingly. \n",
    "\n",
    "The name of the game is as before: we want to find the control parameters $\\theta = (w, b)$ of the neural network $N(\\theta)$ such that the loss $J$ is minimised. As before, we can leverage (stochastic) gradient-based methods in the following way. I am going to put another copy of the schematic here for convenience of reading.\n",
    "\n",
    "<img src=\"https://i.imgur.com/LFTTTa5.jpeg\" width=\"600\" alt='schematic neural network'>\n",
    "\n",
    "#### 1. Feed-forward (i.e. going from left to right) and evaluate loss $J$\n",
    "\n",
    "Guess the initial $\\theta$, do a feed-forward for the training set $X$ to generate $\\hat{Y}$, and compute the loss $J(Y, \\hat{Y})$. In the schematic, this is us going all the way from left to right.\n",
    "\n",
    "#### 2. Form the optimisation problem\n",
    "\n",
    "So we want to solve the problem (abusing notation substantially here) $\\partial J / \\partial \\theta = 0$. Lets suppose we start with the first control variable $w_1$; thus we want to evaluate\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J}{\\partial w_1}.\n",
    "\\end{equation*}\n",
    "\n",
    "#### 3. Apply chain rule (i.e. tracing back from right to left)\n",
    "\n",
    "Starting from all the way on the right, we see where $w_1$ is used. This would be in $h_1$ since $h_1 = h_1(w_1)$. By chain rule we have\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial J}{\\partial w_1} = \\frac{\\partial J}{\\partial h_1}\\frac{\\partial h_1}{\\partial w_1}.\n",
    "\\end{equation*}\n",
    "But then we also know that $h_1 = f(w_1 x_1 + \\ldots)$, so by chain rule and product rule we have\n",
    "\\begin{equation*}\n",
    "    \\frac{\\partial h_1}{\\partial w_1} = f'(w_1 x_1 + \\ldots) + x_1 f(w_1 x_1 + \\ldots).\n",
    "\\end{equation*}\n",
    "We are now back at the beginning. All the terms above can in principle be evaluated, particularly if we choose an activation function $f$ where the derivative is reasonably simple (e.g. no activation would mean $f'=1$, ReLU will give $f'=1$ for $x>0$ and $0$ otherwise). Continue with all the other entries in $\\theta$.\n",
    "\n",
    "#### 4. Do the root finding method\n",
    "\n",
    "The result is a whole load of algebraic equations and you want to find the roots of that. That tells you how to update your $\\theta$; update that, and then repeat until you there is some convergence and/or you get bored.\n",
    "\n",
    "You should be able to see how you might do this for arbitrary-sized networks in principle. Neural networks are constructed in such a way that ***back-propagation*** as described above by the chain rule is particularly easy to do, and is partly related to their success: gradient-based methods can be used, which speeds up convergence, so more control variables can be allowed in principle, which we can allow for more complexity in the neural network that allows for better representation of operators.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Make up a set of inputs/targets and define a loss of your choice; MSE would be a particularly easy one. I would probably just do $Y = X$ or $Y = X^2$ or something like that, but use a non-trivial activation function (because otherwise you can solve the problem in one go with matrix inversion). Try and modify the weights in the subroutine manually to see how you might do it to reduce the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017cac1-2cdb-42ce-aef5-15ea405f6663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b19ace93-238e-4163-bc34-c42387c69f01",
   "metadata": {},
   "source": [
    "---\n",
    "## b) Single-layer Perceptrons\n",
    "\n",
    "***Perceptrons*** is one of the early designs of a neural network. A schematic of this is shown below, for a case with a single layer and one with multiple layers (a MLP).\n",
    "\n",
    "<img src=\"https://i.imgur.com/e4R4nMM.jpeg\" width=\"600\" alt='mlps'>\n",
    "\n",
    "An earlier criticism of the single layer perceptrons is that it can only really do binary classifications on data that is linearly separable, and we can more or less do that already with SVMs. The above comment doesn't apply to the multiple layer case; in that sense MLPs shouldn't really be called perceptrons because they can do more things.\n",
    "\n",
    "Here we are going to make use of the cats and dogs data to demonstrate a few things with single layer perceptrons. Note that the single layer perceptron basically has no hidden layers. In this case there is no nonlinearity and it really is just matrix inversion. For operator $A$, images $X$ and labels $Y$ we have\n",
    "\\begin{equation}\n",
    "    AX = Y \\quad \\Rightarrow \\quad A = YX^\\dagger.\n",
    "\\end{equation}\n",
    "Here $X^\\dagger$ is the ***pseudo-inverse***. The system will most likely be in the over-determined regime so arrays are not going to be square, and we need to consider the problem as one of optimisation. The standard pseudo-inverse considers the problem as one of $L^2$ optimisation without a penalisation, i.e. linear regression.\n",
    "\n",
    "Going to load the cats and dogs data and then train up the model in the old fashioned way of matrix inversion. I'm just going to use all the data for demonstration purposes.\n",
    "\n",
    "> NOTE: We should not expect fantastic skill for the following problems with the cats and dogs data, because what I will be doing is an inherently difficult task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9242602-db0a-4b58-bfce-8f7e1fe6756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't read the headers\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/cat.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df_cats = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X_cats = df_cats.values\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"dog.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/dog.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df_dogs = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X_dogs = df_dogs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292ed92-deba-4358-a3ac-a152c160f401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of 25 indices (generate full list, shuffle, select first 25, so no repeats)\n",
    "ind = np.arange(80)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "# sample show (on-the-fly reshape data)\n",
    "fig = plt.figure(figsize=(8, 6.5))\n",
    "for i in range(20):\n",
    "    ax = plt.subplot(4, 5, i+1)\n",
    "    if i+1 < 10:\n",
    "        ax.imshow(np.reshape(X_cats[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    else:\n",
    "        ax.imshow(np.reshape(X_dogs[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab22fe-2808-4922-80d4-d05c180d4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of \"cats\" and \"dogs\" here is (pixels, index), so it is already flattened\n",
    "\n",
    "n = 64  # take the first 80% (out of 80 entries) of the data just because\n",
    "\n",
    "X_train = np.concatenate((X_cats[:n, :], X_dogs[:n, :]), axis=0)  # combine\n",
    "Y_train = np.concatenate((np.ones(n), -1*np.ones(n)))         # label: cats = 1 and dogs = -1\n",
    "\n",
    "X_test = np.concatenate((X_cats[n::, :], X_dogs[n::, :]), axis=0)\n",
    "Y_test = np.concatenate((np.ones(80-n), -1*np.ones(80-n)))\n",
    "\n",
    "# scale and redefine the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)\n",
    "\n",
    "# obtain model by simple matrix inversion\n",
    "A_pinv = Y_train @ np.linalg.pinv(X_train.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8da0db-50cb-466b-ae56-f7ae0e29ea59",
   "metadata": {},
   "source": [
    "Having got the model (which is just a matrix here), we make make predictions. We multiply the test data set and see what number it would give us. It won't give us the label values exactly (`1` and `-1`), but it essentially say that if it is positive/negative it is `1` and `-1` respectively, i.e. we only care about the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da657130-40c2-4699-adf4-7028e6788036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model by doing matrix multiplication (right answer is [1 1 1 ... -1 -1 -1])\n",
    "Y_pred = np.sign(A_pinv @ X_test.T)  # just need the sign\n",
    "\n",
    "# plot out the predictions (circles should lie on top of crosses if completely correct)\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.axes()\n",
    "ax.plot(Y_pred, 'bx', label=\"predictions\")\n",
    "ax.plot(Y_test, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.plot([15.5, 15.5], [-1.3, 1.3], 'k--', alpha=0.7)\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "# if Y_pred = +-1 and Y_test = +-1 (i.e. correct predictions) then Y_pred * Y_test = 1\n",
    "accuracy = np.sum(Y_pred * Y_test == 1) / len(Y_pred * Y_test)\n",
    "ax.set_title(f\"0-NN (pinv) accracy = {accuracy*100}%\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdccd9b9-bfd9-43f3-8452-ae862c0d1ac2",
   "metadata": {},
   "source": [
    "The accuracy is not great, but again image recognition is an inherently hard problem.\n",
    "\n",
    "The thing that is of possible interest is what does the model actually look like? We can probe this by actually plotting it out: we show below the (normalised) coeffs as a 1d array, then reshaped into an array of the same size as the image it acts on, and also plot a random image in the dataset for comparison reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce3acf-57f7-43a1-a48b-6e80b903dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out normalised coefficients and \"power\" in the image\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot2grid((4, 6), (0, 0), colspan=6)\n",
    "ax.plot(A_pinv / np.max(np.abs(A_pinv)))\n",
    "ax.set_ylabel(f\"coeff\")\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax = plt.subplot2grid((4, 6), (1, 0), rowspan=3, colspan=3)\n",
    "cs = ax.imshow(np.reshape(A_pinv, (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_xticks([]); ax.set_yticks([]);\n",
    "ax.set_title(\"coeffs as image\")\n",
    "\n",
    "ind = np.random.randint(X_test.shape[0])\n",
    "ax = plt.subplot2grid((4, 6), (1, 3), rowspan=3, colspan=3)\n",
    "ax.imshow(np.reshape(X_test[ind, :], (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_xticks([]); ax.set_yticks([]);\n",
    "ax.set_title(f\"#{ind}\")\n",
    "\n",
    "plt.tight_layout(pad=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c41ce0-375c-420c-b18c-441a0d35a5cb",
   "metadata": {},
   "source": [
    "The way to think about the model $A$ as the image is that you take that $A$ and multiply the image's pixels element-wise, then sum up the numbers to get a single number. If that number is bigger than 0 then the model predicts a cat, otherwise it's a dog.\n",
    "\n",
    "The $L^2$ regression here gave us a lot of non-zero coefficients, which is also shown in the reshaped image. Where the colours are particularly white/black is showing where the model things the pixels are important for classifying the image as cat or dog. In that sense the model has interpretability in that it thinks certain pixels are more important than others.\n",
    "\n",
    "We then recall that if we do $L^1$ penalisation then we could promote sparsity in the coefficients for this case. We can piggyback on `sklearn.linear_model.LASSO` in this case to do the same thing. The below code demonstrates how this would be done.\n",
    "\n",
    "> NOTE: We can do this only because the perceptron basically has no hidden layers or activation functions.\n",
    ">\n",
    "> Here I turned down my regularisation parameter `alpha` from the default of `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7883d-251d-460e-88c0-0985312247d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inversion with LASSO\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# some optimum in relation to the regulatisation parameter it seems\n",
    "model = Lasso(alpha=0.1).fit(X_train, Y_train)\n",
    "A_lasso = model.coef_\n",
    "\n",
    "# test model by doing matrix multiplication (right answer is [1 1 1 ... -1 -1 -1])\n",
    "Y_pred = np.sign(A_lasso @ X_test.T)  # just need the sign\n",
    "\n",
    "# plot out the predictions (circles should lie on top of crosses if completely correct)\n",
    "fig = plt.figure(figsize=(10, 2))\n",
    "ax = plt.axes()\n",
    "ax.plot(Y_pred, 'bx', label=\"predictions\")\n",
    "ax.plot(Y_test, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "ax.plot([15.5, 15.5], [-1.3, 1.3], 'k--', alpha=0.7)\n",
    "ax.set_xlabel(\"index\")\n",
    "ax.legend()\n",
    "ax.grid()\n",
    "\n",
    "# if Y_pred = +-1 and Y_test = +-1 (i.e. correct predictions) then Y_pred * Y_test = 1\n",
    "accuracy = np.sum(Y_pred * Y_test == 1) / len(Y_pred * Y_test)\n",
    "ax.set_title(f\"0-NN (LASSO) accracy = {accuracy*100}%\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ffdda2-e34b-4f4c-84f7-a4868a1236fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot out normalised coefficients and \"power\" in the image\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = plt.subplot2grid((4, 6), (0, 0), colspan=6)\n",
    "ax.plot(A_lasso / np.max(np.abs(A_lasso)))\n",
    "ax.set_ylabel(f\"coeff\")\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax = plt.subplot2grid((4, 6), (1, 0), rowspan=3, colspan=3)\n",
    "cs = ax.imshow(np.reshape(A_lasso, (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_xticks([]); ax.set_yticks([]);\n",
    "ax.set_title(\"coeffs as image\")\n",
    "\n",
    "# ind = np.random.randint(X_test.shape[0])\n",
    "ax = plt.subplot2grid((4, 6), (1, 3), rowspan=3, colspan=3)\n",
    "ax.imshow(np.reshape(X_test[ind, :], (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_xticks([]); ax.set_yticks([]);\n",
    "ax.set_title(f\"#{ind}\")\n",
    "\n",
    "plt.tight_layout(pad=0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c750439c-23cd-4bca-bbbc-b76ac734b9c8",
   "metadata": {},
   "source": [
    "As expected there are a lot of zero coefficients, and the skill is actually slightly higher. Thing to note here is that the coefficients of the model are non-zero\n",
    "\n",
    "* Near the eyes\n",
    "* Forehead\n",
    "* Near the mouth?\n",
    "\n",
    "This is interesting because this is possibly in line with our expectations that these might be key features that are useful for cats and dogs classification. Approaches such as these may be of interest to guide our feature creation for such problems (e.g. instead of throwing the whole image in, we may want to train models to label \"eyes\" and \"mouth\" and use those as features for our eventual classifier).\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Consider changing the `alpha` parameter and see how skill and model changes.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Could try other penalisations also (e.g. elastic net). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14ededf-530a-4e62-8373-d0abcfa768f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57642af3-4d12-4a32-8298-ef7b53f7b111",
   "metadata": {},
   "source": [
    "---\n",
    "## c) Usage of `pyTorch` and MLPs\n",
    "\n",
    "While it is in principle possible to create neural networks by hand (see extended exercise in this session), there are quite a few libraries/engines out there that are designed to link up the creation of the hidden layers, loss functions, optimisers and so forth. [PyTorch](https://pytorch.org/) is one of the those that will be considered here; [TensorFlow](https://www.tensorflow.org/) is another. \n",
    "\n",
    "> NOTE: Ultimately the above libraries leverage interfaces for manipulating arrays/***tensors***. In principle [JAX](https://github.com/jax-ml/jax) could also do it, although PyTorch and TensorFlow have higher level calls that make the process a bit easier.\n",
    ">\n",
    "> I am going to keep the implementation quite \"raw\" in this session. We can use an interface over PyTorch and TensorFlow to make our lives even easier (e.g. `Lightning` and/or `keras`), but going to save that for the next session. \n",
    "\n",
    "In the below I am going to do the (multi-layer) perceptron cats and dogs classification example to demonstrate certain aspects of the engine. First going to load the appropriate data. The below code is more of less copy and paste from before, using some of the `sklearn` utilities, except a bit more work is done to create a validation set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacabf65-3ab9-4f07-bb29-2d7518df604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't read the headers\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/cat.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df_cats = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X_cats = df_cats.values\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"dog.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/dog.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df_dogs = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X_dogs = df_dogs.values\n",
    "\n",
    "seed = 42\n",
    "train_ratio = 0.7\n",
    "valid_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "test_valid_ratio = valid_ratio + test_ratio\n",
    "assert train_ratio + valid_ratio + test_ratio == 1\n",
    "\n",
    "# Expected no. of samples\n",
    "print()\n",
    "total_sample = 80*2\n",
    "train_sample = int(train_ratio*total_sample)\n",
    "valid_sample = int(valid_ratio*total_sample)\n",
    "test_sample = total_sample - train_sample - valid_sample\n",
    "print(f\"Expected no. of samples (train, test, valid): {train_sample, test_sample, valid_sample}\")\n",
    "\n",
    "X_total = np.concatenate((X_cats, X_dogs), axis=0)\n",
    "Y_total = np.concatenate((np.ones(int(total_sample/2)), np.zeros(int(total_sample/2))))\n",
    "\n",
    "# train-(test&valid) split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_total, Y_total, test_size=test_valid_ratio, random_state=seed,)\n",
    "# split test set to test and valid set\n",
    "X_test, X_valid, Y_test, Y_valid = train_test_split(\n",
    "    X_test, Y_test, test_size=valid_ratio/test_valid_ratio, random_state=seed,)\n",
    "\n",
    "# check the shape\n",
    "print()\n",
    "print(f\"X_train shape : {X_train.shape}; Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}; Y_valid shape: {Y_valid.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}; Y_test shape: {Y_test.shape}\")\n",
    "\n",
    "# # scale and redefine the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train, X_valid, X_test = \\\n",
    "  scaler.transform(X_train), scaler.transform(X_valid), scaler.transform(X_test)\n",
    "\n",
    "# check the range of values\n",
    "print()\n",
    "print(f\"X_train : {X_train.min()}...{X_train.max()}\")\n",
    "print(f\"X_valid : {X_valid.min()}...{X_valid.max()}\")\n",
    "print(f\"X_test :  {X_test.min()}...{X_test.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b18f9-2ed8-418c-bbf1-947ee7b04ea5",
   "metadata": {},
   "source": [
    "From hereon we invoke `PyTorch` utilities. Zeroth thing we do is to load some packages. Then we are going to proceed as follows:\n",
    "\n",
    "1) Convert the data accordingly into things `PyTorch` can manipulate\n",
    "2) Define the MLP\n",
    "3) Define relevant loss function and optimisers\n",
    "4) Package up the parts in a training loop that is to be iterated on\n",
    "5) Train the model and evaluate things accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c6206-d80f-43ad-8896-4c9a9d312e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load package and data\n",
    "import torch\n",
    "import torch.nn as nn        # where the things to define hidden layers live\n",
    "import torch.optim as optim  # the optimisers used in back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032bbf87-43f1-4265-8429-b16acf19eeb4",
   "metadata": {},
   "source": [
    "### 1. Convert data into tensor objects\n",
    "\n",
    "For the present classification problem the inputs we have were integers, and after the usual standardisation are floats, which are converted into pytorch tensors as `FloatTensor`. The outputs really are integers, and this could be done as `LongTensor` (if it is SIGNED) or `ByteTensor` (if it is UNsigned); I am going to use `LongTensor` even though I don't strictly need to (since my labels are `0` and `1`s).\n",
    "\n",
    "> NOTE: `FloatTensor` here is `float32` rather than `float64`, respecively storing values up to 8 and 16 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af416a-8125-4115-aca7-98741a421389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data convertion in to pytorch objects\n",
    "print(f\"X_train array shape before = {X_train.shape}\")\n",
    "\n",
    "X_train, Y_train = torch.FloatTensor(X_train), torch.LongTensor(Y_train)\n",
    "X_test, Y_test = torch.FloatTensor(X_test), torch.LongTensor(Y_test)\n",
    "X_valid, Y_valid = torch.FloatTensor(X_valid), torch.LongTensor(Y_valid)\n",
    "\n",
    "print(f\"X_train tensor size after = {X_train.size()}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38464c98-1f0f-4173-865b-1d46c609b925",
   "metadata": {},
   "source": [
    "Converting to tensor objects have some advantages with regards to reshaping and so forth, to be demonstrated later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde3ce3-863d-40ee-9a63-b8bb8261a631",
   "metadata": {},
   "source": [
    "### 2. Define the MLP structure\n",
    "\n",
    "The below code define the neural network structure as a class object which can be passed around.\n",
    "\n",
    "* The model is called `nn_classify_pets`, and it includes `nn.Module` as a generic interface to receive inputs accordingly (but we won't have any here as such).\n",
    "* The `__init__` initialises the class with things it needs. The `super` part is for some inheritance reasons so that the `PyTorch` interface needs to see when doing the training later.\n",
    "* The main part is the structure which I wrote in one go in `self.layers`:\n",
    "  - `nn.Sequential` means all the things in that part are run sequentially in the specified order.\n",
    "  - `nn.Linear(64**2, 100)` takes inputs of size $64^2$ (i.e. the number of pixels in the flattened image) and pass that to 100 nodes (which is the `sklearn.MLPClassifier` default).\n",
    "  - You might put then an activation function (e.g. `nn.ReLU()`), although I didn't (which was a choice I made in the last session).\n",
    "  - `nn.Linear(100, 2)` then takes the 100 nodes and converts to two outputs, because in this particular classification problem I have two possibilities (cats or dogs).\n",
    "\n",
    "> NOTE: You might argue surely I just want one number given I want to evaluate whether the numbers are closer to `0` or `1` for the classification. That's not how `PyTorch` and the related loss function works; see later.\n",
    "\n",
    "* To wrap it up, the subroutine `forward` defines how the network does the feed-forward for loss function evaluation, which is needed for recording purposes for the back-propagation later (the recording procedure is sometimes referred to as ***taping*** or ***annotating***)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762aee64-d65c-4036-ae44-314e9a048b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model architecture\n",
    "\n",
    "class nn_classify_pets(nn.Module):\n",
    "    def __init__(self):  # specify input dims below\n",
    "        super(nn_classify_pets, self).__init__()\n",
    "\n",
    "        # nest it in one go\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64**2, 100),  # image is 64**2, 100 nodes (sklearn default)\n",
    "            # nn.ReLU(),            # no activation function\n",
    "            nn.Linear(100, 2)       # two possible outputs\n",
    "        )\n",
    "\n",
    "    # actual model structure: input -> hidden layer -> outputs, ReLU on input and hidden\n",
    "    def forward(self, x):\n",
    "        out =  self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99e38c9-7b45-45ae-96a2-c1c67faa7cfc",
   "metadata": {},
   "source": [
    "Should just say that the below is an equivalent way of defining the same thing as above, but in a slightly more verbose way: I define all the components in `__init__`, then assemble it in the order I want in `forward`.\n",
    "\n",
    "```Python\n",
    "class nn_classify_pets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nn_classify_pets, self).__init__()\n",
    "\n",
    "        self.input_layer = nn.Linear(64**2, 100)\n",
    "        self.output_layer = nn.Linear(100, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  self.input_layer(x)\n",
    "        # out = self.relu(out)  # no activation function here wth comment\n",
    "        out =  self.output_layer(out)\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a4762c-bb98-4e57-80b3-d789e1dbac12",
   "metadata": {},
   "source": [
    "### 3. Initialise relevant loss function and optimisers\n",
    "\n",
    "For the binary classification problem we can use `nn.CrossEntropyLoss()` (which is related to the information gain discussed previously), `nn.HingeEmbeddingLoss`, or some others that you can look up; going to use the first one.\n",
    "\n",
    "For the optimiser there are a variety of these in `torch.optim`; I am going to use `optim.Adam` (which is the default in `sklearn`), but other options are the standard stochastic gradient descent `optim.SGD` or even `optim.LBFGS`, which are also options in `sklearn`. There are more solvers than the ones I have mentioned here, have a look at those if you want.\n",
    "\n",
    "In the below I manually specify the ***learning rate*** (the step size for iteration towards the local minimum in the loss function landscape; the default if `0.001`). A larger value will speed up the training process, but may also overshoot and lead to non-convergence.\n",
    "\n",
    "Here we need to initialise the neural network model in order to pass the model weights to the optimiser; these are the things that will be updated by the optimizer.\n",
    "\n",
    "> NOTE: I additionally specify a seed to force reproducibility. Instead of specifying `random_state` this is done as `torch.manual_seed(NUM)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae618365-7b01-49ea-b29a-33d467e1f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisations here only\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "model = nn_classify_pets()\n",
    "learning_rate = 0.0001\n",
    "J = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # default is 0.001\n",
    "\n",
    "print(f\"model details: {model}\")\n",
    "print(\" \")\n",
    "print(f\"optimizer details: {optimizer}\")\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a019831-5cc1-4636-b984-0bbbec7f253c",
   "metadata": {},
   "source": [
    "### 4. Wrap up the training\n",
    "\n",
    "There are multiple parts to this. Below code wraps everything up in one subroutine `training` (because I am going to call this again and again), and what it does are the following:\n",
    "\n",
    "* Take in some required inputs (e.g. `model`, `optimizer`, `J`, training data for training, validation data to evalute model performance on the fly). The number of training epochs has a default choice, and `out_epoch` is a variable controlling how often diagonstics are thrown out to screen.\n",
    "* `train_J` and `test_J` are variables that will be dumped into with data needed for plotting the loss curves.\n",
    "* At every cycle of the epoch, we perform the following:\n",
    "  - Switch on taping model.\n",
    "  - Clear gradients if they already exist just to avoid contamination with `optimizer.zero_grad()`.\n",
    "  - Do a feed-forward with `Y_pred = model(X_train)`.\n",
    "  - Evalute the training loss with `J_train = J(Y_pred, Y_train)`, which has been taped/annotated accordingly.\n",
    "  - Do back-propagation with `J_train.backward()`, because we have the taping.\n",
    "  - Update the model weights with `optimizer.step()` (because the optimizer knows these are the control variables from `optim.Adam(model.parameters(), lr=learning_rate)` before).\n",
    "* Where relevant, evaluate also the loss value associated with the test set for diagnostic purposes, but the training itself never sees that data. Need to remember to switch off taping to avoid contamination.\n",
    "* Return the final model state and relevant diagnostics once the loop has ended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2376ab-bca6-4c99-af49-18312c0ed568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap the training up\n",
    "\n",
    "def training(model, optimizer, J, X_train, Y_train, X_valid, Y_valid,\n",
    "             num_epochs=500, out_epoch=50):\n",
    "\n",
    "    # define things to dump into for loss curve\n",
    "    train_J = np.zeros(num_epochs)\n",
    "    valid_J = np.zeros(num_epochs)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # iteration step\n",
    "\n",
    "        model.train()  # put the model in training mode (taping is on)\n",
    "        optimizer.zero_grad()  # clear gradients if it exists (from loss.backward())\n",
    "        Y_pred = model(X_train)  # feed-forward\n",
    "        J_train = J(Y_pred, Y_train) # compute loss\n",
    "        J_train.backward()  # back propagation\n",
    "        optimizer.step()  # iterate\n",
    "        model.eval()   # put the model in evaluation mode (taping is off for diags below)\n",
    "\n",
    "        # diagnostics: evaluation of metrics as we go along\n",
    "        with torch.no_grad():  # force no taping just in case\n",
    "            Y_pred = model(X_valid)\n",
    "            J_valid = J(Y_pred, Y_valid)\n",
    "            train_J[epoch] = J_train.item()\n",
    "            valid_J[epoch] = J_valid.item()\n",
    "\n",
    "        if (epoch + 1) % out_epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                + f\"Train Loss: {J_train.item():.4f}, \"\n",
    "                + f\"Validation Loss: {J_valid.item():.4f}\")\n",
    "\n",
    "    return model, train_J, valid_J"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3312def-bf9f-4fe9-94bf-b282952432d1",
   "metadata": {},
   "source": [
    "### 5. Train the model and evaluate it accordingly.\n",
    "\n",
    "What it says on the tin. Going to train it first, plot the loss functions and then say other things about the evaluation part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251efc4-ec95-4bdb-8a49-b60253b7c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the actual training and plot the loss curves\n",
    "model, train_J, valid_J = training(model, optimizer, J, \n",
    "                                   X_train, Y_train, \n",
    "                                   X_valid, Y_valid,\n",
    "                                   num_epochs=200, out_epoch=20)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_J, label=\"training loss\")\n",
    "ax.plot(valid_J, label=\"validaiton loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da968089-bae5-4a76-a328-501dc377c2c4",
   "metadata": {},
   "source": [
    "Because all the data is normalised a loss value above 1 is actually a bad thing. In the above the training loss is decreasing and settling down as it should, but the validation loss is going up.\n",
    "\n",
    "We can make predictions by running the model once more. In the below I wrapped out the prediction step with `torch.no_grad()` to force it to not do taping. Strictly speaking it doesn't matter if you never re-run the training again by restarting **properly**; see comment towards the end of the section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134daa4-34d2-4134-bf3f-ac36ea881070",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(X_test)\n",
    "print(predictions_train[0:10, :])\n",
    "print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005014aa-bba9-4fb1-8f1b-a2ecc4339166",
   "metadata": {},
   "source": [
    "The way to read the above output from the model for this classification problem is the score for the guess at position `0` (for dogs) and `1` for cats, and we want the **largest positive score**. We can pick out that by cycling through all the samples and using `np.argmax` argument, but we probably want to convert the tensor object back to numpy arrays; which can be done by the imbued function `.numpy()`. The below wraps up the various functions calls in one go as a subroutine, because I am going to reuse it again later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4580f83c-7a6c-4209-915b-da0ecd32348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do model prediction and evaluate skill\n",
    "def classification_skill(predictions, truths):\n",
    "    dum = predictions.numpy()\n",
    "    Y_pred = np.zeros(dum.shape[0])\n",
    "    N = len(Y_pred)\n",
    "    for i in range(N):\n",
    "        Y_pred[i] = np.argmax(dum[i, :])\n",
    "    skill_all = np.sum(Y_pred == truths.numpy())\n",
    "\n",
    "    print(f\"overall skill: {skill_all} correct out of {N} ({skill_all/N*100:.2f}%)\")\n",
    "    print(\" \")\n",
    "\n",
    "# plot out the predictions (circles should lie on top of crosses if completely correct)\n",
    "def classification_plot(predictions, Y_test):\n",
    "\n",
    "    dum = predictions.numpy()\n",
    "    Y_pred = np.zeros(dum.shape[0])\n",
    "    N = len(Y_pred)\n",
    "    for i in range(N):\n",
    "        Y_pred[i] = np.argmax(dum[i, :])\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 2))\n",
    "    ax = plt.axes()\n",
    "    ax.plot(Y_pred, 'bx', label=\"predictions\")\n",
    "    ax.plot(Y_test, 'ro', fillstyle=\"none\", label=\"truth\")\n",
    "    ax.set_ylim([-0.3, 1.3])\n",
    "    ax.set_xlabel(\"index\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    \n",
    "    accuracy = np.sum(Y_pred == Y_test.numpy()) / N\n",
    "    ax.set_title(f\"accuracy = {accuracy*100:.2f}%\");\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f45b46-a1cc-4d41-8970-60abc1c1d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute skill and plot actual predictions\n",
    "print(\"training set:\")\n",
    "classification_skill(predictions_train, Y_train)\n",
    "print(\"test     set:\")\n",
    "classification_skill(predictions_test, Y_test)\n",
    "\n",
    "fig = classification_plot(predictions_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abefa17-65b5-47c9-bce4-0eb47a0d8deb",
   "metadata": {},
   "source": [
    "So the above shows that we got all the training set classficiation correct (which is not too surprising), but it struggles on the test set, giving an accuracy that is not dissimilar to what we had last time with `sklearn` (with the understanding I used a different test size compared with before), so we have some consistency with previous results at least.\n",
    "\n",
    "In the below code I am going to define three more networks, one to have ReLU activation, one to have `hidden_layer=(200, 50)`, and one with 5 layers. These give roughly the same amount of skill (subject to the same caveat).\n",
    "\n",
    "> NOTE: Word of warning here. If you only run the training loop without re-initialising the model with `model = nn_classify_pets()` steps etc. it will probably restart from where you **just left off** rather than from zero. This is because the taping hasn't been cleared. If you have called `model` accordingly then it might do some intermediate taping, which may give you some weird things. It is best to start fresh, unless you really do mean to do restarts for whatever reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bd873-815e-4aa0-a01c-96b2d5e80cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the networks\n",
    "\n",
    "class nn_classify_pets_relu(nn.Module):\n",
    "    def __init__(self):  # specify input dims below\n",
    "        super(nn_classify_pets_relu, self).__init__()\n",
    "\n",
    "        # nest it in one go\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64**2, 100),\n",
    "            nn.ReLU(),              # with activation\n",
    "            nn.Linear(100, 2)\n",
    "        )\n",
    "\n",
    "    # actual model structure: input -> hidden layer -> outputs, ReLU on input and hidden\n",
    "    def forward(self, x):\n",
    "        out =  self.layers(x)\n",
    "        return out\n",
    "\n",
    "class nn_classify_pets_2layers(nn.Module):\n",
    "    def __init__(self):  # specify input dims below\n",
    "        super(nn_classify_pets_2layers, self).__init__()\n",
    "\n",
    "        # nest it in one go (two hidden layers, 64**2 -> 200 -> 50 -> 2)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64**2, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out =  self.layers(x)\n",
    "        return out\n",
    "\n",
    "class nn_classify_pets_3layers(nn.Module):\n",
    "    def __init__(self): \n",
    "        super(nn_classify_pets_3layers, self).__init__()\n",
    "\n",
    "        # (3 hidden layers, 64**2 -> 400 -> 200 -> 100 -> 2)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(64**2, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 2)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        out = self.sigmoid(out) # last activation important\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699238e8-8523-4372-b80c-453e40daa517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisations\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "model = nn_classify_pets_relu()\n",
    "learning_rate = 0.0001\n",
    "J = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # default is 0.001\n",
    "\n",
    "# do the actual training\n",
    "model, train_J, valid_J = training(model, optimizer, J, X_train, Y_train, X_valid, Y_valid,\n",
    "                                   num_epochs=200, out_epoch=20)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_J, label=\"training loss\")\n",
    "ax.plot(valid_J, label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0779ca7e-9df2-4d95-9b0c-998720151ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(X_test)\n",
    "\n",
    "print(\"training set:\")\n",
    "classification_skill(predictions_train, Y_train)\n",
    "print(\"test     set:\")\n",
    "classification_skill(predictions_test, Y_test)\n",
    "\n",
    "fig = classification_plot(predictions_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6c7320-af42-435c-a1f7-20abff77fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisations\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "model = nn_classify_pets_2layers()\n",
    "learning_rate = 0.0001\n",
    "J = nn.CrossEntropyLoss()  # for classification tasks, H = -p log q\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # default is 0.001\n",
    "\n",
    "# do the actual training\n",
    "model, train_J, valid_J = training(model, optimizer, J, X_train, Y_train, X_valid, Y_valid,\n",
    "                                   num_epochs=200, out_epoch=20)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_J, label=\"training loss\")\n",
    "ax.plot(valid_J, label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009bf3b8-a845-4898-88a7-b4f79f47e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(X_test)\n",
    "\n",
    "print(\"training set:\")\n",
    "classification_skill(predictions_train, Y_train)\n",
    "print(\"test     set:\")\n",
    "classification_skill(predictions_test, Y_test)\n",
    "\n",
    "fig = classification_plot(predictions_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f25948b-9116-4902-8e8d-dc999008da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisations\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "model = nn_classify_pets_3layers()\n",
    "learning_rate = 0.0001\n",
    "J = nn.CrossEntropyLoss()  # for classification tasks, H = -p log q\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # default is 0.001\n",
    "\n",
    "# do the actual training\n",
    "model, train_J, valid_J = training(model, optimizer, J, X_train, Y_train, X_valid, Y_valid,\n",
    "                                   num_epochs=200, out_epoch=20)\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_J, label=\"training loss\")\n",
    "ax.plot(valid_J, label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb3fdec-aca3-42c8-a7ef-364a552f4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions_train = model(X_train)\n",
    "    predictions_test = model(X_test)\n",
    "\n",
    "print(\"training set:\")\n",
    "classification_skill(predictions_train, Y_train)\n",
    "print(\"test     set:\")\n",
    "classification_skill(predictions_test, Y_test)\n",
    "\n",
    "fig = classification_plot(predictions_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7590537-e9df-4bac-8988-6d73a6679ab6",
   "metadata": {},
   "source": [
    "To close the very quick intro to using `PyTorch`, we can print out the model as `model.state_dict()`. This can then be saved and loaded accordingly as\n",
    "\n",
    "```Python\n",
    "    torch.save(model.state_dict(), \"model_001.pth\")\n",
    "    print(\"Saved PyTorch Model State to model_001.pth\")\n",
    "    model = nn_model()  # or whatever you defined the class to be\n",
    "    model.load_state_dict(torch.load(\"model_001.pth\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c356103-b843-476a-91b0-f87faadceedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the mode to have a look\n",
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8508e285-faf8-46b6-a4e5-0a49d4223766",
   "metadata": {},
   "source": [
    "One main constraint you should be careful about is the neural network size: MLPs by construction are all fully connected, so your parameter space increases very quickly with the number of nodes/layers. You could try putting this on a GPU for bigger networks etc., but there are quite a few details you will need to add in (which we will come to in two sessions time).\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> I have personally not found decreasing `learning_rate` to help with the classification task that much, but see what happens if you decrease it further. You could try increasing it, but for me the loss function on the test set blows up fairly spectacularly...\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try exploring other classification relevant loss functions.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> One thing that is used to stablise the problem and/or to avoid overfitting is to have ***early stopping***, which stops the training once the model training is not improving that much. Have a look how to implement this (in `PyTorch` this is done as a `Handler` instance).\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Another thing that is used to stablise the problem and/or to avoid overfitting is to do ***dropout***. Have a look what that is and how to implement this.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Another thing that is used to stablise the problem and/or to avoid overfitting is to stabilise the problem. This can be done in a bespoke way by changing the definition of the loss function (cf. linear models before, and PINNs later), but can also be done through the options specified through the optimiser. Have a see how you would do through `optim.SDG`, `optim.Adam` and/or `optim.AdamW` solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9954fa0d-6bf5-485d-8850-de4501471878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c19b1dd-79a7-4bdf-bb46-961902768f32",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook\n",
    "\n",
    "## 1) Code your own neural network by hand\n",
    "\n",
    "Define appropriate inputs and outputs (e.g. $Y = X$, $Y = X^2$, $Y = \\sin (X)$), use a non-trivial activation function and a loss function (MSE would be a good one). Link up the `simple_nn` subroutine with `scipy.optimize` to optimize for the parameters; the default L-BFGS solver will probably be fine.\n",
    "\n",
    "If you've done that then you've successfully coded your own (simple) neural network by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3cc2ae-2ab9-4eea-aa14-97fd9a85c324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9306aa0-7d17-4e11-aace-5856930ec620",
   "metadata": {},
   "source": [
    "## 2) Penguins data\n",
    "\n",
    "Try doing the classifer and regressors with the penguins data. I seriously doubt you need that much complexity in the networks for this though: the feature space dimension is low, and number of samples is not that high (in contrast to the cats + dogs data above, where the raw feature space dimension is large ($64^2$) but the sample size is probably not large enough to balance that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dfa8a6-2d42-4ce7-b16e-1daef7df76c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "628d361f-87ba-44fe-8cb7-f4d20d4b2161",
   "metadata": {},
   "source": [
    "## 3) Turtle and penguins data\n",
    "\n",
    "The Kaggle dataset of [penguins and turtles](https://www.kaggle.com/datasets/abbymorgan/penguins-vs-turtles) encountered previously might be better to do for neural network training; try doing stuff on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec8ccb-6bcf-4f6b-8222-125cfe331e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77984388-1c20-4ccc-9697-d64d93e20919",
   "metadata": {},
   "source": [
    "## 4) Time series data\n",
    "\n",
    "Consider training regressors for doing time-series prediction, using the `elnino34_sst.data`, or from model data (e.g. Lotka-Volterra, Lorenz etc.)\n",
    "\n",
    "If you want, try and do classifying problems, e.g. using a few data points before to predict whether there will be an upcoming El-Nino / La Nina episode. This requires you to provide labels to the data first, but you should know how to do that in principle already from the exercises in the previous sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d353a-1728-41ec-9bcf-6555b44dad6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d10ad40-a468-4f87-b1a8-eae04fb6b5ad",
   "metadata": {},
   "source": [
    "## 5) More data is always a good thing?\n",
    "\n",
    "Redo the above cats and dogs classification problem but only using the top or bottom half of the image (cf. exercise 7 below if you want some code that may be useful for the data processing). I personally find I can get high accuracy with a smaller/simpler network than if I throw in all the data (I can get up to 80% accuracy).\n",
    "\n",
    "Possibly a demonstration that working smarter at the data processing stage can potentially yield more skill than throwing the whole kitchen sink in through the model complexity (cf. a well-designed experiment likely beats the fancy statistical tests you can do on the resulting data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59ed891-71a8-4a23-aef2-9254976f5fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d1f1f39-f509-4eb2-a683-3e2c0e178a12",
   "metadata": {},
   "source": [
    "## 6) Extended cats dataset\n",
    "\n",
    "This may require a bit more computing power than you would have in a regular instance, and may be worth skipping for now but revisiting in the next session when we deal with Convolutional Neural Networks. The extended cats dataset have 2000 images in (instead of the basic one with 80), which may or may not be better for training a regressor on. Try and train a regressor accordingly and quantify the skill in a way of your choosing.\n",
    "\n",
    "You should note there are images within that dataset that may want to be excluded accordingly, as alluded to back in session 02 when we did eigencats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5a7258-84e0-4214-a5bb-b293b75a0a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8f6115a-f568-416f-8acd-8217c604fc9a",
   "metadata": {},
   "source": [
    "## 7) MLP as regressors\n",
    "\n",
    "You could try also the regression problem, although it might be easier to see this after the CNN section next time where I demonstrate it once anyway. Try predicting one half of the face with the other; see results I show in the lecture slides. \n",
    "\n",
    "See below for some code on how I tidied the data as top and bottom half; you could do left and right (the latter option is marginally simpler because of how the data is structured)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4c22fd-4895-4658-b416-cd09b1908c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subroutine to split top and bottom half of pixels: reshape, split, then flatten for sklearn\n",
    "def split_top_bottom(data):\n",
    "    n, width = data.shape[0], int(np.sqrt(data.shape[1]))\n",
    "    data = np.reshape(data, (n, width, width))\n",
    "    top_half, bottom_half = data[:, :, 0:width//2], data[:, :, width//2::]\n",
    "    top_half = np.reshape(top_half, (n, width*width//2))\n",
    "    bottom_half = np.reshape(bottom_half, (n, width*width//2))\n",
    "\n",
    "    return top_half, bottom_half\n",
    "\n",
    "# shape of \"cats\" here is (pixels, index), so it is already flattened\n",
    "n = 64\n",
    "\n",
    "# manually split up into train and test\n",
    "X_train, Y_train = split_top_bottom(X_cats[:n, :])\n",
    "X_test, Y_test = split_top_bottom(X_cats[n::, :])\n",
    "\n",
    "# scale the data\n",
    "scale_X, scale_Y = StandardScaler().fit(X_train), StandardScaler().fit(Y_train)\n",
    "X_train, X_test = scale_X.transform(X_train), scale_X.transform(X_test)\n",
    "Y_train, Y_test = scale_Y.transform(Y_train), scale_Y.transform(Y_test)\n",
    "\n",
    "# generate a list of 25 indices (generate full list, shuffle, select first 25, so no repeats)\n",
    "ind = np.arange(n)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "# sample show (on-the-fly reshape data)\n",
    "fig = plt.figure(figsize=(8, 2))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(2, 5, i+1)\n",
    "    ax.imshow(np.reshape(X_train[ind[i], :], (64, 32)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"$X$\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "    ax = plt.subplot(2, 5, i+1+5)\n",
    "    ax.imshow(np.reshape(Y_train[ind[i], :], (64, 32)).T, cmap=\"gray\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"$Y$\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
