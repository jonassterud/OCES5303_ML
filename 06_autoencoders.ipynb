{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4807e94-963f-41df-b5d1-086e3df6c76b",
   "metadata": {},
   "source": [
    "*updated 21 Jan 2026, Julian Mak (whatever with copyright, do what you want with this)\n",
    "\n",
    "### As part of material for OCES 5303 \"AI and Machine Learning in Ocean Science\" delivered at HKUST\n",
    "\n",
    "For the latest version of the material, go to the public facing [GitHub](https://github.com/julianmak/OCES5303_ML_ocean) page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844859c7-e5a4-4df2-b2dc-b4386bd70af8",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Autoencoders\n",
    "\n",
    "***Auto-encoders*** can be thought of as *unsupervised* neural networks, although they are trained in the supervising sense. The **auto** refers to output and input being (basically) the same (acting on itself in a sense). Here we are going to build various types of autoencoders and use it to do a few things; I am going to leverage the `keras` interface for this.\n",
    "\n",
    "> ## Key Objective(s)\n",
    "> 1. Introduce auto-encoders and demonstrate some possible uses of those.\n",
    "> 2. Implement a convolution auto-encoder (CAE) for similar tasks.\n",
    "\n",
    "> NOTE: Some of these can be a bit slow and you may want to put these on a GPU to do training with. If you use `keras` with the `PyTorch` backend, then it seems that as long as `PyTorch` can detect the GPU (and that may only be true for certain types/brands of GPUs), then the `keras` interface handles all the moving on and off the GPU already, so you ***probably*** don't need to do anything extra (which is big bonus of using `keras`; I say \"probably\" because I am not quite sure how to definitively check this...)\n",
    ">\n",
    "> In the PINNs session I will do everything in `PyTorch` instead and highlight some of the related syntax and subtleties you have to be careful of to make the code run with GPUs.\n",
    "\n",
    "Loading some relevant packages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d928ddb6-de42-4475-8edf-389eb0ff6a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# keras related\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"  # use PyTorch as backend\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "keras.backend.clear_session() # force a clean keras session (clears models etc.)\n",
    "\n",
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# check for GPUs availability\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca55a2c-5094-4518-8ddf-712aacd0fb3d",
   "metadata": {},
   "source": [
    "---\n",
    "## a) Structure and uses of auto-encoders\n",
    "\n",
    "There are the two parts to the auto-encoder architecture:\n",
    "\n",
    "1. The ***encoder***, which compresses information by squeezing the feature dimension down to some ***latent space***, and is a neural network of some description.\n",
    "2. The ***decoder***, which blows up the latent space representation back up to the original dimension, and is a also neural network of some description.\n",
    "\n",
    "Pictorially this is represented as follows:\n",
    "\n",
    "<img src=\"https://i.imgur.com/n2dcFVU.png\" width=\"600\" alt='autoencoder'>\n",
    "\n",
    "> NOTE: The decoder does not have to have a structure that exactly mirror the encoder or vice-versa. You just need them to have appropriate input/output dimensions.\n",
    "\n",
    "The idea is that the encoder is trained to squash out the unnecessary information, resulting in a lower dimension representation. The decoder on the other hand is trained to recover the original data using that lower dimension representation. The below demonstrates two possible uses of this (although it's not done all that well, because the chosen problem is deliberately hard).\n",
    "\n",
    "### 1) Latent space representation (cf. alternative to PCA et al.)\n",
    "\n",
    "In session 2 we considered several ways to reduce the dimensionality of data, but we could also do it using neural networks. Going to do the easy case with `penguins` data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42908da9-b09c-47fa-a3d2-8ad9243a55c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the penguin data\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"penguins.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/penguins.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "features = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\n",
    "X = df[features].values\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d12e8-d09d-4e65-bdf3-5243214717d6",
   "metadata": {},
   "source": [
    "Here I am going to define a really simple single layer autoencoder, consisting of `layer_in`, which is used in the encoder to map the four dimensional features space to two, and `layer_out`, which is the decoder to map it back to four dimensions, both to have ReLU as the activation function. The forward then consists of encoder followed by the decoder.\n",
    "\n",
    "I am writing this as a subroutine to wrap up the models in a slightly more tidy way. The sequence of operations in this case is taped accordingly by `keras`. I am outputting all three (uncompiled) models because I want to use all three of them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1dcf35-4111-4f08-a668-a81caee56ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define really simple auto-encoder (two parts jammed together: encoder and decoder)\n",
    "\n",
    "def ae_penguin():\n",
    "\n",
    "    # 1) define the autoencoder and the layers: 4 dim -> 2 dim -> 4 dim\n",
    "    inputs = keras.Input(shape=(4,))\n",
    "    encoded = layers.Dense(2, activation='relu')(inputs)\n",
    "    decoded = layers.Dense(4, activation='relu')(encoded)\n",
    "    autoencoder = keras.Model(inputs, decoded, name=\"autoencoder\")\n",
    "\n",
    "    # 2) pull out the encoder, 4 dim -> 2 dim\n",
    "    encoder = keras.Model(inputs, encoded, name=\"encoder\")\n",
    "\n",
    "    # 3) pull out the decoder, 2 dim -> 4 dim (this one needs a bit more syntax)\n",
    "    encoded_input = keras.Input(shape=(2,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = keras.Model(encoded_input, decoder_layer(encoded_input), name=\"decoder\")\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# initialise the model and see what it has\n",
    "dummy, _, _ = ae_penguin()\n",
    "dummy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1bf7f1-2ffd-4a05-bf66-1bf1a8b25079",
   "metadata": {},
   "source": [
    "For this auto-encoder, the input and output data is the same. Below sets the seed, initialises model, compiles the options and trains it (it's just a copy-and-paste job from last time, with the validation part removed). \n",
    "\n",
    "Instead of leveraging batching through `DataLoader` I've set batching on through the `autoencoder.fit(..., batch_size=...)`, showing extra power of `keras` in interfacing things in a neat way.\n",
    "\n",
    "> NOTE: Not going to use `Dataset` and `DataLoader` for this one just to demonstrate some syntax. Observe we also don't need to turn these into tensors if we are using `keras`, which is quite convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f008ab4-7cca-4397-a5a9-0cd700cb81ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise, compile and train the model\n",
    "\n",
    "# define data\n",
    "X_train, Y_train = X_scaled, X_scaled\n",
    "\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# initialise model\n",
    "autoencoder, encoder, decoder = ae_penguin()\n",
    "\n",
    "# compile\n",
    "learning_rate = 0.001\n",
    "autoencoder.compile(loss=keras.losses.MeanSquaredError(), \n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    )\n",
    "\n",
    "# train\n",
    "train_log = autoencoder.fit(X_train, Y_train, \n",
    "                            epochs=300,\n",
    "                            batch_size=50,\n",
    "                            verbose=0,\n",
    "                            callbacks=[TqdmCallback(verbose=1)],\n",
    "                           )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d407d-b695-4850-9c2f-052f6e3e7761",
   "metadata": {},
   "source": [
    "For this one I am mainly interested in showing the ***latent*** space representation. This I can get by calling the encoder part of the model, and because I chose my encoder to map from four to two dimensions. Below does the call and plots out the results, labelled by the species.\n",
    "\n",
    "> NOTE: Here I don't need to set whether the model is in training or predictive mode when using `keras`, which I would probably should do if I were doing this through `PyTorch`.\n",
    ">\n",
    "> Note also that even though I trained the `autoencoder`, the `encoder` knows the resulting weights, thanks to the appropriate taping of operations enabled through `keras`. In `PyTorch` this is also true, although you'd need to define the encoder and decoder probably as a subroutine within the neural network class object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955acc2-07ed-4ded-9dd3-7f8be484fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space representation\n",
    "latent_rep_train = encoder.predict(X_train)\n",
    "\n",
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = plt.axes()\n",
    "for species in df[\"species\"].unique():   # pick out all unique entries under `species`\n",
    "    ax.scatter(latent_rep_train[df[\"species\"] == species, 0],\n",
    "               latent_rep_train[df[\"species\"] == species, 1],\n",
    "               label=species,\n",
    "               alpha=0.5,  # fade this for demonstration later\n",
    "               )\n",
    "ax.set_xlabel(f\"latent dim 1\")\n",
    "ax.set_ylabel(f\"latent dim 2\")\n",
    "ax.grid(lw=0.5, zorder=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab9db19-f983-4a90-92da-804009d6fe4b",
   "metadata": {},
   "source": [
    "You can think of this as a neural network equivalent of PCA or similar, where the network finds the optimal co-ordinate transformation/project (note for most of my attempts the `Adelie` class gets mapped to things close to zero though...) \n",
    "\n",
    "Given enough depth and breadth it may be that the auto-encoder captures a good latent space representation that may be computationally out of reach with other procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d33a5c5-ec42-4934-afca-18cf8773f984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5c4d517-edf1-49d8-b46d-debfdb6a23d4",
   "metadata": {},
   "source": [
    "### 2) Generating new data from the decoder\n",
    "\n",
    "Above only really uses the encoder, but you could imagine that you can choose numbers strategically in the latent space representation, feed that into the decoder to artificially generate new data; this may be a useful way to generate new data from an existing dataset. Below is an example taken from the keras blog for Variational Autoencoder for the MNIST dataset, where each image is generated by specifying two numbers (or a point in the latent space plane in this case):\n",
    "\n",
    "<img src=\"https://blog.keras.io/img/ae/vae_digits_manifold.png\" width=\"400\" alt='vae generative from keras'>\n",
    "\n",
    "An equivalent larger dimension problem would be you take a set of images, train an auto-encoder, and use the decoder part to generate new images. Going to briefly show how that works with the cats image set.\n",
    "\n",
    "> NOTE: It doesn't work very well here because the number of images is rather low, and also to make the model train in sensible time I didn't make the auto-encoder too deep or wide. \n",
    ">\n",
    "> In this case having CNN based encoders and decoders would probably work better anyway; see later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a601a896-30c0-4255-a68e-e022fbc27c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't read the headers\n",
    "\n",
    "option = \"remote\"\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"cat.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/cat.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df_cats = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X_cats = df_cats.values\n",
    "\n",
    "if option == \"local\":\n",
    "    print(\"loading data locally (assumes file has already been downloaded)\")\n",
    "    path = \"dog.csv\"\n",
    "elif option == \"remote\":\n",
    "    print(\"loading data remotely\")\n",
    "    path = \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/dog.csv\"\n",
    "else:\n",
    "    raise ValueError(\"INVALID OPTION: use 'remote' or 'local'\")\n",
    "\n",
    "df_dogs = pd.read_csv(path, header=None).T # make \"features\" the axis=-1\n",
    "X_dogs = df_dogs.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59f886-5560-4355-82c3-955af461d733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "seed = 42\n",
    "\n",
    "# normalise to [0, 1]\n",
    "X_total = X_cats / 255 # TODO: cat/dog mix to generate more cursed examples?\n",
    "Y_total = X_cats / 255\n",
    "\n",
    "# only do train/test split; validation split later with keras\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_total, Y_total, train_size=0.90, random_state=seed,)\n",
    "\n",
    "# check the shape\n",
    "print()\n",
    "print(f\"X_train shape : {X_train.shape}; Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}; Y_test shape: {Y_test.shape}\")\n",
    "\n",
    "# check the range of values\n",
    "print()\n",
    "print(f\"X_train : {X_train.min()}...{X_train.max()}\")\n",
    "print(f\"X_test :  {X_test.min()}...{X_test.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d66d01-4d83-4be4-a013-61c156e73d81",
   "metadata": {},
   "source": [
    "In the autoencoder below I make it go from $64^2$ to $16^2$ then $4^2$ with the usual fully connected neural networks, then I reverse that process. The jumps in the dimensions are reasonably big, so we don't really expect this to work all that well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f4842c-603f-424d-8651-bf8ac3cbd05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define really simple auto-encoder (two parts jammed together: encoder and decoder)\n",
    "\n",
    "def ae_catdog():\n",
    "\n",
    "    # 1) define the autoencoder and the layers\n",
    "    inputs = keras.Input(shape=(64**2,))\n",
    "    encoded = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(16**2, activation='relu'),  # no input here\n",
    "            layers.Dense(4**2, activation='relu'),\n",
    "        ]\n",
    "    )(inputs)  # input to be passed here\n",
    "    decoded = keras.Sequential(\n",
    "        [\n",
    "            layers.Dense(16**2, activation='relu'),\n",
    "            layers.Dense(64**2, activation='sigmoid'),\n",
    "        ]\n",
    "    )(encoded)\n",
    "    autoencoder = keras.Model(inputs, decoded, name=\"autoencoder\")\n",
    "\n",
    "    # 2) pull out the encoder\n",
    "    encoder = keras.Model(inputs, encoded, name=\"encoder\")\n",
    "\n",
    "    # 3) pull out the decoder (this one needs a bit more syntax)\n",
    "    encoded_input = keras.Input(shape=(4**2,))\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = keras.Model(encoded_input, decoder_layer(encoded_input), name=\"decoder\")\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# initialise the model and see what it has\n",
    "dummy, _, _ = ae_catdog()\n",
    "dummy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f378d-15d4-45e7-8e4c-cea41fa3047c",
   "metadata": {},
   "source": [
    "The data has already been prepped so I just go ahead with throwing it in (again as `numpy` arrays). Three things I did slightly different here:\n",
    "\n",
    "* I used `optimizers.AdamW` instead of the usual `Adam` with the `weight_decay` option, which is a regulariser of sorts for the solver (this I got from Jonathan)\n",
    "* Instead of defining validation dataset manually, I split this out from the train set through `keras` via `.fit(..., validation_split=0.2, ...)`, which is splitting out 20% of the train set for validation.\n",
    "* I don't do batching here; there is no particular reason for this except for comparing with my other implementation in `PyTorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21487418-8fb5-40d5-a8ef-875356fe370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise model, compile, train and plot loss\n",
    "\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# initialise model\n",
    "autoencoder, encoder, decoder = ae_catdog()\n",
    "\n",
    "# compile\n",
    "learning_rate = 0.0005\n",
    "autoencoder.compile(loss=keras.losses.MeanSquaredError(), \n",
    "                    # note the use of a different Adam\n",
    "                    optimizer=keras.optimizers.AdamW(learning_rate=learning_rate,\n",
    "                                                     weight_decay=0.001),\n",
    "                    )\n",
    "\n",
    "# train + validation (split out 20% of train data for validation)\n",
    "train_log = autoencoder.fit(X_train, Y_train, \n",
    "                            epochs=600,\n",
    "                            validation_split=0.2,  # get validation data from train set\n",
    "                            verbose=0,\n",
    "                            callbacks=[TqdmCallback(verbose=1)],\n",
    "                           )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.plot(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac03148-3c75-4cac-b868-3d01608f52b2",
   "metadata": {},
   "source": [
    "It looks not unreasonable on the training loss, but it's pretty bad on the validation dataset. Again this is not entirely surprising, because I have kept the dataset size and neural network complexity low for speed reasons. The below code runs a model prediction to see what the results look like on both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c4f668-2b3a-4fc9-9286-70d9de5a5370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample show\n",
    "predictions_train = autoencoder.predict(X_train)\n",
    "predictions_test = autoencoder.predict(X_test)\n",
    "\n",
    "# generate a list of indices (generate full list, shuffle, select first bit, so no repeats)\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, i+1)\n",
    "    ax.imshow(np.reshape(X_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "# same for test set\n",
    "ind = np.arange(X_test.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, 10+i+1)\n",
    "    ax.imshow(np.reshape(X_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 10+5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10cca32-fa7b-4f8a-bc6e-d957b169c86a",
   "metadata": {},
   "source": [
    "(Again, I never said this was going to \"good\"!)\n",
    "\n",
    "We can now do a \"PCA\" like thing by calling only the encoder to give us the latent space representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae68a4a-4b01-4488-a314-5a01e460687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latent space representation\n",
    "latent_rep_train = encoder.predict(X_train)\n",
    "\n",
    "# generate the same indices used in the above for train set\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "# plot out\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(5):\n",
    "    ax.plot(latent_rep_train[ind[i], :], label=f\"#{ind[i]}\")\n",
    "ax.set_xlabel(r\"latent space dim index\")\n",
    "ax.set_ylabel(r\"latent space co-ord\")\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f840585-ab1f-4d2f-a057-4bd34a5e0d0b",
   "metadata": {},
   "source": [
    "Each image thus has a \"barcode\" of sorts tagged with it, given by the numbers associated with each latent space dimension (cf. the PCs under the EOF transformation). You might expect images that are more \"alike\" (in whatever sense the neural networks decided for it to mean) would be closer to each other.\n",
    "\n",
    "> NOTE: We may come back to this point relating to barcodes if we talk about ***topological data analysis***.\n",
    "\n",
    "Now you could choose a selection of numbers (16 in this case), pass it to the *decoder*, and it will spit out an image for you. This we will try now, by:\n",
    "\n",
    "1. Throwing in a \"mean\" barcode by taking the mean value of the above.\n",
    "2. Throwing in a \"random\" barcode, by randomly selecting existing values from the training dataset as we cycle through the latent space dimension.\n",
    "\n",
    "> NOTE: The \"random\" one is constrained to be sampled from the training dataset, because making it truly random is probably going to do something weird; you can try this yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea10119-77dc-4e2a-aca8-735f7c9a82e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate two additional barcodes and plot that out\n",
    "\n",
    "# take an \"average\" along the components\n",
    "mean_representation = np.mean(latent_rep_train, axis=0)\n",
    "\n",
    "# randomly select some values for the representation from existing pool\n",
    "n_samples, n_dim = latent_rep_train.shape\n",
    "rand_representation = np.zeros(n_dim)\n",
    "for i in range(len(rand_representation)):\n",
    "    rand_representation[i] = latent_rep_train[np.random.randint(n_samples), i]\n",
    "\n",
    "# plot out\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "for i in range(5):\n",
    "    ax.plot(latent_rep_train[ind[i], :], alpha=0.4)\n",
    "ax.plot(mean_representation, 'k--', label=\"mean\")\n",
    "ax.plot(rand_representation, 'k:', label=\"random\")\n",
    "ax.set_xlabel(r\"latent space dim index\")\n",
    "ax.set_ylabel(r\"latent space co-ord\")\n",
    "ax.legend()\n",
    "ax.grid();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e93cf0-cc81-492f-af49-d3c05b9f783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursed beast from decoder\n",
    "gen_mean = decoder.predict(mean_representation.reshape(1, 16))\n",
    "gen_rand = decoder.predict(rand_representation.reshape(1, 16))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.imshow(np.reshape(gen_mean, (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_xticks([]); ax.set_yticks([]);\n",
    "ax.set_title(r\"mean cat\")\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.imshow(np.reshape(gen_rand, (64, 64)).T, cmap=\"gray\")\n",
    "ax.set_xticks([]); ax.set_yticks([]);\n",
    "ax.set_title(r\"'random' cat\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef489c-efc9-47b6-9473-8ee4a35a0cad",
   "metadata": {},
   "source": [
    "Again, I never said they were going to be good!\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> Try randomly generating a barcode to pass to the encoder to generate more cursed images.\n",
    ">\n",
    "> <span style=\"color:red\">Q.</span> Try doing similar things but now including cats and dogs. Investigate if the latent space representation of the two classes look distinctly different, which would indicate it may have skill in doing classification problems (e.g. session 3, or doing classification with neural networks).\n",
    ">\n",
    "> Also see what kind of cursed cat-dog things you can generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32d95e3-c80e-4386-b6a7-6ece9a8a6fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c47705d-82d4-4837-bbc3-0503fb72d0de",
   "metadata": {},
   "source": [
    "### 3) De-noising data\n",
    "\n",
    "One application we could use with an autoencoder is ***de-noising*** the data. The idea here is that the output is clean data, while the input is the same data but with noise in, so the autoencoder is trained to remove the noise via some compression and decompression. For this we can more or less do a copy-and-paste job from above, where the only difference is what we put in as input and output data. The below set of code basically does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aa0209-bc70-4774-ac03-27a66bddb311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data again for safety\n",
    "seed = 42\n",
    "\n",
    "# normalise to [0, 1]\n",
    "X_total = X_cats / 255 # TODO: cat/dog mix to generate more cursed examples?\n",
    "Y_total = X_cats / 255\n",
    "\n",
    "# only do train/test split; validation split later with keras\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_total, Y_total, train_size=0.90, random_state=seed,)\n",
    "\n",
    "# check the shape\n",
    "print()\n",
    "print(f\"X_train shape : {X_train.shape}; Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}; Y_test shape: {Y_test.shape}\")\n",
    "\n",
    "# check the range of values\n",
    "print()\n",
    "print(f\"X_train : {X_train.min()}...{X_train.max()}\")\n",
    "print(f\"X_test :  {X_test.min()}...{X_test.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84fbc2-ff9c-4d22-9957-29362232ab9f",
   "metadata": {},
   "source": [
    "Here I am going to add some Gaussian noise to the data with some specified `noise_level`, and then plot out some of the noisy and clean image that are exposed to the autoencoder.\n",
    "\n",
    "> NOTE: My expectation here is that the larger the `noise_level` the harder the autoencoder has to work to remove it; play around with this for later to investigate whether that is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8b19d-d3f9-438e-a35a-6c154e1e26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# denoising: add noise to the X_train but keep Y_train clean\n",
    "\n",
    "noise_level = 0.2  # uniform noise with some magnitude (data here is NORMALISED to [0, 1])\n",
    "\n",
    "X_train += noise_level * np.random.rand(X_train.shape[0], \n",
    "                                        X_train.shape[1])\n",
    "X_test  += noise_level * np.random.rand(X_test.shape[0], \n",
    "                                        X_test.shape[1])\n",
    "\n",
    "# generate a list of indices (generate full list, shuffle, select first whatever, so no repeats)\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3.5))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(2, 5, i+1)\n",
    "    ax.imshow(np.reshape(X_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r\"with noise\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(2, 5, 5+i+1)\n",
    "    ax.imshow(np.reshape(Y_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r\"clean image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be747fb-6320-4d39-90fc-c99a6b3665cc",
   "metadata": {},
   "source": [
    "I am going to be lazy and use the same autoencoder defined above; the only thing I need to do is to reinitialise it. The block below is literally copy-and-pasted from above, the only difference being I redefined the *input* data (which has noise included)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10658d3-d263-4f5c-a6ed-0639b567729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise model, compile, train and plot loss\n",
    "\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# initialise model\n",
    "autoencoder, encoder, decoder = ae_catdog()\n",
    "\n",
    "# compile\n",
    "learning_rate = 0.0005\n",
    "autoencoder.compile(loss=keras.losses.MeanSquaredError(), \n",
    "                    # note the use of a different Adam\n",
    "                    optimizer=keras.optimizers.AdamW(learning_rate=learning_rate,\n",
    "                                                     weight_decay=0.001),\n",
    "                    )\n",
    "\n",
    "# train + validation (split out 20% of train data for validation)\n",
    "train_log = autoencoder.fit(X_train, Y_train, \n",
    "                            epochs=600,\n",
    "                            validation_split=0.2,  # get validation data from train set\n",
    "                            verbose=0,\n",
    "                            callbacks=[TqdmCallback(verbose=1)],\n",
    "                           )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.plot(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b2295-8a8e-49db-9303-b1f2b011dcbe",
   "metadata": {},
   "source": [
    "Below evaluates on both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f30b6c0-431a-4723-b840-75e9ea7daae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample show\n",
    "predictions_train = autoencoder.predict(X_train)\n",
    "predictions_test = autoencoder.predict(X_test)\n",
    "\n",
    "# generate a list of indices (generate full list, shuffle, select first bit, so no repeats)\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, i+1)\n",
    "    ax.imshow(np.reshape(X_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "# same for test set\n",
    "ind = np.arange(X_test.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, 10+i+1)\n",
    "    ax.imshow(np.reshape(X_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 10+5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bc51ce-3e3d-4a9b-ab26-c2dbb0c9c191",
   "metadata": {},
   "source": [
    "It is at least believable for the train set, but it's not great on the test set in that it seems to be generating different cats! This is consistent with the loss curves above, and perhaps expected because the sample size is low so it doesn't know how to generalise.\n",
    "\n",
    "As a just-for-fun test, I am going to hit the three ad-hoc TAs with the trained auto-encoder to see what cursedness results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b603a9f-9527-4cb3-a1b3-fdb3556c7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# load data remotely\n",
    "targets_path = {\n",
    "    \"miffy_gormless\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/miffy_gormless.jpg\",\n",
    "    \"blauhaj\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/blauhaj.jpg\",\n",
    "    \"clippy\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/clippy.jpg\",\n",
    "}\n",
    "\n",
    "targets = {}\n",
    "\n",
    "for file_name, file_url in targets_path.items():\n",
    "    response = requests.get(file_url)\n",
    "    targets[file_name] = Image.open(BytesIO(response.content))\n",
    "\n",
    "# massage the data into the right shape, standardise it, and pass it to the model\n",
    "data = np.concatenate( (np.array(targets[\"miffy_gormless\"]).T.reshape(-1, 64**2),\n",
    "                        np.array(targets[\"blauhaj\"]).T.reshape(-1, 64**2),\n",
    "                        np.array(targets[\"clippy\"]).T.reshape(-1, 64**2),\n",
    "                       ), dtype=\"float64\"\n",
    "                     )\n",
    "data /= 255.0  # map it to [0, 1]\n",
    "data += noise_level * np.random.rand(data.shape[0], \n",
    "                                     data.shape[1])\n",
    "predictions_oos = autoencoder.predict(data)\n",
    "\n",
    "# out-of-sample cursedness\n",
    "fig = plt.figure(figsize=(6, 3.5))\n",
    "for i in range(3):\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    ax.imshow(np.reshape(data[i, :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(2, 3, 3+i+1)\n",
    "    ax.imshow(np.reshape(predictions_oos[i, :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead27f4d-aef0-4141-b9d3-1dafdcc39714",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">Q.</span> Experiment with changing the network architecture, train-test-validate ratios and the `noise_level` to see what impact this has on the model skill etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14360295-9fd8-4470-a541-2ebdc3b070c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b19ace93-238e-4163-bc34-c42387c69f01",
   "metadata": {},
   "source": [
    "---\n",
    "## b) Convolutional Autoencoders (CAEs)\n",
    "\n",
    "The autoencoder above is just MLPs as encoder and decoders stuck together, and there is nothing stopping you doing the same thing using CNNs (or others, e.g. LSTM). That's basically what we are going to do here. Observe that going to lower dimensions is easy because a CNN does this anyway, so we don't have problems with defining the encoder. For the decoder we need the corresponding commands to \"undo\" those processes.\n",
    "\n",
    "> NOTE: I use the word \"undo\" very loosely here, because you again you don't have to mirror the operations. I just mean that if we reduced dimension we need to increase dimension somehow for the input; see later on how you might do this.\n",
    "\n",
    "Here we want to load data as images instead, and I am going to revisit the denoising problem. I am going to reload the data, split it, add noise, then reshape it into the image shape that `keras` is expecting (in the default form with \"channel last\" convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41723cb-ea39-4b65-b96d-42545fd00c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data again for safety\n",
    "seed = 42\n",
    "\n",
    "# normalise to [0, 1]\n",
    "X_total = X_cats / 255\n",
    "Y_total = X_cats / 255\n",
    "\n",
    "# only do train/test split; validation split later with keras\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_total, Y_total, train_size=0.90, random_state=seed,)\n",
    "\n",
    "# add noise\n",
    "noise_level = 0.2  # uniform noise with some magnitude (data here is NORMALISED to [0, 1])\n",
    "\n",
    "X_train += noise_level * np.random.rand(X_train.shape[0], \n",
    "                                        X_train.shape[1])\n",
    "X_test  += noise_level * np.random.rand(X_test.shape[0], \n",
    "                                        X_test.shape[1])\n",
    "\n",
    "# reshape (channel last convention)\n",
    "X_train, Y_train = X_train.reshape(-1, 64, 64, 1), Y_train.reshape(-1, 64, 64, 1)\n",
    "X_test, Y_test = X_test.reshape(-1, 64, 64, 1), Y_test.reshape(-1, 64, 64, 1)\n",
    "\n",
    "# check the shape\n",
    "print()\n",
    "print(f\"X_train shape : {X_train.shape}; Y_train shape: {Y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}; Y_test shape: {Y_test.shape}\")\n",
    "\n",
    "# check the range of values\n",
    "print()\n",
    "print(f\"X_train : {X_train.min()}...{X_train.max()}\")\n",
    "print(f\"X_test :  {X_test.min()}...{X_test.max()}\")\n",
    "\n",
    "# generate a list of indices (generate full list, shuffle, select first whatever, so no repeats)\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3.5))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(2, 5, i+1)\n",
    "    ax.imshow(X_train[ind[i], :, :, 0].T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r\"with noise\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(2, 5, 5+i+1)\n",
    "    ax.imshow(Y_train[ind[i], :, :, 0].T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(r\"clean image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0536d17-5a69-4725-b545-362c68141685",
   "metadata": {},
   "source": [
    "Going to define the relevant convolution layers and architecture (I largely took it from [here](https://keras.io/examples/vision/autoencoder/)). The things we are doing are that:\n",
    "\n",
    "* Encoder is the usual, but we \"pad\" it out accordingly when doing the `Conv2D`, so that only the `MaxPooling` is reducing the dimensions (by a factor of 2).\n",
    "* The `Conv2DTranspose` \"undo\"s the `Conv2D` operations.\n",
    "* The `strides=2` are there to \"undo\" the convolution every two steps and blow the data back up to the original dimension (to mirror the `MaxPooling` in terms of what it is doing to the data dimensions).\n",
    "\n",
    "Note that this autoencoder is not symmetric, in the sense that the decoder is not exactly \"undo\"ing the operations of the encoder. That is allowed, as you can do whatever you want in some sense. There are other ways to define the decoder to get the dimensions you want for defining the autoencoder.\n",
    "\n",
    "Thing you probably want to do is to convince yourself the specified dimensions here make sense.\n",
    "\n",
    "> NOTE: You could \"cheat\" a bit here with the dimension checking (like I realised later). You can define every operation explicitly, e.g.\n",
    "```Python\n",
    "x = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "x = layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "...\n",
    "x = layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "```\n",
    "> form the model and then call `.summary()`, which will then tell you what the dimensions are at each step, then you write these back into the subroutine properly.\n",
    ">\n",
    "> In that sense `keras` is quite nice in that it will try make things work, and you can also query what it did try and do for fixing other things. In `PyTorch` you would need to be very specific with the inputs and outputs (like I did the first time round...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a243b-5a79-4eb4-8706-c435c8c72e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define really simple auto-encoder (two parts jammed together: encoder and decoder)\n",
    "\n",
    "def convae_catdog():\n",
    "\n",
    "    # 1) define the autoencoder and the layers\n",
    "    inputs = keras.Input(shape=(64, 64, 1))\n",
    "    encoded = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.MaxPooling2D((2, 2), padding=\"same\"),\n",
    "            layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"),\n",
    "            layers.MaxPooling2D((2, 2), padding=\"same\"),\n",
    "        ]\n",
    "    )(inputs)  # input to be passed here\n",
    "    decoded = keras.Sequential(\n",
    "        [\n",
    "            layers.Conv2DTranspose(32, (3, 3), strides=2, \n",
    "                                   activation=\"relu\", padding=\"same\"),\n",
    "            layers.Conv2DTranspose(32, (3, 3), strides=2, \n",
    "                                   activation=\"relu\", padding=\"same\"),\n",
    "            layers.Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\"),\n",
    "        ]\n",
    "    )(encoded)\n",
    "    autoencoder = keras.Model(inputs, decoded, name=\"autoencoder\")\n",
    "\n",
    "    # 2) pull out the encoder\n",
    "    encoder = keras.Model(inputs, encoded, name=\"encoder\")\n",
    "\n",
    "    # 3) pull out the decoder (this one needs a bit more syntax)\n",
    "    encoded_input = keras.Input(shape=(16, 16, 32,)) # <--- CAREFUL HERE\n",
    "    decoder_layer = autoencoder.layers[-1]\n",
    "    decoder = keras.Model(encoded_input, decoder_layer(encoded_input), name=\"decoder\")\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "# initialise the model and see what it has\n",
    "dummy, _, _ = convae_catdog()\n",
    "dummy.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f7fd39-3eb7-4639-b7d0-a64816f11f59",
   "metadata": {},
   "source": [
    "I am going to train for substantially fewer epochs compared to the MLP based autoencoder above (300 vs. 600)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262dc40-89f9-4c68-a441-b2abf871b759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise model, compile, train and plot loss\n",
    "\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# initialise model\n",
    "autoencoder, encoder, decoder = convae_catdog()\n",
    "\n",
    "# compile\n",
    "learning_rate = 0.0005\n",
    "autoencoder.compile(loss=keras.losses.MeanSquaredError(), \n",
    "                    # note the use of a different Adam\n",
    "                    optimizer=keras.optimizers.AdamW(learning_rate=learning_rate,\n",
    "                                                     weight_decay=0.001),\n",
    "                    )\n",
    "\n",
    "# train + validation (split out 20% of train data for validation)\n",
    "train_log = autoencoder.fit(X_train, Y_train, \n",
    "                            epochs=300,\n",
    "                            validation_split=0.2,  # get validation data from train set\n",
    "                            verbose=0,\n",
    "                            callbacks=[TqdmCallback(verbose=1)],\n",
    "                           )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.plot(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267a4e2e-b0f5-4cec-870e-ad8666f8ebfa",
   "metadata": {},
   "source": [
    "The losses look good in this case. Lets see what the images actually look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d913b35-f10e-4891-b339-5ae612edd4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample show\n",
    "predictions_train = autoencoder.predict(X_train)\n",
    "predictions_test = autoencoder.predict(X_test)\n",
    "\n",
    "# generate a list of indices (generate full list, shuffle, select first bit, so no repeats)\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, i+1)\n",
    "    ax.imshow(np.reshape(X_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "# same for test set\n",
    "ind = np.arange(X_test.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, 10+i+1)\n",
    "    ax.imshow(np.reshape(X_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 10+5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c1a5d1-37c1-4e82-be74-486f0d943258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# load data remotely\n",
    "targets_path = {\n",
    "    \"miffy_gormless\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/miffy_gormless.jpg\",\n",
    "    \"blauhaj\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/blauhaj.jpg\",\n",
    "    \"clippy\" : \"https://raw.githubusercontent.com/julianmak/OCES5303_ML_ocean/refs/heads/main/clippy.jpg\",\n",
    "}\n",
    "\n",
    "targets = {}\n",
    "\n",
    "for file_name, file_url in targets_path.items():\n",
    "    response = requests.get(file_url)\n",
    "    targets[file_name] = Image.open(BytesIO(response.content))\n",
    "\n",
    "# massage the data into the right shape, standardise it, and pass it to the model\n",
    "data = np.concatenate( (np.array(targets[\"miffy_gormless\"]).T.reshape(-1, 64**2),\n",
    "                        np.array(targets[\"blauhaj\"]).T.reshape(-1, 64**2),\n",
    "                        np.array(targets[\"clippy\"]).T.reshape(-1, 64**2),\n",
    "                       ), dtype=\"float64\"\n",
    "                     )\n",
    "data /= 255.0  # map it to [0, 1]\n",
    "data += noise_level * np.random.rand(data.shape[0], \n",
    "                                     data.shape[1])\n",
    "data = data.reshape(-1, 64, 64, 1)\n",
    "predictions_oos = autoencoder.predict(data)\n",
    "\n",
    "# out-of-sample cursedness\n",
    "fig = plt.figure(figsize=(6, 3.5))\n",
    "for i in range(3):\n",
    "    ax = plt.subplot(2, 3, i+1)\n",
    "    ax.imshow(np.reshape(data[i, :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(2, 3, 3+i+1)\n",
    "    ax.imshow(np.reshape(predictions_oos[i, :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5eec560-6564-4f73-852b-458d6f8ba4c5",
   "metadata": {},
   "source": [
    "So it's doing better in that it is at least preserving the images so that the output is at least the same cat/ad-hoc TA, but it is pretty blurred.\n",
    "\n",
    "> <span style=\"color:red\">Q.</span> You could have used `layers.UpSampling2D((2, 2))` to \"undo\" the `layers.MaxPooling2D((2, 2))` instead, and not use `layers.Conv2dTranspose(...,stride=2, ...)`. See if that makes any difference (notably if it will make the output images sharper).\n",
    "> \n",
    "> <span style=\"color:red\">Q.</span> You don't strictly need to use `layers.Conv2dTranspose` I suppose for square images, because `layers.Conv2d` would be fine also (why?). Try swapping it out and convincing yourself it doesn't matter (then you can suppress a warning about `Conv2dTranspose` may not be consistent depending on backend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a6ee13-4ad4-4453-9cc6-7c5c79d7b5d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae039d84-e055-4933-8960-d837236047ce",
   "metadata": {},
   "source": [
    "### Choice of loss function?\n",
    "\n",
    "So when I was reading [this example](https://keras.io/examples/vision/autoencoder/) I noticed they use `BinaryCrossentropy()` as the loss function, which puzzled me because that didn't seem to make theoretical sense to me. Upon reading a bit more, the conclusions largely seems to be that \"it is machine learning so you can do whatever you want as long as it works\" (well within reason, e.g. you can't really use MSE for classification problems)...\n",
    "\n",
    "A more nuanced rationale is that when you are optimising you want to find the optimum, but the optimum doesn't have to be zero necessarily. Sure your MSE would be zero if the images match completely, but it doesn't mean the images are not \"good\" if you choose another loss function that is not zero at the optimum. Main point is that it isn't **not** invalid (double negative here), although in that sense introduces an extra dimension for hyper-parameter selection.\n",
    "\n",
    "Below swaps out the loss function as well changing the solver and adding batching in. The results are largely similar to above though, but then again the images used here are more complex and much smaller in sample size than the MNIST dataset in the example on the `keras` webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac2ecf-bb64-4067-a4cf-89cc0ebcf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialise model, compile, train and plot loss\n",
    "\n",
    "keras.utils.set_random_seed(4321)\n",
    "\n",
    "# initialise model\n",
    "autoencoder, encoder, decoder = convae_catdog()\n",
    "\n",
    "# compile\n",
    "learning_rate = 0.001\n",
    "autoencoder.compile(loss=keras.losses.BinaryCrossentropy(), \n",
    "                    # note the use of a different Adam\n",
    "                    optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                    )\n",
    "\n",
    "# train + validation (split out 20% of train data for validation)\n",
    "train_log = autoencoder.fit(X_train, Y_train, \n",
    "                            epochs=300,\n",
    "                            validation_split=0.1,  # get validation data from train set\n",
    "                            batch_size=20,\n",
    "                            verbose=0,\n",
    "                            callbacks=[TqdmCallback(verbose=1)],\n",
    "                           )\n",
    "\n",
    "# plot the loss curves\n",
    "fig = plt.figure(figsize=(6, 3))\n",
    "ax = plt.axes()\n",
    "ax.plot(train_log.epoch, train_log.history[\"loss\"], label=\"training loss\")\n",
    "ax.plot(train_log.epoch, train_log.history[\"val_loss\"], label=\"validation loss\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(r\"$J$\")\n",
    "ax.grid()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96561603-e569-4d95-8be7-39d596270025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample show\n",
    "predictions_train = autoencoder.predict(X_train)\n",
    "predictions_test = autoencoder.predict(X_test)\n",
    "\n",
    "# generate a list of indices (generate full list, shuffle, select first bit, so no repeats)\n",
    "ind = np.arange(X_train.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)  # syntax for shuffle: not used like a function with input output...\n",
    "\n",
    "fig = plt.figure(figsize=(8, 7))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, i+1)\n",
    "    ax.imshow(np.reshape(X_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_train[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "\n",
    "# same for test set\n",
    "ind = np.arange(X_test.shape[0])\n",
    "np.random.seed(123)\n",
    "np.random.shuffle(ind)\n",
    "\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(4, 5, 10+i+1)\n",
    "    ax.imshow(np.reshape(X_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_title(f\"#{ind[i]}\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);\n",
    "    ax = plt.subplot(4, 5, 10+5+i+1)\n",
    "    ax.imshow(np.reshape(predictions_test[ind[i], :], (64, 64)).T, cmap=\"gray\")\n",
    "    ax.set_xticks([]); ax.set_yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0866cc8f-c145-4011-b1df-ebedb9384f23",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">Q.</span> Experiment with changing the network architecture, dataset ratios, the `noise_level` and other hyper-parameters to see what impact this has on the model skill etc.\n",
    ">\n",
    "> You could also do this on the MNIST dataset, which is smaller in image size but has a much larger sample size that will presumably help with the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555eacef-453e-4818-9e99-efe3b58a5269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c19b1dd-79a7-4bdf-bb46-961902768f32",
   "metadata": {},
   "source": [
    "----------------\n",
    "# More involved exercises with this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a1f7c-d4df-4ac8-a022-929b40f9e7c6",
   "metadata": {},
   "source": [
    "## 1) Using the larger cats image dataset\n",
    "\n",
    "The above codes demonstrate how an autoencoder works but the performance is not great, largely attributed to the sample size being rather low. Have a look at using the larger set of cats images that I used for eigencat in a previous session instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cae3147-39d2-49b4-9202-58080e1a4b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b452b15-804a-49a7-b878-c0aa71edcc5f",
   "metadata": {},
   "source": [
    "## 2) Implementation in `PyTorch`\n",
    "\n",
    "Good exercise to try and see how you would do the same thing in `PyTorch` (to appreciate how `keras` makes things that much easier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0bc669-bb96-47a3-a8f2-90a1b9431c6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b65641f-a35c-4b7b-a77e-a83dcf83ff7a",
   "metadata": {},
   "source": [
    "## 3) Variational Autoencoders\n",
    "\n",
    "Have a look towards the bottom of [this page](https://blog.keras.io/building-autoencoders-in-keras.html) for a demonstration of what a ***Variational Autoencoder*** is, and try and implement this for yourself. Note that the code set is out of date in that entry; this [more updated one](https://github.com/keras-team/keras-io/blob/master/examples/generative/vae.py) may help, but you still need to modify it assuming you don't use an unholy mix of `TensorFlow` together with `PyTorch`.\n",
    "\n",
    "You will probably also need to read up on what the ***KL divergence*** is, in relation to how the loss function is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9554d-43db-4177-8701-2b4382f4a911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5f2b5e8-944c-44db-88da-b2613212fe7b",
   "metadata": {},
   "source": [
    "## 4) Map data of some sort\n",
    "\n",
    "Below may be ideas for the extended project.\n",
    "\n",
    "Try and find some spatio-temporal data on a regular grid (e.g. satellite products, numerical simulation data), and train an autoencoder to reconstruct missing grid cells (that you artificially remove for example) or similar reconstruction tasks.\n",
    "\n",
    "One thing we know is that autoencoders in the latent space representation is a bit like a PCA, which is related to finding EOFs (Empirical Orthogonal Functions). Climate variability such as El-Nino (ENSO) is usually picked out by EOFs, so see if what you actually get if you apply autoencoders to the associated data instead. There is some reconstructed SST data [here](https://github.com/julianmak/OCES3301_data_analysis/raw/refs/heads/main/ersstv5_ssta.nc); you can look into my OCES 3301 lecture 9/10 and assignment 3 to see what kind of manipulations you would need to do in the \"tradional\" sense if you haven't done it before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdea6c77-dabf-41a4-b50a-061d30c9a6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3784639-e29f-44e4-b3c2-dc7168b4dfa1",
   "metadata": {},
   "source": [
    "## 5) Using MNIST data\n",
    "\n",
    "Try doing standard/convolution/variational autoencoders for the MNIST data or similar; see previous extended exercises for where to get those etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9be97-0e18-44bc-9897-3f5416cc0598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
